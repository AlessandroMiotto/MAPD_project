{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04c09336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:8787/status\n"
     ]
    }
   ],
   "source": [
    "# CLUSTER DEPLOYMENT, TO BE SUBSTITUED WHEN GOING CLUSTER\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# For now, local deployment on my computer (multicore)\n",
    "ncore = 8\n",
    "cluster = LocalCluster(n_workers=ncore, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "\n",
    "# Print the dashboard link over the port 8787\n",
    "print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7495557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dask.array<array, shape=(20640, 8), dtype=float64, chunksize=(2580, 8), chunktype=numpy.ndarray>\n"
     ]
    }
   ],
   "source": [
    "import dask.array as da\n",
    "import dask\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Download California Housing dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "# Convert features into Dask Array (it's a matrix). N partition = 16\n",
    "X_da = da.from_array(data.data.values, chunks=(2580, data.data.shape[1]))\n",
    "\n",
    "print(X_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a795cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_choleskyQR_parallel(X_da : dask.array.Array):\n",
    "    # A list of delayed tasks for each partition of the dataset\n",
    "    # Each partition computes the local Gram matrix\n",
    "    chunks_delayed = [dask.delayed(lambda x : x.T @ x)(chunk) for chunk in X_da.to_delayed().ravel()]\n",
    "\n",
    "    # Now sum all the local Gram matrices to get the global Gram matrix\n",
    "    Gram_global_delayed = dask.delayed(sum)(chunks_delayed)\n",
    "\n",
    "    # Compute R as the Cholesky decomposition on the global Gram matrix (as a delayed even if a serial operation just call .compute at the end)\n",
    "    R = dask.delayed(np.linalg.cholesky)(Gram_global_delayed)\n",
    "    #R.visualize(\"CholeskyR.png\")\n",
    "    R = R.compute() # Compute R\n",
    "    R_inv = np.linalg.inv(R) # It's a small matrix, so this operation is fast even if serial\n",
    "\n",
    "    Q = X_da.map_blocks(lambda block: block @ R_inv, dtype=X_da.dtype)\n",
    "    #Q.visualize(\"CholeskyQ.png\")\n",
    "    Q = Q.compute() # Compute Q\n",
    "    return Q, R\n",
    "\n",
    "def compute_choleskyQR_serial(X):\n",
    "    # Global gram matrix\n",
    "    G = X.T @ X\n",
    "    R = np.linalg.cholesky(G)\n",
    "    R_inv = np.linalg.inv(R)\n",
    "    Q = X @ R_inv\n",
    "    \n",
    "    return Q, R\n",
    "\n",
    "def compute_choleskyR_parallel(X_da : dask.array.Array):\n",
    "    # A list of delayed tasks for each partition of the dataset\n",
    "    # Each partition computes the local Gram matrix\n",
    "    chunks_delayed = [dask.delayed(lambda x : x.T @ x)(chunk) for chunk in X_da.to_delayed().ravel()]\n",
    "    # Now sum all the local Gram matrices to get the global Gram matrix\n",
    "    Gram_global_delayed = dask.delayed(sum)(chunks_delayed)\n",
    "    # Compute R as the Cholesky decomposition on the global Gram matrix (as a delayed even if a serial operation just call .compute at the end)\n",
    "    R = dask.delayed(np.linalg.cholesky)(Gram_global_delayed)\n",
    "    R = R.compute() # Compute R\n",
    "    return  R\n",
    "\n",
    "def compute_choleskyR_serial(X):\n",
    "    # Global gram matrix\n",
    "    G = X.T @ X\n",
    "    R = np.linalg.cholesky(G)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a094b150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 98.5 ms, sys: 12.7 ms, total: 111 ms\n",
      "Wall time: 120 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# parallel\n",
    "Q_p, R_p = compute_choleskyQR_parallel(X_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96e57eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.49 ms, sys: 2.89 ms, total: 4.38 ms\n",
      "Wall time: 3.01 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# serial\n",
    "Q_s, R_s = compute_choleskyQR_serial(data.data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50244094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||R_parallel - R_serial||_2 = 6.6703992450451735e-09\n",
      "||Q_parallel - Q_serial||_2 = 5.281864770776042e-10\n",
      "||Q^T @ Q- I||_2 = 7971678.680290628\n",
      "||X - Q @ R||_2 = 2.838161229486053e-10\n"
     ]
    }
   ],
   "source": [
    "# Let's see whether the results are compatible\n",
    "diffR = np.linalg.norm(R_p - R_s, 2)\n",
    "diffQ = np.linalg.norm(Q_p - Q_s, 2)\n",
    "print(f\"||R_parallel - R_serial||_2 = {diffR}\")\n",
    "print(f\"||Q_parallel - Q_serial||_2 = {diffQ}\")\n",
    "\n",
    "# Check orthogonality of Q\n",
    "orthogonality_metric = np.linalg.norm(Q_s.T @ Q_s - np.eye(Q_s.shape[1]), 2)\n",
    "print(f\"||Q^T @ Q- I||_2 = {orthogonality_metric}\")\n",
    "# Check decomposition\n",
    "decomp_metric = np.linalg.norm(data.data.values - Q_s @ R_s, 2)\n",
    "print(f\"||X - Q @ R||_2 = {decomp_metric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8f3404",
   "metadata": {},
   "source": [
    "As expected, the decomposition yielded a non reasonnable result (Q is not orthogonal, the algorithm is highly unstable)\n",
    "\n",
    "\n",
    "Let's try with a different and larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bdb666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_csv(\"HIGGS.csv\", header=None, blocksize=\"400MB\")\n",
    "X_df = df.iloc[:, 1:]  # no labels\n",
    "X_da = X_df.to_dask_array(lengths=True)\n",
    "\n",
    "N = 1000000\n",
    "X_sample = X_df.head(N).to_numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d06face3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.08 s, sys: 3.92 s, total: 9 s\n",
      "Wall time: 39.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Q, R = compute_choleskyQR_parallel(X_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ed87c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 100 ms, sys: 32 ms, total: 132 ms\n",
      "Wall time: 118 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Q, R = compute_choleskyQR_serial(X_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envLabComp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
