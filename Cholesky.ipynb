{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c193d167",
   "metadata": {},
   "source": [
    "## Cholesky QR decomposition of a Tall and Skinny matrix\n",
    "\n",
    "N.B. This is not the direct TSQR method proposed in the article. The Cholesky-based approach lacks precision and numerical stability, making it impractical for high-performance environments. Nevertheless, we found it interesting to implement it as an exercise, exploring the Dask implementation and benchmarking it, despite its inherent numerical instability\n",
    "\n",
    "Let $B$ be a symmetric and positive definite $n\\times n$ matrix. Then its Cholesky decomposition is:\n",
    "$$\n",
    "B = L L^T \n",
    "$$\n",
    "where $L$ is a lower triangular $n\\times n$ matrix. Cholesky decomposition turns out to be particularly useful when computing the QR decomposition of a $m\\times n$ matrix $A$. Let's first build the temporary matrix $T = A^T A$, symmetric by definition. Hence, if:\n",
    "$$\n",
    "A = QR\n",
    "$$\n",
    "then:\n",
    "$$\n",
    "T = A^T A = (QR)^T(QR) = R^T Q^T Q R\n",
    "$$\n",
    "The matrix $Q$ is orthogonal, so $T = R^T R$. Since $R$ is a triangular matrix too, then we have effectively found the Cholesky decomposition of $T$ in terms of $R$. In other words, the Cholesky QR decomposition proceeds as follow:\n",
    "1) Given $A$, build the symmetric matrix $T = A^T A$\n",
    "2) Apply the Cholesky decomposition on $ T = L L^T$\n",
    "3) Obtain $R = L^T$. Obtain Q by solving $A = QR$\n",
    "\n",
    "This procedure is numerically unstable, and for certain matrices $A$ it may fail to produce accurate results, particularly for the orthogonal matrix $Q$. However, it is easily parallelizable, making it a useful testbed for experimenting with Dask.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04c09336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:8787/status\n"
     ]
    }
   ],
   "source": [
    "# CLUSTER DEPLOYMENT, TO BE EXECUTED ONLY IN A LOCAL ENVIRONMENT!!\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# For now, local deployment on my computer (multicore)\n",
    "ncore = 4\n",
    "cluster = LocalCluster(n_workers=ncore, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "\n",
    "# Print the dashboard link over the port 8787\n",
    "print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2654bbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLUSTER DEPLOYMENT ON CLOUDVENETO\n",
    "from dask.distributed import Client, SSHCluster\n",
    "\n",
    "cluster = SSHCluster(\n",
    "    [\"10.67.22.154\", \"10.67.22.216\", \"10.67.22.116\", \"10.67.22.113\"],\n",
    "    connect_options={\"known_hosts\": None},\n",
    "    remote_python=\"/home/ubuntu/miniconda3/bin/python\",\n",
    "    scheduler_options={\"port\": 8786, \"dashboard_address\": \":8797\"},\n",
    "    worker_options={\n",
    "        \"nprocs\": 1,     \n",
    "        \"nthreads\": 1  \n",
    "    }\n",
    ")\n",
    "\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd8c5194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://127.0.0.1:42685' processes=4 threads=4, memory=13.49 GiB>\n"
     ]
    }
   ],
   "source": [
    "# check if everything went smoothly\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eee55fb",
   "metadata": {},
   "source": [
    "Import the necessary stuff along with the California housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7495557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Dask partitions: 4\n",
      "Length of each partition: 5160 rows\n",
      "Length of the whole dataset: 20640 rows\n"
     ]
    }
   ],
   "source": [
    "import dask.array as da\n",
    "import dask\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Download California Housing dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "\n",
    "# Convert features into Dask Array (it's a matrix).\n",
    "n_partition = 3        # number of partition in memory. We have 4 VMS (1 master + 3 workers), so let's start with just 3 partitions\n",
    "length_partition = data.data.shape[0] // n_partition\n",
    "X_da = da.from_array(data.data.values, chunks=(length_partition, data.data.shape[1]))\n",
    "\n",
    "print(\"Number of Dask partitions:\",  X_da.npartitions) \n",
    "print(\"Length of each partition:\", length_partition, \"rows\")\n",
    "print(\"Length of the whole dataset:\", data.data.shape[0], \"rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17254d35",
   "metadata": {},
   "source": [
    "Now we'll define the parallel and serial algorithm for the Cholesky QR decomposition.\n",
    "\n",
    "This first parallel version of the Cholesky method works as follows:\n",
    "1) The array should already be splitted by rows in partitions across workers (let's call each partition $A_p$). Each worker computes a local version of $A^T A$, i.e. $A_p^T A_p$. Since $A_p$ is smaller than $A$, the matrix multiplication should proceed faster. Furthermore, $A_p$ being smaller may fully reside in the RAM of a worker\n",
    "2) Once each worker has finished, the full Gram matrix $A^T A$ is computed in a single worker by summing up all the smaller and local $A_p$: $A^T A = \\sum_p A_P^T A_p$\n",
    "3) The matrix $A^T A$ is small, $n\\times n$. A serial Cholesky decomposition is performed and will output the final $R$ matrix\n",
    "4) To get $Q$, we will use the defining equation $A = QR \\Rightarrow Q = A R^{-1}$. Computing the inverse of $R$ is straightforward and can be done by a single worker, whereas the MatMul between $A$ and the inverse of $R$ can be parallelized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a795cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_choleskyQR_parallel(X_da : dask.array.Array):\n",
    "    # A list of delayed tasks for each partition of the dataset\n",
    "    # Each partition computes the local Gram matrix (as a delayed task)\n",
    "    chunks_delayed = [dask.delayed(lambda x : x.T @ x)(chunk) for chunk in X_da.to_delayed().ravel()]\n",
    "\n",
    "    # Now sum all the local Gram matrices to get the global Gram matrix\n",
    "    Gram_global_delayed = dask.delayed(sum)(chunks_delayed)   ## !! This is not strictly parallel, meaning that a single worker will perform the sum instead of a tree-like operation. This is ok here, I guess, since we only have 8 chunks that need to be summed up\n",
    "\n",
    "    # Compute R as the Cholesky decomposition on the global Gram matrix (as a delayed even if a serial operation just call .compute at the end)\n",
    "    R = dask.delayed(np.linalg.cholesky)(Gram_global_delayed)\n",
    "    #R.visualize(\"fig/CholeskyR.png\")\n",
    "    R = R.compute() # Compute R. This will put a stop at the parallel operation\n",
    "    R_inv = np.linalg.inv(R) # It's a small matrix, so this operation is fast even if serial\n",
    "\n",
    "    Q = X_da.map_blocks(lambda block: block @ R_inv, dtype=X_da.dtype)\n",
    "    #Q.visualize(\"fig/CholeskyQ.png\")\n",
    "    Q = Q.compute() # Compute Q\n",
    "    return Q, R\n",
    "\n",
    "def compute_choleskyQR_serial(X):\n",
    "    # Global gram matrix\n",
    "    G = X.T @ X\n",
    "    R = np.linalg.cholesky(G)\n",
    "    R_inv = np.linalg.inv(R)\n",
    "    Q = X @ R_inv\n",
    "    \n",
    "    return Q, R\n",
    "\n",
    "def compute_choleskyR_parallel(X_da : dask.array.Array):\n",
    "    # A list of delayed tasks for each partition of the dataset\n",
    "    # Each partition computes the local Gram matrix\n",
    "    chunks_delayed = [dask.delayed(lambda x : x.T @ x)(chunk) for chunk in X_da.to_delayed().ravel()]\n",
    "    # Now sum all the local Gram matrices to get the global Gram matrix\n",
    "    Gram_global_delayed = dask.delayed(sum)(chunks_delayed)\n",
    "    # Compute R as the Cholesky decomposition on the global Gram matrix (as a delayed even if a serial operation just call .compute at the end)\n",
    "    R = dask.delayed(np.linalg.cholesky)(Gram_global_delayed)\n",
    "    R = R.compute() # Compute R\n",
    "    return  R\n",
    "\n",
    "def compute_choleskyR_serial(X):\n",
    "    # Global gram matrix\n",
    "    G = X.T @ X\n",
    "    R = np.linalg.cholesky(G)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09255965",
   "metadata": {},
   "source": [
    "The DAG should look like (for the computation of R)\n",
    "\n",
    "\n",
    "![](fig/CholeskyR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abb0291",
   "metadata": {},
   "source": [
    "Let's measure the time it takes to perform the parallel Cholesky QR decomposition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a094b150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.1 ms, sys: 8.66 ms, total: 52.8 ms\n",
      "Wall time: 65 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# parallel\n",
    "Q_p, R_p = compute_choleskyQR_parallel(X_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046bac1a",
   "metadata": {},
   "source": [
    "As of now, we have 3 VMs and we specifically asked Dask to only create one worker per node, thus we have deployed 3 workers. Accessing the dashboard, we can see what happens under the hood:\n",
    "\n",
    "![](fig/CloudVeneto_Cal_3workers.png)\n",
    "\n",
    "Since we have 3 workers, we will see three horizontal stripes corresponding to each worker. The first three bands (green-ish) are labeled *\"array\"* by Dask and are probably related to array accessing and reading. The following parallel blocks (three of them, as expected) represents the *lambda* function, i.e. the local matrix multiplication. The red block (and the following yellowe one) is performed on a single worker as requested. They represent the serial sum: a single worker collects all the temporary Gram matrices (red block) and perform the sum operation (yellow). All the gaps in between the colored bands represent the Dask overhead (orchestration, ...) which, in this specific case, looks like a lot of time. In fact, running the same algorithm in a serial fashion: \n",
    "\n",
    "Workers were idle most of the time. Additionally, there are some long transfers (red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96e57eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 822 µs, sys: 721 µs, total: 1.54 ms\n",
      "Wall time: 847 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# serial\n",
    "Q_s, R_s = compute_choleskyQR_serial(data.data.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad19947e",
   "metadata": {},
   "source": [
    "However, the serial algorithm is faster than the parallel one. This is probably because the California dataset is not that big (only $20k$ rows, easily fittable in the RAM). As of now, we still can't see a real speedup\n",
    "\n",
    "Cholesky QR is sadly known to be unstable. In fact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50244094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see whether the results are compatible\n",
    "diffR = np.linalg.norm(R_p - R_s, 2)\n",
    "diffQ = np.linalg.norm(Q_p - Q_s, 2)\n",
    "print(f\"||R_parallel - R_serial||_2 = {diffR}\")\n",
    "print(f\"||Q_parallel - Q_serial||_2 = {diffQ}\")\n",
    "\n",
    "# Check orthogonality of Q\n",
    "orthogonality_metric = np.linalg.norm(Q_s.T @ Q_s - np.eye(Q_s.shape[1]), 2)\n",
    "print(f\"||Q^T @ Q- I||_2 = {orthogonality_metric}\")\n",
    "# Check decomposition\n",
    "decomp_metric = np.linalg.norm(data.data.values - Q_s @ R_s, 2)\n",
    "print(f\"||X - Q @ R||_2 = {decomp_metric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8f3404",
   "metadata": {},
   "source": [
    "As expected, the decomposition yielded a non reasonnable result (Q is not orthogonal, the algorithm is highly unstable)\n",
    "\n",
    "\n",
    "Let's try with a different and larger dataset (HIGGS dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bdb666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# A huge dataset\n",
    "df = dd.read_csv(\"HIGGS.csv\", header=None, blocksize=\"400MB\")\n",
    "X_df = df.iloc[:, 1:] \n",
    "X_da = X_df.to_dask_array(lengths=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06face3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Q, R = compute_choleskyQR_parallel(X_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c1ebfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envLabComp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
