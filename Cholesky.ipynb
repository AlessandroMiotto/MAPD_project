{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04c09336",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lrizzi17/anaconda3/envs/envLabComp/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 38813 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:38813/status\n"
     ]
    }
   ],
   "source": [
    "# CLUSTER DEPLOYMENT, TO BE SUBSTITUED WHEN GOING CLUSTER\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# For now, local deployment on my computer (multicore)\n",
    "ncore = 4\n",
    "cluster = LocalCluster(n_workers=ncore, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "\n",
    "# Print the dashboard link over the port 8787\n",
    "print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2654bbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLUSTER DEPLOYMENT ON CLOUDVENETO\n",
    "from dask.distributed import Client, SSHCluster\n",
    "\n",
    "cluster = SSHCluster(\n",
    "    [\"10.67.22.154\", \"10.67.22.216\", \"10.67.22.116\", \"10.67.22.113\"],\n",
    "    connect_options={\"known_hosts\": None},\n",
    "    scheduler_options={\"port\": 8786, \"dashboard_address\": \":8797\"}\n",
    ")\n",
    "\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cd8c5194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://127.0.0.1:34793' processes=4 threads=4, memory=13.49 GiB>\n"
     ]
    }
   ],
   "source": [
    "# check if everything went smoothly\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b71b50",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LocalCluster' object has no attribute 'alt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcluster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malt\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LocalCluster' object has no attribute 'alt'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2eee55fb",
   "metadata": {},
   "source": [
    "Import the necessary stuff along with the California housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7495557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Dask partitions: 4\n",
      "Length of each partition: 5160 rows\n",
      "Length of the whole dataset: 20640 rows\n"
     ]
    }
   ],
   "source": [
    "import dask.array as da\n",
    "import dask\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Download California Housing dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "\n",
    "# Convert features into Dask Array (it's a matrix). N partition = 16\n",
    "n_partition = 4        # number of partition in memory\n",
    "length_partition = data.data.shape[0] // n_partition\n",
    "X_da = da.from_array(data.data.values, chunks=(length_partition, data.data.shape[1]))\n",
    "\n",
    "print(\"Number of Dask partitions:\",  X_da.npartitions) \n",
    "print(\"Length of each partition:\", length_partition, \"rows\")\n",
    "print(\"Length of the whole dataset:\", data.data.shape[0], \"rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17254d35",
   "metadata": {},
   "source": [
    "Now we'll define the parallel and serial algorithm for the Cholesky QR decomposition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a795cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_choleskyQR_parallel(X_da : dask.array.Array):\n",
    "    # A list of delayed tasks for each partition of the dataset\n",
    "    # Each partition computes the local Gram matrix\n",
    "    chunks_delayed = [dask.delayed(lambda x : x.T @ x)(chunk) for chunk in X_da.to_delayed().ravel()]\n",
    "\n",
    "    # Now sum all the local Gram matrices to get the global Gram matrix\n",
    "    Gram_global_delayed = dask.delayed(sum)(chunks_delayed)   ## !! This is not strictly parallel, meaning that a single worker will perform the sum instead of a tree-like operation. This is ok here, I guess, since we only have 8 chunks that need to be summed up\n",
    "\n",
    "    # Compute R as the Cholesky decomposition on the global Gram matrix (as a delayed even if a serial operation just call .compute at the end)\n",
    "    R = dask.delayed(np.linalg.cholesky)(Gram_global_delayed)\n",
    "    #R.visualize(\"fig/CholeskyR.png\")\n",
    "    R = R.compute() # Compute R. This will put a stop at the parallel operation\n",
    "    R_inv = np.linalg.inv(R) # It's a small matrix, so this operation is fast even if serial\n",
    "\n",
    "    Q = X_da.map_blocks(lambda block: block @ R_inv, dtype=X_da.dtype)\n",
    "    #Q.visualize(\"fig/CholeskyQ.png\")\n",
    "    Q = Q.compute() # Compute Q\n",
    "    return Q, R\n",
    "\n",
    "def compute_choleskyQR_serial(X):\n",
    "    # Global gram matrix\n",
    "    G = X.T @ X\n",
    "    R = np.linalg.cholesky(G)\n",
    "    R_inv = np.linalg.inv(R)\n",
    "    Q = X @ R_inv\n",
    "    \n",
    "    return Q, R\n",
    "\n",
    "def compute_choleskyR_parallel(X_da : dask.array.Array):\n",
    "    # A list of delayed tasks for each partition of the dataset\n",
    "    # Each partition computes the local Gram matrix\n",
    "    chunks_delayed = [dask.delayed(lambda x : x.T @ x)(chunk) for chunk in X_da.to_delayed().ravel()]\n",
    "    # Now sum all the local Gram matrices to get the global Gram matrix\n",
    "    Gram_global_delayed = dask.delayed(sum)(chunks_delayed)\n",
    "    # Compute R as the Cholesky decomposition on the global Gram matrix (as a delayed even if a serial operation just call .compute at the end)\n",
    "    R = dask.delayed(np.linalg.cholesky)(Gram_global_delayed)\n",
    "    R = R.compute() # Compute R\n",
    "    return  R\n",
    "\n",
    "def compute_choleskyR_serial(X):\n",
    "    # Global gram matrix\n",
    "    G = X.T @ X\n",
    "    R = np.linalg.cholesky(G)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abb0291",
   "metadata": {},
   "source": [
    "Let's measure the time it takes to perform the Cholesky QR decomposition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a094b150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 90.9 ms, sys: 28.9 ms, total: 120 ms\n",
      "Wall time: 105 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# parallel\n",
    "Q_p, R_p = compute_choleskyQR_parallel(X_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ccd279",
   "metadata": {},
   "source": [
    "The measured wall time is smaller than the total (cpu+system) time, meaning that the program went parallel (and in fact, the ratio wall / total should converge to n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "96e57eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.38 ms, sys: 435 Âµs, total: 3.81 ms\n",
      "Wall time: 2.38 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# serial\n",
    "Q_s, R_s = compute_choleskyQR_serial(data.data.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad19947e",
   "metadata": {},
   "source": [
    "Here again, wall time is smaller than total time. This is because even when using serial Numpy function, under the hood the BLAS implementation of certain algorithm leverages multithreading\n",
    "\n",
    "However, the serial algorithm is faster than the parallel one. This is probably because the California dataset is not that big (only $20k$ rows, easily fittable in the RAM). As of now, we still can't see a real speedup\n",
    "\n",
    "Cholesky QR is sadly known to be unstable. In fact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50244094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||R_parallel - R_serial||_2 = 6.6703992450451735e-09\n",
      "||Q_parallel - Q_serial||_2 = 5.281864770776042e-10\n",
      "||Q^T @ Q- I||_2 = 7971678.680290628\n",
      "||X - Q @ R||_2 = 2.838161229486053e-10\n"
     ]
    }
   ],
   "source": [
    "# Let's see whether the results are compatible\n",
    "diffR = np.linalg.norm(R_p - R_s, 2)\n",
    "diffQ = np.linalg.norm(Q_p - Q_s, 2)\n",
    "print(f\"||R_parallel - R_serial||_2 = {diffR}\")\n",
    "print(f\"||Q_parallel - Q_serial||_2 = {diffQ}\")\n",
    "\n",
    "# Check orthogonality of Q\n",
    "orthogonality_metric = np.linalg.norm(Q_s.T @ Q_s - np.eye(Q_s.shape[1]), 2)\n",
    "print(f\"||Q^T @ Q- I||_2 = {orthogonality_metric}\")\n",
    "# Check decomposition\n",
    "decomp_metric = np.linalg.norm(data.data.values - Q_s @ R_s, 2)\n",
    "print(f\"||X - Q @ R||_2 = {decomp_metric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8f3404",
   "metadata": {},
   "source": [
    "As expected, the decomposition yielded a non reasonnable result (Q is not orthogonal, the algorithm is highly unstable)\n",
    "\n",
    "\n",
    "Let's try with a different and larger dataset (HIGGS dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75bdb666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-27 11:08:01,683 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 1.07 GiB -- Worker memory limit: 1.69 GiB\n",
      "2025-08-27 11:08:01,801 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 1.07 GiB -- Worker memory limit: 1.69 GiB\n",
      "2025-08-27 11:08:01,824 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 1.07 GiB -- Worker memory limit: 1.69 GiB\n",
      "2025-08-27 11:08:01,838 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 1.07 GiB -- Worker memory limit: 1.69 GiB\n",
      "2025-08-27 11:08:01,843 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 1.07 GiB -- Worker memory limit: 1.69 GiB\n",
      "2025-08-27 11:08:01,974 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 1.07 GiB -- Worker memory limit: 1.69 GiB\n",
      "2025-08-27 11:08:02,334 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 1.06 GiB -- Worker memory limit: 1.69 GiB\n",
      "2025-08-27 11:08:02,409 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 1.04 GiB -- Worker memory limit: 1.69 GiB\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# A huge dataset\n",
    "df = dd.read_csv(\"HIGGS.csv\", header=None, blocksize=\"400MB\")\n",
    "X_df = df.iloc[:, 1:] \n",
    "X_da = X_df.to_dask_array(lengths=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06face3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Q, R = compute_choleskyQR_parallel(X_da)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envLabComp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
