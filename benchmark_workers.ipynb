{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a005811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import cholesky_tsqr, direct_tsqr, indirect_tsqr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ecb4e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from dask.distributed import wait\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdacfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard: http://127.0.0.1:8787/status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-14 16:17:27,418 - tornado.application - ERROR - Uncaught exception GET /status/ws (127.0.0.1)\n",
      "HTTPServerRequest(protocol='http', host='127.0.0.1:8787', method='GET', uri='/status/ws', version='HTTP/1.1', remote_ip='127.0.0.1')\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/riccorte/miniconda3/envs/dask-env/lib/python3.11/site-packages/tornado/websocket.py\", line 965, in _accept_connection\n",
      "    open_result = handler.open(*handler.open_args, **handler.open_kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/riccorte/miniconda3/envs/dask-env/lib/python3.11/site-packages/tornado/web.py\", line 3375, in wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/riccorte/miniconda3/envs/dask-env/lib/python3.11/site-packages/bokeh/server/views/ws.py\", line 149, in open\n",
      "    raise ProtocolError(\"Token is expired. Configure the app with a larger value for --session-token-expiration if necessary\")\n",
      "bokeh.protocol.exceptions.ProtocolError: Token is expired. Configure the app with a larger value for --session-token-expiration if necessary\n",
      "2025-09-14 16:17:33,093 - tornado.application - ERROR - Uncaught exception GET /status/ws (127.0.0.1)\n",
      "HTTPServerRequest(protocol='http', host='127.0.0.1:8787', method='GET', uri='/status/ws', version='HTTP/1.1', remote_ip='127.0.0.1')\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/riccorte/miniconda3/envs/dask-env/lib/python3.11/site-packages/tornado/websocket.py\", line 965, in _accept_connection\n",
      "    open_result = handler.open(*handler.open_args, **handler.open_kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/riccorte/miniconda3/envs/dask-env/lib/python3.11/site-packages/tornado/web.py\", line 3375, in wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/riccorte/miniconda3/envs/dask-env/lib/python3.11/site-packages/bokeh/server/views/ws.py\", line 149, in open\n",
      "    raise ProtocolError(\"Token is expired. Configure the app with a larger value for --session-token-expiration if necessary\")\n",
      "bokeh.protocol.exceptions.ProtocolError: Token is expired. Configure the app with a larger value for --session-token-expiration if necessary\n",
      "2025-09-14 16:17:42,259 - tornado.application - ERROR - Uncaught exception GET /status/ws (127.0.0.1)\n",
      "HTTPServerRequest(protocol='http', host='127.0.0.1:8787', method='GET', uri='/status/ws', version='HTTP/1.1', remote_ip='127.0.0.1')\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/riccorte/miniconda3/envs/dask-env/lib/python3.11/site-packages/tornado/websocket.py\", line 965, in _accept_connection\n",
      "    open_result = handler.open(*handler.open_args, **handler.open_kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/riccorte/miniconda3/envs/dask-env/lib/python3.11/site-packages/tornado/web.py\", line 3375, in wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/riccorte/miniconda3/envs/dask-env/lib/python3.11/site-packages/bokeh/server/views/ws.py\", line 149, in open\n",
      "    raise ProtocolError(\"Token is expired. Configure the app with a larger value for --session-token-expiration if necessary\")\n",
      "bokeh.protocol.exceptions.ProtocolError: Token is expired. Configure the app with a larger value for --session-token-expiration if necessary\n",
      "2025-09-14 16:17:59,174 - tornado.application - ERROR - Uncaught exception GET /status/ws (127.0.0.1)\n",
      "HTTPServerRequest(protocol='http', host='127.0.0.1:8787', method='GET', uri='/status/ws', version='HTTP/1.1', remote_ip='127.0.0.1')\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/riccorte/miniconda3/envs/dask-env/lib/python3.11/site-packages/tornado/websocket.py\", line 965, in _accept_connection\n",
      "    open_result = handler.open(*handler.open_args, **handler.open_kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/riccorte/miniconda3/envs/dask-env/lib/python3.11/site-packages/tornado/web.py\", line 3375, in wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/riccorte/miniconda3/envs/dask-env/lib/python3.11/site-packages/bokeh/server/views/ws.py\", line 149, in open\n",
      "    raise ProtocolError(\"Token is expired. Configure the app with a larger value for --session-token-expiration if necessary\")\n",
      "bokeh.protocol.exceptions.ProtocolError: Token is expired. Configure the app with a larger value for --session-token-expiration if necessary\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# Example: 4 workers, 1 thread each\n",
    "cluster = LocalCluster(n_workers=4, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "\n",
    "print(\"Dashboard:\", client.dashboard_link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10a08da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://127.0.0.1:38757' processes=4 threads=4, memory=5.79 GiB>\n",
      "LocalCluster(b41228c5, 'tcp://127.0.0.1:38757', workers=4, threads=4, memory=5.79 GiB)\n"
     ]
    }
   ],
   "source": [
    "# check if everything went smoothly\n",
    "print(client)\n",
    "print(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e189b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Dask partitions: 3\n",
      "Length of each partition: 6880 rows\n",
      "Length of the whole dataset: 20640 rows\n"
     ]
    }
   ],
   "source": [
    "import dask.array as da\n",
    "import dask\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.linalg import solve_triangular \n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Download California Housing dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "\n",
    "# Convert features into Dask Array (it's a matrix).\n",
    "n_partition = 3        # number of partition in memory. We have 4 VMS (1 master + 3 workers), so let's start with just 3 partitions\n",
    "length_partition = data.data.shape[0] // n_partition\n",
    "X_da = da.from_array(data.data.values, chunks=(length_partition, data.data.shape[1]))\n",
    "\n",
    "print(\"Number of Dask partitions:\",  X_da.npartitions) \n",
    "print(\"Length of each partition:\", length_partition, \"rows\")\n",
    "print(\"Length of the whole dataset:\", data.data.shape[0], \"rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171e3246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dask.array<array, shape=(20640, 8), dtype=float64, chunksize=(6880, 8), chunktype=numpy.ndarray>\n"
     ]
    }
   ],
   "source": [
    "#X_cached = X_da.rechunk((1_000_000, -1)).persist()\n",
    "\n",
    "print(X_da)\n",
    "A0 = X_da.rechunk((1_000_000, -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "defc3989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from dask.distributed import wait\n",
    "from time import sleep\n",
    "\n",
    "def ensure_scaled(target, timeout=60):\n",
    "    \"\"\"Request scaling and wait until at least `target` workers are available.\n",
    "       Returns the actual number of workers available (may be > target).\"\"\"\n",
    "    cluster.scale(target)\n",
    "    t0 = time.time()\n",
    "    while time.time() - t0 < timeout:\n",
    "        info = client.scheduler_info()\n",
    "        nworkers = len(info.get(\"workers\", {}))\n",
    "        if nworkers >= target:\n",
    "            return nworkers\n",
    "        sleep(0.5)\n",
    "    # timeout expired â€” return current state\n",
    "    return len(client.scheduler_info().get(\"workers\", {}))\n",
    "\n",
    "def print_workers_info():\n",
    "    info = client.scheduler_info().get(\"workers\", {})\n",
    "    for addr, w in info.items():\n",
    "        print(addr, \"nthreads:\", w.get(\"nthreads\"), \"processing:\", len(w.get(\"processing\", {})),\n",
    "              \"memory:\", w.get(\"memory_limit\"))\n",
    "\n",
    "def run_once_indirect(A):\n",
    "    t0 = time.time()\n",
    "    Q, R = indirect_tsqr(A)\n",
    "    Q = Q.persist()\n",
    "    try:\n",
    "        wait(Q, timeout=120)    # avoid hanging forever\n",
    "    except Exception as e:\n",
    "        print(\"wait(Q) raised:\", repr(e))\n",
    "        # show scheduler state for debugging\n",
    "        print_workers_info()\n",
    "        raise\n",
    "    t1 = time.time()\n",
    "    client.cancel(Q)\n",
    "    return t1 - t0\n",
    "\n",
    "def time_vs_workers(workers_list, reps=2):\n",
    "    results = {}\n",
    "    for w in workers_list:\n",
    "        actual = ensure_scaled(w, timeout=120)\n",
    "        print(f\"\\nRequested {w}, actual available {actual}\")\n",
    "        print_workers_info()\n",
    "\n",
    "        # adapt chunking to available workers so tasks can be distributed\n",
    "        # here we assume A0 has its first axis as the partitioning axis\n",
    "        try:\n",
    "            n_rows = int(A0.shape[0])\n",
    "            rows_per_chunk = (n_rows + actual - 1) // actual  # ceil division\n",
    "            A = A0.rechunk((rows_per_chunk, -1)).persist()\n",
    "            wait(A, timeout=120)\n",
    "        except Exception as e:\n",
    "            print(\"Error persisting A after rechunk:\", repr(e))\n",
    "            print_workers_info()\n",
    "            # Try a simpler test to see if workers can do anything:\n",
    "            test = client.submit(lambda x: x+1, 1)\n",
    "            print(\"simple submit result:\", test.result(timeout=10))\n",
    "            raise\n",
    "\n",
    "        # Warmup + measures\n",
    "        _ = run_once_indirect(A)\n",
    "        times = [run_once_indirect(A) for _ in range(reps)]\n",
    "        results[w] = (float(np.mean(times)), float(np.std(times)))\n",
    "        print(f\"w={w}: mean={results[w][0]:.3f}s  std={results[w][1]:.3f}s\")\n",
    "\n",
    "        # cleanup\n",
    "        client.cancel(A)\n",
    "        del A\n",
    "        # optionally scale down between runs:\n",
    "        # cluster.scale(0)\n",
    "        # sleep(2)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95e298c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requested 1, actual available 4\n",
      "tcp://127.0.0.1:36993 nthreads: 1 processing: 0 memory: 1554740224\n",
      "w=1: mean=0.237s  std=0.049s\n",
      "\n",
      "Requested 2, actual available 2\n",
      "tcp://127.0.0.1:36993 nthreads: 1 processing: 0 memory: 1554740224\n",
      "tcp://127.0.0.1:41841 nthreads: 1 processing: 0 memory: 1554740224\n",
      "w=2: mean=0.113s  std=0.001s\n",
      "\n",
      "Requested 3, actual available 3\n",
      "tcp://127.0.0.1:36993 nthreads: 1 processing: 0 memory: 1554740224\n",
      "tcp://127.0.0.1:41841 nthreads: 1 processing: 0 memory: 1554740224\n",
      "tcp://127.0.0.1:46247 nthreads: 1 processing: 0 memory: 1554740224\n",
      "w=3: mean=0.136s  std=0.021s\n",
      "\n",
      "Requested 4, actual available 4\n",
      "tcp://127.0.0.1:36993 nthreads: 1 processing: 0 memory: 1554740224\n",
      "tcp://127.0.0.1:41841 nthreads: 1 processing: 0 memory: 1554740224\n",
      "tcp://127.0.0.1:42393 nthreads: 1 processing: 0 memory: 1554740224\n",
      "tcp://127.0.0.1:46247 nthreads: 1 processing: 0 memory: 1554740224\n",
      "w=4: mean=0.197s  std=0.024s\n",
      "\n",
      "Risultati: {1: (0.23735105991363525, 0.04942309856414795), 2: (0.11348235607147217, 0.0014253854751586914), 3: (0.13585007190704346, 0.020914196968078613), 4: (0.1970146894454956, 0.023967385292053223)}\n"
     ]
    }
   ],
   "source": [
    "# ESEMPIO: misura con 1, 2, 4, 8 worker (se la macchina lo consente)\n",
    "res = time_vs_workers([1,2,3, 4], reps=2)\n",
    "print(\"\\nRisultati:\", res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65020466",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dask-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
