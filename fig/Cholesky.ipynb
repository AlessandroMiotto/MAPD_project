{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c193d167",
   "metadata": {},
   "source": [
    "## Cholesky QR decomposition of a Tall and Skinny matrix\n",
    "\n",
    "N.B. This is not the direct TSQR method proposed in the article. The Cholesky-based approach lacks precision and numerical stability, making it impractical for high-performance environments. Nevertheless, we found it interesting to implement it as an exercise, exploring the Dask implementation and benchmarking it, despite its inherent numerical instability\n",
    "\n",
    "Let $B$ be a symmetric and positive definite $n\\times n$ matrix. Then its Cholesky decomposition is:\n",
    "$$\n",
    "B = L L^T \n",
    "$$\n",
    "where $L$ is a lower triangular $n\\times n$ matrix. Cholesky decomposition turns out to be particularly useful when computing the QR decomposition of a $m\\times n$ matrix $A$. Let's first build the temporary matrix $T = A^T A$, symmetric by definition. Hence, if:\n",
    "$$\n",
    "A = QR\n",
    "$$\n",
    "then:\n",
    "$$\n",
    "T = A^T A = (QR)^T(QR) = R^T Q^T Q R\n",
    "$$\n",
    "The matrix $Q$ is orthogonal, so $T = R^T R$. Since $R$ is a triangular matrix too, then we have effectively found the Cholesky decomposition of $T$ in terms of $R$. In other words, the Cholesky QR decomposition proceeds as follow:\n",
    "1) Given $A$, build the symmetric matrix $T = A^T A$\n",
    "2) Apply the Cholesky decomposition on $ T = L L^T$\n",
    "3) Obtain $R = L^T$. Obtain Q by solving $A = QR$\n",
    "\n",
    "This procedure is numerically unstable, and for certain matrices $A$ it may fail to produce accurate results, particularly for the orthogonal matrix $Q$. However, it is easily parallelizable, making it a useful testbed for experimenting with Dask.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04c09336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:8787/status\n"
     ]
    }
   ],
   "source": [
    "# CLUSTER DEPLOYMENT, TO BE EXECUTED ONLY IN A LOCAL ENVIRONMENT!!\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# For now, local deployment on my computer (multicore)\n",
    "ncore = 4\n",
    "cluster = LocalCluster(n_workers=ncore, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "\n",
    "# Print the dashboard link over the port 8787\n",
    "print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2654bbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 07:40:01,974 - distributed.deploy.ssh - INFO - 2025-09-09 07:40:01,974 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "2025-09-09 07:40:01,994 - distributed.deploy.ssh - INFO - 2025-09-09 07:40:01,994 - distributed.scheduler - INFO - State start\n",
      "2025-09-09 07:40:01,995 - distributed.deploy.ssh - INFO - 2025-09-09 07:40:01,995 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/scheduler-fgzafylc', purging\n",
      "2025-09-09 07:40:01,998 - distributed.deploy.ssh - INFO - 2025-09-09 07:40:01,998 - distributed.scheduler - INFO -   Scheduler at:   tcp://10.67.22.154:8786\n",
      "/home/ubuntu/miniconda3/lib/python3.13/site-packages/distributed/deploy/ssh.py:100: FutureWarning: The nprocs argument will be removed in a future release. It has been renamed to n_workers.\n",
      "  warnings.warn(\n",
      "2025-09-09 07:40:03,289 - distributed.deploy.ssh - INFO - 2025-09-09 07:40:03,288 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.116:44889'\n",
      "2025-09-09 07:40:03,292 - distributed.deploy.ssh - INFO - 2025-09-09 07:40:03,283 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.113:45477'\n",
      "2025-09-09 07:40:03,483 - distributed.deploy.ssh - INFO - 2025-09-09 07:40:03,477 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.216:35883'\n",
      "2025-09-09 07:40:03,608 - distributed.deploy.ssh - INFO - 2025-09-09 07:40:03,606 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.116:40923\n",
      "2025-09-09 07:40:03,615 - distributed.deploy.ssh - INFO - 2025-09-09 07:40:03,606 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.113:37979\n",
      "2025-09-09 07:40:03,870 - distributed.deploy.ssh - INFO - 2025-09-09 07:40:03,864 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.216:38855\n"
     ]
    }
   ],
   "source": [
    "# CLUSTER DEPLOYMENT ON CLOUDVENETO\n",
    "from dask.distributed import Client, SSHCluster\n",
    "\n",
    "cluster = SSHCluster(\n",
    "    [\"10.67.22.154\", \"10.67.22.216\", \"10.67.22.116\", \"10.67.22.113\"],\n",
    "    connect_options={\"known_hosts\": None},\n",
    "    remote_python=\"/home/ubuntu/miniconda3/bin/python\",\n",
    "    scheduler_options={\"port\": 8786, \"dashboard_address\": \":8797\"},\n",
    "    worker_options={\n",
    "        \"nprocs\": 1,     \n",
    "        \"nthreads\": 1  \n",
    "    }\n",
    ")\n",
    "\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cd8c5194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://10.67.22.154:8786' processes=3 threads=3, memory=5.81 GiB>\n"
     ]
    }
   ],
   "source": [
    "# check if everything went smoothly\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eee55fb",
   "metadata": {},
   "source": [
    "Import the necessary stuff along with the California housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f7495557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Dask partitions: 3\n",
      "Length of each partition: 6880 rows\n",
      "Length of the whole dataset: 20640 rows\n"
     ]
    }
   ],
   "source": [
    "import dask.array as da\n",
    "import dask\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Download California Housing dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "\n",
    "# Convert features into Dask Array (it's a matrix).\n",
    "n_partition = 3        # number of partition in memory. We have 4 VMS (1 master + 3 workers), so let's start with just 3 partitions\n",
    "length_partition = data.data.shape[0] // n_partition\n",
    "X_da = da.from_array(data.data.values, chunks=(length_partition, data.data.shape[1]))\n",
    "\n",
    "print(\"Number of Dask partitions:\",  X_da.npartitions) \n",
    "print(\"Length of each partition:\", length_partition, \"rows\")\n",
    "print(\"Length of the whole dataset:\", data.data.shape[0], \"rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17254d35",
   "metadata": {},
   "source": [
    "Now we'll define the parallel and serial algorithm for the Cholesky QR decomposition.\n",
    "\n",
    "This first parallel version of the Cholesky method works as follows:\n",
    "1) The array should already be splitted by rows in partitions across workers (let's call each partition $A_p$). Each worker computes a local version of $A^T A$, i.e. $A_p^T A_p$. Since $A_p$ is smaller than $A$, the matrix multiplication should proceed faster. Furthermore, $A_p$ being smaller may fully reside in the RAM of a worker\n",
    "2) Once each worker has finished, the full Gram matrix $A^T A$ is computed in a single worker by summing up all the smaller and local $A_p$: $A^T A = \\sum_p A_P^T A_p$\n",
    "3) The matrix $A^T A$ is small, $n\\times n$. A serial Cholesky decomposition is performed and will output the final $R$ matrix\n",
    "4) To get $Q$, we will use the defining equation $A = QR \\Rightarrow Q = A R^{-1}$. Computing the inverse of $R$ is straightforward and can be done by a single worker, whereas the MatMul between $A$ and the inverse of $R$ can be parallelized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5a795cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_choleskyQR_parallel(X_da : dask.array.Array):\n",
    "    # X_da.persist()\n",
    "    # A list of delayed tasks for each partition of the dataset\n",
    "    # Each partition computes the local Gram matrix (as a delayed task)\n",
    "    chunks_delayed = [dask.delayed(lambda x : x.T @ x)(chunk) for chunk in X_da.to_delayed().ravel()]\n",
    "\n",
    "    # Now sum all the local Gram matrices to get the global Gram matrix\n",
    "    Gram_global_delayed = dask.delayed(sum)(chunks_delayed)   ## !! This is not strictly parallel, meaning that a single worker will perform the sum instead of a tree-like operation. This is ok here, I guess, since we only have 8 chunks that need to be summed up\n",
    "\n",
    "    # Compute R as the Cholesky decomposition on the global Gram matrix (as a delayed even if a serial operation just call .compute at the end)\n",
    "    R = dask.delayed(np.linalg.cholesky)(Gram_global_delayed)\n",
    "    #R.visualize(\"fig/CholeskyR.png\")\n",
    "    R = R.compute() # Compute R. This will put a stop at the parallel operation\n",
    "    R_inv = np.linalg.inv(R) # It's a small matrix, so this operation is fast even if serial\n",
    "\n",
    "    Q = X_da.map_blocks(lambda block: block @ R_inv, dtype=X_da.dtype)\n",
    "    #Q.visualize(\"fig/CholeskyQ.png\")\n",
    "    Q = Q.compute() # Compute Q\n",
    "    return Q, R\n",
    "\n",
    "def compute_choleskyQR_serial(X):\n",
    "    # Global gram matrix\n",
    "    G = X.T @ X\n",
    "    R = np.linalg.cholesky(G)\n",
    "    R_inv = np.linalg.inv(R)\n",
    "    Q = X @ R_inv\n",
    "    \n",
    "    return Q, R\n",
    "\n",
    "def compute_choleskyR_parallel(X_da : dask.array.Array):\n",
    "    # A list of delayed tasks for each partition of the dataset\n",
    "    # Each partition computes the local Gram matrix\n",
    "    chunks_delayed = [dask.delayed(lambda x : x.T @ x)(chunk) for chunk in X_da.to_delayed().ravel()]\n",
    "    # Now sum all the local Gram matrices to get the global Gram matrix\n",
    "    Gram_global_delayed = dask.delayed(sum)(chunks_delayed)\n",
    "    # Compute R as the Cholesky decomposition on the global Gram matrix (as a delayed even if a serial operation just call .compute at the end)\n",
    "    R = dask.delayed(np.linalg.cholesky)(Gram_global_delayed)\n",
    "    R = R.compute() # Compute R\n",
    "    return  R\n",
    "\n",
    "def compute_choleskyR_serial(X):\n",
    "    # Global gram matrix\n",
    "    G = X.T @ X\n",
    "    R = np.linalg.cholesky(G)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09255965",
   "metadata": {},
   "source": [
    "The DAG should look like (for the computation of R)\n",
    "\n",
    "\n",
    "![](fig/CholeskyR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abb0291",
   "metadata": {},
   "source": [
    "Let's measure the time it takes to perform the parallel Cholesky QR decomposition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a094b150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.67 ms, sys: 0 ns, total: 8.67 ms\n",
      "Wall time: 66.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# parallel\n",
    "Q_p, R_p = compute_choleskyQR_parallel(X_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046bac1a",
   "metadata": {},
   "source": [
    "As of now, we have 3 VMs and we specifically asked Dask to only create one worker per node, thus we have deployed 3 workers. Accessing the dashboard, we can see what happens under the hood:\n",
    "\n",
    "![](fig/CloudVeneto_Cal_3workers.png)\n",
    "\n",
    "Since we have three workers, we can see three horizontal segments, each corresponding to a worker. The first three bands (greenish) are labeled _array_ by Dask and are related to array access and reading. This is because the dataset is stored in the scheduler VM.\n",
    "When workers need to access this data, the scheduler sends it over the network. If we had run _X_da.persist()_ prior to the function call, all the data would already have been stored in the workers' memory, and no additional time or transfer would have been required.\n",
    "\n",
    "The following parallel blocks (three, as expected) correspond to the lambda function, i.e., the local MatMul.\n",
    "\n",
    "The red block (followed by the yellow one) is executed on a single worker, as requested. These blocks represent the serial sum: a single worker collects all the temporary Gram matrices (red block) and performs the sum operation (yellow block).\n",
    "\n",
    "All the gaps between the colored bands represent Dask overhead (orchestration, scheduling, etc.), which, in this specific case, appears to consume a significant amount of time. This essentially means that workers were idle most of the time and long and slow transfers (red blocks) can be observed\n",
    "\n",
    "Indeed, running the same algorithm serially:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "96e57eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.04 ms, sys: 0 ns, total: 1.04 ms\n",
      "Wall time: 1.06 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# serial\n",
    "Q_s, R_s = compute_choleskyQR_serial(data.data.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad19947e",
   "metadata": {},
   "source": [
    "The serial implementation is much faster than the parallel one. This was, unfortunately, expected for several reasons:\n",
    "\n",
    "1) The dataset is relatively small (only $20k$ rows). It easily fits in the master's RAM, so there is really no need to create partitions and transfer them over the network. Numpy, which also uses multithreading internally, will certainly be faster in this case.\n",
    "2) The algorithm still has some limitations, mainly the bottleneck caused by the serial part: only one worker is responsible for summing all the local matrices.\n",
    "3) We used $n\\_partitions = n\\_workers$, which might not be the optimal configuration. Creating more partitions means that individual parallel operations are faster (because each data block is smaller), but it also means that a single worker may have to process multiple partitions, generating additional overhead.\n",
    "\n",
    "Furthermore, Cholesky QR is sadly known to be unstable. In fact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "50244094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||R_parallel - R_serial||_2 = 1.8265817285310699e-09\n",
      "||Q_parallel - Q_serial||_2 = 1.0852657367110989e-10\n",
      "||Q^T @ Q- I||_2 = 7971678.680289975\n",
      "||X - Q @ R||_2 = 8.723161201902093e-10\n"
     ]
    }
   ],
   "source": [
    "# Let's see whether the results are compatible\n",
    "diffR = np.linalg.norm(R_p - R_s, 2)\n",
    "diffQ = np.linalg.norm(Q_p - Q_s, 2)\n",
    "print(f\"||R_parallel - R_serial||_2 = {diffR}\")\n",
    "print(f\"||Q_parallel - Q_serial||_2 = {diffQ}\")\n",
    "\n",
    "# Check orthogonality of Q\n",
    "orthogonality_metric = np.linalg.norm(Q_s.T @ Q_s - np.eye(Q_s.shape[1]), 2)\n",
    "print(f\"||Q^T @ Q- I||_2 = {orthogonality_metric}\")\n",
    "# Check decomposition\n",
    "decomp_metric = np.linalg.norm(data.data.values - Q_s @ R_s, 2)\n",
    "print(f\"||X - Q @ R||_2 = {decomp_metric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8f3404",
   "metadata": {},
   "source": [
    "As expected, the decomposition yielded a non reasonnable result (Q is not orthogonal, the algorithm is highly unstable)\n",
    "\n",
    "## A larger dataset\n",
    "\n",
    "Let's try with a different and larger dataset (HIGGS dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bf589d60-3e8a-43ab-aafa-d7ec16a55bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 08:49:44,174 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:44,173 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "2025-09-09 08:49:44,193 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:44,193 - distributed.scheduler - INFO - State start\n",
      "2025-09-09 08:49:44,196 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:44,196 - distributed.scheduler - INFO -   Scheduler at:   tcp://10.67.22.154:8786\n",
      "/home/ubuntu/miniconda3/lib/python3.13/site-packages/distributed/deploy/ssh.py:100: FutureWarning: The nprocs argument will be removed in a future release. It has been renamed to n_workers.\n",
      "  warnings.warn(\n",
      "2025-09-09 08:49:45,702 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:45,704 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.113:33701'\n",
      "2025-09-09 08:49:45,705 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:45,707 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.113:40119'\n",
      "2025-09-09 08:49:45,707 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:45,709 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.113:36585'\n",
      "2025-09-09 08:49:45,708 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:45,710 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.113:44455'\n",
      "2025-09-09 08:49:45,709 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:45,709 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.116:44543'\n",
      "2025-09-09 08:49:45,711 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:45,711 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.116:41619'\n",
      "2025-09-09 08:49:45,717 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:45,717 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.116:39261'\n",
      "2025-09-09 08:49:45,718 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:45,718 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.116:40863'\n",
      "2025-09-09 08:49:46,033 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,035 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.113:34987\n",
      "2025-09-09 08:49:46,034 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,035 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.113:34987\n",
      "2025-09-09 08:49:46,035 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,035 - distributed.worker - INFO -          dashboard at:         10.67.22.113:35481\n",
      "2025-09-09 08:49:46,036 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,035 - distributed.worker - INFO - Waiting to connect to:    tcp://10.67.22.154:8786\n",
      "2025-09-09 08:49:46,037 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,035 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-09 08:49:46,037 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,035 - distributed.worker - INFO -               Threads:                          1\n",
      "2025-09-09 08:49:46,037 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,036 - distributed.worker - INFO -                Memory:                   1.94 GiB\n",
      "2025-09-09 08:49:46,038 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,036 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l3v4c6i2\n",
      "2025-09-09 08:49:46,038 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,036 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-09 08:49:46,039 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,034 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.116:46629\n",
      "2025-09-09 08:49:46,039 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,034 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.116:46629\n",
      "2025-09-09 08:49:46,040 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,034 - distributed.worker - INFO -          dashboard at:         10.67.22.116:37305\n",
      "2025-09-09 08:49:46,040 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,034 - distributed.worker - INFO - Waiting to connect to:    tcp://10.67.22.154:8786\n",
      "2025-09-09 08:49:46,041 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,034 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-09 08:49:46,041 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,035 - distributed.worker - INFO -               Threads:                          1\n",
      "2025-09-09 08:49:46,041 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,035 - distributed.worker - INFO -                Memory:                   1.94 GiB\n",
      "2025-09-09 08:49:46,042 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,035 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8v4c57ax\n",
      "2025-09-09 08:49:46,042 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,035 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-09 08:49:46,043 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,039 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.113:36593\n",
      "2025-09-09 08:49:46,043 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,039 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.113:36593\n",
      "2025-09-09 08:49:46,044 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,039 - distributed.worker - INFO -          dashboard at:         10.67.22.113:42201\n",
      "2025-09-09 08:49:46,044 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,039 - distributed.worker - INFO - Waiting to connect to:    tcp://10.67.22.154:8786\n",
      "2025-09-09 08:49:46,044 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,039 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-09 08:49:46,045 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,039 - distributed.worker - INFO -               Threads:                          1\n",
      "2025-09-09 08:49:46,045 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,039 - distributed.worker - INFO -                Memory:                   1.94 GiB\n",
      "2025-09-09 08:49:46,045 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,039 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u0saqjoc\n",
      "2025-09-09 08:49:46,046 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,039 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-09 08:49:46,047 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,045 - distributed.worker - INFO - Starting Worker plugin shuffle\n",
      "2025-09-09 08:49:46,048 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,046 - distributed.worker - INFO -         Registered to:    tcp://10.67.22.154:8786\n",
      "2025-09-09 08:49:46,048 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,046 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-09 08:49:46,048 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,046 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.113:33765\n",
      "2025-09-09 08:49:46,049 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,046 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.113:33765\n",
      "2025-09-09 08:49:46,049 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,046 - distributed.worker - INFO -          dashboard at:         10.67.22.113:43507\n",
      "2025-09-09 08:49:46,049 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,046 - distributed.worker - INFO - Waiting to connect to:    tcp://10.67.22.154:8786\n",
      "2025-09-09 08:49:46,050 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,046 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-09 08:49:46,050 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,046 - distributed.worker - INFO -               Threads:                          1\n",
      "2025-09-09 08:49:46,050 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,046 - distributed.worker - INFO -                Memory:                   1.94 GiB\n",
      "2025-09-09 08:49:46,051 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,046 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l6wy2s__\n",
      "2025-09-09 08:49:46,051 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,046 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-09 08:49:46,051 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,046 - distributed.core - INFO - Starting established connection to tcp://10.67.22.154:8786\n",
      "2025-09-09 08:49:46,052 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,048 - distributed.worker - INFO - Starting Worker plugin shuffle\n",
      "2025-09-09 08:49:46,052 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,048 - distributed.worker - INFO -         Registered to:    tcp://10.67.22.154:8786\n",
      "2025-09-09 08:49:46,052 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,048 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-09 08:49:46,052 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,048 - distributed.core - INFO - Starting established connection to tcp://10.67.22.154:8786\n",
      "2025-09-09 08:49:46,054 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,046 - distributed.worker - INFO - Starting Worker plugin shuffle\n",
      "2025-09-09 08:49:46,054 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,047 - distributed.worker - INFO -         Registered to:    tcp://10.67.22.154:8786\n",
      "2025-09-09 08:49:46,054 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,047 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-09 08:49:46,054 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,047 - distributed.core - INFO - Starting established connection to tcp://10.67.22.154:8786\n",
      "2025-09-09 08:49:46,056 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,051 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.113:36579\n",
      "2025-09-09 08:49:46,057 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,056 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.116:34257\n",
      "2025-09-09 08:49:46,057 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,056 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.116:34257\n",
      "2025-09-09 08:49:46,057 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,056 - distributed.worker - INFO -          dashboard at:         10.67.22.116:36749\n",
      "2025-09-09 08:49:46,058 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,056 - distributed.worker - INFO - Waiting to connect to:    tcp://10.67.22.154:8786\n",
      "2025-09-09 08:49:46,058 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,056 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-09 08:49:46,058 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,056 - distributed.worker - INFO -               Threads:                          1\n",
      "2025-09-09 08:49:46,058 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,056 - distributed.worker - INFO -                Memory:                   1.94 GiB\n",
      "2025-09-09 08:49:46,059 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,056 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7plszcvh\n",
      "2025-09-09 08:49:46,059 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,056 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-09 08:49:46,063 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,063 - distributed.worker - INFO - Starting Worker plugin shuffle\n",
      "2025-09-09 08:49:46,063 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,063 - distributed.worker - INFO -         Registered to:    tcp://10.67.22.154:8786\n",
      "2025-09-09 08:49:46,063 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,063 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-09 08:49:46,064 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,063 - distributed.core - INFO - Starting established connection to tcp://10.67.22.154:8786\n",
      "2025-09-09 08:49:46,080 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,080 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.116:36021\n",
      "2025-09-09 08:49:46,080 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,080 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.116:36021\n",
      "2025-09-09 08:49:46,081 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,080 - distributed.worker - INFO -          dashboard at:         10.67.22.116:42919\n",
      "2025-09-09 08:49:46,082 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,080 - distributed.worker - INFO - Waiting to connect to:    tcp://10.67.22.154:8786\n",
      "2025-09-09 08:49:46,082 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,080 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-09 08:49:46,082 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,080 - distributed.worker - INFO -               Threads:                          1\n",
      "2025-09-09 08:49:46,083 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,080 - distributed.worker - INFO -                Memory:                   1.94 GiB\n",
      "2025-09-09 08:49:46,083 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,080 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oaakkolm\n",
      "2025-09-09 08:49:46,083 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,080 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-09 08:49:46,084 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:46,081 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.116:41745\n",
      "2025-09-09 08:49:47,103 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,098 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.216:42861'\n",
      "2025-09-09 08:49:47,111 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,107 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.216:35023'\n",
      "2025-09-09 08:49:47,115 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,111 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.216:41161'\n",
      "2025-09-09 08:49:47,122 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,118 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.216:36057'\n",
      "2025-09-09 08:49:47,581 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,577 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.216:36433\n",
      "2025-09-09 08:49:47,582 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,577 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.216:46189\n",
      "2025-09-09 08:49:47,583 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,577 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.216:46189\n",
      "2025-09-09 08:49:47,584 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,577 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.216:35561\n",
      "2025-09-09 08:49:47,585 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,577 - distributed.worker - INFO -          dashboard at:         10.67.22.216:41199\n",
      "2025-09-09 08:49:47,586 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,577 - distributed.worker - INFO - Waiting to connect to:    tcp://10.67.22.154:8786\n",
      "2025-09-09 08:49:47,586 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,577 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.216:35561\n",
      "2025-09-09 08:49:47,587 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,577 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-09 08:49:47,587 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,577 - distributed.worker - INFO -          dashboard at:         10.67.22.216:40667\n",
      "2025-09-09 08:49:47,588 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,577 - distributed.worker - INFO - Waiting to connect to:    tcp://10.67.22.154:8786\n",
      "2025-09-09 08:49:47,588 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,577 - distributed.worker - INFO -               Threads:                          1\n",
      "2025-09-09 08:49:47,590 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,577 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-09 08:49:47,591 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,577 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.216:36433\n",
      "2025-09-09 08:49:47,591 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,577 - distributed.worker - INFO -                Memory:                   1.94 GiB\n",
      "2025-09-09 08:49:47,592 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,577 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sr5o3gt1\n",
      "2025-09-09 08:49:47,592 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,577 - distributed.worker - INFO -          dashboard at:         10.67.22.216:32865\n",
      "2025-09-09 08:49:47,593 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,577 - distributed.worker - INFO -               Threads:                          1\n",
      "2025-09-09 08:49:47,594 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,577 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-09 08:49:47,594 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,577 - distributed.worker - INFO -                Memory:                   1.94 GiB\n",
      "2025-09-09 08:49:47,595 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,577 - distributed.worker - INFO - Waiting to connect to:    tcp://10.67.22.154:8786\n",
      "2025-09-09 08:49:47,596 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,577 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xcrgt6s6\n",
      "2025-09-09 08:49:47,596 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,577 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-09 08:49:47,597 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,577 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-09 08:49:47,597 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,577 - distributed.worker - INFO -               Threads:                          1\n",
      "2025-09-09 08:49:47,598 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,578 - distributed.worker - INFO -                Memory:                   1.94 GiB\n",
      "2025-09-09 08:49:47,598 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,578 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-06nn5zhm\n",
      "2025-09-09 08:49:47,600 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,578 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-09 08:49:47,602 - distributed.deploy.ssh - INFO - 2025-09-09 08:49:47,583 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.216:35675\n"
     ]
    }
   ],
   "source": [
    "# create again a cluster\n",
    "# CLUSTER DEPLOYMENT ON CLOUDVENETO\n",
    "client.close()   \n",
    "cluster.close()\n",
    "\n",
    "cluster = SSHCluster(\n",
    "    [\"10.67.22.154\", \"10.67.22.216\", \"10.67.22.116\", \"10.67.22.113\"],\n",
    "    connect_options={\"known_hosts\": None},\n",
    "    remote_python=\"/home/ubuntu/miniconda3/bin/python\",\n",
    "    scheduler_options={\"port\": 8786, \"dashboard_address\": \":8797\"},\n",
    "    worker_options={\n",
    "        \"nprocs\": 4,     \n",
    "        \"nthreads\": 1  \n",
    "    }\n",
    ")\n",
    "\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6b75f318-4a62-4694-b56d-d6ec059a6135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://10.67.22.154:8786' processes=12 threads=12, memory=23.25 GiB>\n"
     ]
    }
   ],
   "source": [
    "print(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd8faee-d1c2-4564-a1fa-662c3ef82752",
   "metadata": {},
   "source": [
    "Now we have 12 workers (4 worker on each VM, excluding the scheduler/master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "75bdb666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import os\n",
    "\n",
    "os.chdir(\"/home/ubuntu\") \n",
    "path_HIGGS = os.getcwd() + \"/datasets/HIGGS.csv\"\n",
    "# A huge dataset\n",
    "df = dd.read_csv(path_HIGGS, header=None, blocksize=\"200MB\")\n",
    "X_df = df.iloc[:, 1:] \n",
    "X_da = X_df.to_dask_array(lengths=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fd6f6125-46f1-4cd0-afab-b592d2a581f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 2.29 GiB </td>\n",
       "                        <td> 58.75 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (11000000, 28) </td>\n",
       "                        <td> (275002, 28) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 40 chunks in 1 graph layer </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> float64 numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"75\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"5\" x2=\"25\" y2=\"5\" />\n",
       "  <line x1=\"0\" y1=\"12\" x2=\"25\" y2=\"12\" />\n",
       "  <line x1=\"0\" y1=\"17\" x2=\"25\" y2=\"17\" />\n",
       "  <line x1=\"0\" y1=\"24\" x2=\"25\" y2=\"24\" />\n",
       "  <line x1=\"0\" y1=\"29\" x2=\"25\" y2=\"29\" />\n",
       "  <line x1=\"0\" y1=\"35\" x2=\"25\" y2=\"35\" />\n",
       "  <line x1=\"0\" y1=\"42\" x2=\"25\" y2=\"42\" />\n",
       "  <line x1=\"0\" y1=\"48\" x2=\"25\" y2=\"48\" />\n",
       "  <line x1=\"0\" y1=\"54\" x2=\"25\" y2=\"54\" />\n",
       "  <line x1=\"0\" y1=\"63\" x2=\"25\" y2=\"63\" />\n",
       "  <line x1=\"0\" y1=\"69\" x2=\"25\" y2=\"69\" />\n",
       "  <line x1=\"0\" y1=\"74\" x2=\"25\" y2=\"74\" />\n",
       "  <line x1=\"0\" y1=\"81\" x2=\"25\" y2=\"81\" />\n",
       "  <line x1=\"0\" y1=\"87\" x2=\"25\" y2=\"87\" />\n",
       "  <line x1=\"0\" y1=\"93\" x2=\"25\" y2=\"93\" />\n",
       "  <line x1=\"0\" y1=\"99\" x2=\"25\" y2=\"99\" />\n",
       "  <line x1=\"0\" y1=\"105\" x2=\"25\" y2=\"105\" />\n",
       "  <line x1=\"0\" y1=\"111\" x2=\"25\" y2=\"111\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 25.412616514582485,0.0 25.412616514582485,120.0 0.0,120.0\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.706308\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >28</text>\n",
       "  <text x=\"45.412617\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,45.412617,60.000000)\">11000000</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<read-_to_string_dtype-values, shape=(11000000, 28), dtype=float64, chunksize=(275002, 28), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's print it\n",
    "X_da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8203a325-270e-423f-af3c-526376be2ae9",
   "metadata": {},
   "source": [
    "As of now, nothing has yet happened. Let's load in the worker's memory the partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d06face3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 2.29 GiB </td>\n",
       "                        <td> 58.75 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (11000000, 28) </td>\n",
       "                        <td> (275002, 28) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 40 chunks in 1 graph layer </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> float64 numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"75\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"5\" x2=\"25\" y2=\"5\" />\n",
       "  <line x1=\"0\" y1=\"12\" x2=\"25\" y2=\"12\" />\n",
       "  <line x1=\"0\" y1=\"17\" x2=\"25\" y2=\"17\" />\n",
       "  <line x1=\"0\" y1=\"24\" x2=\"25\" y2=\"24\" />\n",
       "  <line x1=\"0\" y1=\"29\" x2=\"25\" y2=\"29\" />\n",
       "  <line x1=\"0\" y1=\"35\" x2=\"25\" y2=\"35\" />\n",
       "  <line x1=\"0\" y1=\"42\" x2=\"25\" y2=\"42\" />\n",
       "  <line x1=\"0\" y1=\"48\" x2=\"25\" y2=\"48\" />\n",
       "  <line x1=\"0\" y1=\"54\" x2=\"25\" y2=\"54\" />\n",
       "  <line x1=\"0\" y1=\"63\" x2=\"25\" y2=\"63\" />\n",
       "  <line x1=\"0\" y1=\"69\" x2=\"25\" y2=\"69\" />\n",
       "  <line x1=\"0\" y1=\"74\" x2=\"25\" y2=\"74\" />\n",
       "  <line x1=\"0\" y1=\"81\" x2=\"25\" y2=\"81\" />\n",
       "  <line x1=\"0\" y1=\"87\" x2=\"25\" y2=\"87\" />\n",
       "  <line x1=\"0\" y1=\"93\" x2=\"25\" y2=\"93\" />\n",
       "  <line x1=\"0\" y1=\"99\" x2=\"25\" y2=\"99\" />\n",
       "  <line x1=\"0\" y1=\"105\" x2=\"25\" y2=\"105\" />\n",
       "  <line x1=\"0\" y1=\"111\" x2=\"25\" y2=\"111\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 25.412616514582485,0.0 25.412616514582485,120.0 0.0,120.0\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.706308\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >28</text>\n",
       "  <text x=\"45.412617\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,45.412617,60.000000)\">11000000</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<read-_to_string_dtype-values, shape=(11000000, 28), dtype=float64, chunksize=(275002, 28), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_da.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23c14a8-c2a2-45c2-b439-de6c5c69c2eb",
   "metadata": {},
   "source": [
    "The dataset now resides in the worker's memory. Having a look at the dashboard:\n",
    "\n",
    "\n",
    "!!! IMMAGINE\n",
    "\n",
    "\n",
    "This means that the dataset was uploaded partially on the worker's RAM and partially in their mass memory\n",
    "\n",
    "Let's now run our algorithm. However, we can't obtain both $Q$ and $R$. This is because $Q$ is a $m \\times m$ matrix, where $m$ here is approximately $11$ billion. If each element is a double ($8 \\> B$), then $Q$ is about $88 \\> GB$, way to much to be collected on the client RAM. Hence, we modify our function so that both $R$ and $Q$ are persisted but not directly sent to the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "73a0983d-45fe-4b4f-bdd5-7592a32bc695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_choleskyQR_parallel(X_da : dask.array.Array):\n",
    "    \n",
    "    def gramMatMul(x): #Declaring it this way will make the name appear in the Dask dashboard\n",
    "        return x.T @ x\n",
    "    def MatMul(x): \n",
    "        return x @ R_inv\n",
    "        \n",
    "    # A list of delayed tasks for each partition of the dataset. Each partition computes the local Gram matrix (as a delayed task)\n",
    "    chunks_delayed = [dask.delayed(gramMatMul)(chunk) for chunk in X_da.to_delayed().ravel()]\n",
    "    # Now sum all the local Gram matrices to get the global Gram matrix\n",
    "    Gram_global_delayed = dask.delayed(sum)(chunks_delayed)   ## !! This is not parallel\n",
    "    # Compute R as the Cholesky decomposition on the global Gram matrix (as a delayed even if a serial operation just call .compute at the end)\n",
    "    R = dask.delayed(np.linalg.cholesky)(Gram_global_delayed)\n",
    "    #R.visualize(\"fig/CholeskyR.png\")\n",
    "    R = R.compute() # Compute R. This will put a stop at the parallel operation\n",
    "    R_inv = np.linalg.inv(R) # It's a small matrix, so this operation is fast even if serial\n",
    "\n",
    "    \n",
    "    X_da = X_da.persist()    # Persist again X_da, since X_da.to_delayed seems to cause troubles\n",
    "    Q = X_da.map_blocks(MatMul, dtype=X_da.dtype)\n",
    "    #Q.visualize(\"fig/CholeskyQ.png\")\n",
    "    Q = Q.persist() # Compute Q\n",
    "    return Q, R\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ab0c9c6a-8abb-4fb2-b3c1-3bc2ba39e31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.4 ms, sys: 3.12 ms, total: 20.6 ms\n",
      "Wall time: 238 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Q, R = compute_choleskyQR_parallel(X_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd07125-5f73-4129-b1a8-c37373232954",
   "metadata": {},
   "source": [
    "Qui da incollare l'immagine della dashboard spiegando i vari blocchi\n",
    "\n",
    "Nota che qui, con dataset più grandi, la storia della somma in parallelo/raccolta dati importa poco"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b97675-aee2-4f22-9e2d-8f798baa3fe4",
   "metadata": {},
   "source": [
    "## Tree reduction\n",
    "\n",
    "Qui la storia della somma in parallelo, riduzione come da articolo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "1696f704-d532-4fe3-a7c9-acb87b10d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_choleskyQR_parallel_tree(X_da : dask.array.Array):\n",
    "    \n",
    "    def gramMatMul(x): #Declaring it this way will make the name appear in the Dask dashboard\n",
    "        return x.T @ x\n",
    "    def MatMul(x, R_inv): \n",
    "        return x @ R_inv\n",
    "    def PartialSum(a,b):\n",
    "        return a+b\n",
    "    def Inverse(R):\n",
    "        return np.linalg.inv(R)\n",
    "        \n",
    "    # A list of delayed tasks for each partition of the dataset. Each partition computes the local Gram matrix (as a delayed task)\n",
    "    chunks_delayed = [dask.delayed(gramMatMul)(chunk) for chunk in X_da.to_delayed().ravel()]\n",
    "    while len(chunks_delayed) > 1:\n",
    "        new_level = []\n",
    "        for i in range(0, len(chunks_delayed), 2):\n",
    "            if i + 1 < len(chunks_delayed):\n",
    "                new_level.append(dask.delayed(PartialSum)(chunks_delayed[i], chunks_delayed[i+1]))\n",
    "            else:\n",
    "                new_level.append(chunks_delayed[i])\n",
    "        chunks_delayed = new_level\n",
    "\n",
    "    Gram_global_delayed = chunks_delayed[0]\n",
    "    # Compute R as the Cholesky decomposition on the global Gram matrix (as a delayed even if a serial operation just call .compute at the end)\n",
    "    R = dask.delayed(np.linalg.cholesky)(Gram_global_delayed)\n",
    "    #R.visualize(\"fig/CholeskyR.png\")\n",
    "    R = R.persist()\n",
    "    #R = R.compute() # Compute R. This will put a stop at the parallel operation\n",
    "    R_inv = dask.delayed(Inverse)(R) # It's a small matrix, so this operation is fast even if serial\n",
    "    \n",
    "    X_da = X_da.persist()    # Persist again X_da, since X_da.to_delayed seems to cause troubles\n",
    "    Q = X_da.map_blocks(MatMul,R_inv, dtype=X_da.dtype)\n",
    "    #Q.visualize(\"fig/CholeskyQ.png\")\n",
    "    Q = Q.persist() # Compute Q\n",
    "    return Q, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "d8791e88-2f2d-42cb-8ba7-d23b60a4c151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.3 ms, sys: 0 ns, total: 16.3 ms\n",
      "Wall time: 16.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "client.cancel(Q)\n",
    "client.cancel(R)\n",
    "Q, R = compute_choleskyQR_parallel_tree(X_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "9a6658b2-f882-42ab-b2e7-83470d3e3db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.3 ms, sys: 253 μs, total: 21.5 ms\n",
      "Wall time: 168 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "client.cancel(Q)\n",
    "client.cancel(R)\n",
    "Q, R = compute_choleskyQR_parallel(X_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408ef4ea-2eeb-4b6f-95f2-1c52603b823c",
   "metadata": {},
   "source": [
    "Commento sulla versione definitiva, migliorata con la tree reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7293b995-8977-47b8-92f4-92137c053735",
   "metadata": {},
   "source": [
    "## Definitive function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "81411921-d3b9-403d-b065-1dcbb11065db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_choleskyQR_parallel_optimal(X_da : dask.array.Array):\n",
    "    def gramMatMul(x): #Declaring it this way will make the name appear in the Dask dashboard\n",
    "        return x.T @ x\n",
    "    def MatMul(x, R_inv): \n",
    "        return x @ R_inv\n",
    "    def PartialSum(a,b):\n",
    "        return a+b\n",
    "    def Inverse(R):\n",
    "        return np.linalg.inv(R)\n",
    "        \n",
    "    # A list of delayed tasks for each partition of the dataset. Each partition computes the local Gram matrix (as a delayed task)\n",
    "    chunks_delayed = [dask.delayed(gramMatMul)(chunk) for chunk in X_da.to_delayed().ravel()]\n",
    "    while len(chunks_delayed) > 1:\n",
    "        new_level = []\n",
    "        for i in range(0, len(chunks_delayed), 2):\n",
    "            if i + 1 < len(chunks_delayed):\n",
    "                new_level.append(dask.delayed(PartialSum)(chunks_delayed[i], chunks_delayed[i+1]))\n",
    "            else:\n",
    "                new_level.append(chunks_delayed[i])\n",
    "        chunks_delayed = new_level\n",
    "\n",
    "    Gram_global_delayed = chunks_delayed[0]\n",
    "    # Compute R as the Cholesky decomposition on the global Gram matrix (as a delayed even if a serial operation just call .compute at the end)\n",
    "    R = dask.delayed(np.linalg.cholesky)(Gram_global_delayed)\n",
    "    #R.visualize(\"fig/CholeskyR.png\")\n",
    "    R = R.persist()\n",
    "    #R = R.compute() # Compute R. This will put a stop at the parallel operation\n",
    "    R_inv = dask.delayed(Inverse)(R) # It's a small matrix, so this operation is fast even if serial\n",
    "    \n",
    "    X_da = X_da.persist()    # Persist again X_da, since X_da.to_delayed seems to cause troubles\n",
    "    Q = X_da.map_blocks(MatMul,R_inv, dtype=X_da.dtype)\n",
    "    #Q.visualize(\"fig/CholeskyQ.png\")\n",
    "    Q = Q.persist() # Compute Q\n",
    "    return Q, R"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
