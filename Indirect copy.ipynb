{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d3a4cb7",
   "metadata": {},
   "source": [
    "# Indirect TSQR\n",
    "\n",
    "\n",
    "**Input**: Matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m \\gg n$.  \n",
    "\n",
    "The indirect TSQR method avoids explicitly assembling the final $Q$ matrix block-by-block. The idea is to first compute a stable global $R$ factor first, and then derives $Q$ implicitly from it.  \n",
    "\n",
    "---\n",
    "\n",
    "### 1) First step (local QR factorizations)  \n",
    "- The matrix $A$ is divided into $p$ row blocks:  \n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} A_1^T & A_2^T & \\cdots & A_p^T \\end{bmatrix}^T,\n",
    "\\quad A_j \\in \\mathbb{R}^{m_j \\times n}.\n",
    "$$  \n",
    "\n",
    "- Each block is used locally to factor the small R_j blocks:  \n",
    "\n",
    "$$\n",
    "A_j = Q_j^{(1)} R_j, \n",
    "\\quad Q_j^{(1)} \\in \\mathbb{R}^{m_j \\times n}, \\; R_j \\in \\mathbb{R}^{n \\times n}.\n",
    "$$  \n",
    "\n",
    "  \n",
    "- The $Q_j^{(1)}$ matrices obtained are discarded and only the small $R_j$ are passed along.  \n",
    "\n",
    "\n",
    "\n",
    "### 2) Second step (global QR reduction)  \n",
    "- The local triangular matrices are stacked vertically:  \n",
    "\n",
    "$$\n",
    "R_{\\text{stack}} = \n",
    "\\begin{bmatrix} \n",
    "R_1 \\\\ R_2 \\\\ \\vdots \\\\ R_p \n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{pn \\times n}.\n",
    "$$  \n",
    "\n",
    "- To obtain the **final global $R$** it is necessary to perform a second QR factorization:  \n",
    "\n",
    "$$\n",
    "R_{\\text{stack}} = \\tilde{Q} \\, R, \n",
    "\\quad \\tilde{Q} \\in \\mathbb{R}^{pn \\times n}, \\; R \\in \\mathbb{R}^{n \\times n}.\n",
    "$$  \n",
    "\n",
    "- Unlike other methods, the $Q_j^{(1)}$ blocks are not explicitily multiplied with pieces of $\\tilde{Q}$ to assemble the final $Q$, instead they are discarded.  \n",
    "\n",
    "\n",
    "\n",
    "### 3) Recovering $Q$ indirectly  \n",
    "- By construction, $A = Q R$.  \n",
    "- Since $R$ is already available, $Q$ can be obtained as:  \n",
    "\n",
    "$$\n",
    "Q = A R^{-1}.\n",
    "$$  \n",
    "\n",
    "- This avoids explicitly combining the intermediate $Q_j^{(1)}$ and $\\tilde{Q}$ matrices.  \n",
    "- Instead, a final *map* step applies the small matrix $R^{-1}$ (size $n \\times n$) to each row block of $A$, yielding the blocks of $Q$ on the fly.  \n",
    "\n",
    "---\n",
    "\n",
    "The optimization idea is that only  the small $R_j$ factors are passed between workers, never the large $Q_j^{(1)}$. The tradeoff is that it  requires access to the full $A$ again to compute $Q$, which may be costly for very large datasets, but avoids storing intermediate $Q_j$.\n",
    "On the other hand the two-level QR decomposition ensures orthogonality and therefore an improved numerical stability.  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df22db80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import dask\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.linalg import solve_triangular \n",
    "from sklearn.datasets import fetch_california_housing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce40f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLUSTER DEPLOYMENT ON CLOUDVENETO\n",
    "from dask.distributed import Client, SSHCluster\n",
    "\n",
    "cluster = SSHCluster(\n",
    "    [\"10.67.22.154\", \"10.67.22.216\", \"10.67.22.116\", \"10.67.22.113\"],\n",
    "    connect_options={\"known_hosts\": None},\n",
    "    remote_python=\"/home/ubuntu/miniconda3/bin/python\",\n",
    "    scheduler_options={\"port\": 8786, \"dashboard_address\": \":8797\"},\n",
    "    worker_options={\n",
    "        \"n_workers\": 4,       # N. of processess per VM. CloudVeneto's large VM offers 4-core CPU, but for now we only spawn 1 process per VM\n",
    "        \"nthreads\": 1      # N. of threads per process\n",
    "    }\n",
    ")\n",
    "\n",
    "client = Client(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd1b8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://127.0.0.1:38527' processes=4 threads=4, memory=5.79 GiB>\n",
      "LocalCluster(102e741e, 'tcp://127.0.0.1:38527', workers=4, threads=4, memory=5.79 GiB)\n"
     ]
    }
   ],
   "source": [
    "# check if everything went smoothly\n",
    "print(client)\n",
    "print(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b933587a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dask.array<array, shape=(20640, 8), dtype=float64, chunksize=(6880, 8), chunktype=numpy.ndarray>\n"
     ]
    }
   ],
   "source": [
    "X_cached = X_da.rechunk((1_000_000, -1)).persist()\n",
    "\n",
    "X_da\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba551f4",
   "metadata": {},
   "source": [
    "## Variants of the Indirect TSQR Method\n",
    "In the following, different versions of the **Indirect TSQR** algorithm are presented.  \n",
    "The main difference across these implementations lies in how the global $R$ factor is handled: computed, persisted across workers, or kept as a delayed/Dask object.  \n",
    "\n",
    "---\n",
    "\n",
    "• **Serial version of the Indirect TSQR**\n",
    "\n",
    "This approach showcases the **basic formulation** of the indirect method using only NumPy. It has some serious limitations due to the lack of parallelizzation as everything runs on a single core The memory usage scales with the dataset size, making this approach infeasible for very large datasets (e.g. HIGGS) and only suitable for local analysis.\n",
    "\n",
    "---\n",
    "• **Parallel / Dask approach (version 1)**\n",
    "\n",
    "This version introduces Dask for parallelism. Each block of $A$ is processed in parallel using the function:\n",
    "```python\n",
    "R_blocks = X_da.map_blocks(compute_R, dtype=X_da.dtype, chunks=(n_cols, n_cols))\n",
    "```\n",
    "This is where the parallelizzation happens and local QR are created for each block. However to compute the inverse: $R^{-1}$, we call .compute() on $R$, pulling it back to the driver as a NumPy array, this introduces a bottleneck that breaks the laziness and centralizes $R$ on the driver.\n",
    "\n",
    "---\n",
    "• **Parallel / Dask approach (version 2)**\n",
    "\n",
    "This version is equivalent to version 1 but replaces `.compute()` with the function: `.persist()`.\n",
    "The function `.persist()` keeps $R$ distributed across the workers rather than pulling it to the driver.\n",
    "This allows for improved results since the scheduler tracks dependencies and ensures $R$ is reused without recomputation.\n",
    "\n",
    "Dask has da.linalg.qr, but it assumes the whole array is large and chunked regularly.\n",
    "To get the final global R, you must combine all the Ri\n",
    "That means at some point, the data has to come together into a single place (can’t keep it sharded).\n",
    "So we bring the data to the driver since it is very small, this it optimizes the uses of np.linalg.qr, we are gathering the small stuff\n",
    "\n",
    "---\n",
    "\n",
    "• **Parallel / Dask approach (version 3)**\n",
    "\n",
    "In this case instead of computing $R$ immediately it is left as a delayed object `dask.delayed()`, this means that no computation happens until `.compute()` is called at the end.\n",
    "Afterwards instead of calculating the inverse immediately with NumPy this process is also delayed. This avoids pulling $R$ to the driver until the very end and allows Dask to schedule the inversion after $R$ is available in the graph, instead of serializing execution manually.\n",
    "Ultimately also the final computation of $Q_da$ is lazy.\n",
    "This fully delayed version allows the scheduler to optimize the entire pipeline together.\n",
    "\n",
    "Cons:\n",
    "\n",
    "Larger and more complex task graph (can become a scheduling overhead).\n",
    "\n",
    "If you only need $R$ (or reuse $R$ multiple times), delaying everything might be inefficient compared to persist().\n",
    "\n",
    "Execution time may fluctuate more since all steps (QR, stacking, inversion, multiplication) are chained into one big compute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e427157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indirect_serial(A, n_div):\n",
    "    \"\"\"\n",
    "    Indirect TSQR (serial, NumPy).\n",
    "    Splits A by rows into n_div blocks, computes local R_i via QR,\n",
    "    reduces to global R by QR on the stacked R_i, then recovers Q = A R^{-1}.\n",
    "    Returns (Q, R).\n",
    "    \"\"\"\n",
    "\n",
    "    n_samp = A.shape[0]\n",
    "    \n",
    "    div_points = int(np.floor(n_samp/n_div))\n",
    "    A_divided = []\n",
    "    Ri = []\n",
    "    \n",
    "    A_divided = [A[div_points * i : div_points * (i + 1)] for i in range(n_div - 1)]    # Divide the A matrix into multiple chunks\n",
    "    A_divided.append(A[(n_div - 1) * div_points:, :])   # In the case n_samp wasn't divisible by n_div\n",
    "\n",
    "    Ri = [np.linalg.qr(Ai, mode=\"reduced\")[1] for Ai in A_divided]\n",
    "    R_stack = np.concatenate(Ri, axis = 0)\n",
    "    _, R = np.linalg.qr(R_stack, mode=\"reduced\")\n",
    "\n",
    "    # Here you could also use the numpy function \"np.linalg.inv(R)\" function but the triangular decomposition grants more numerical stability\n",
    "    I = np.eye(n_samp, dtype=A.dtype)\n",
    "    Rinv = solve_triangular(R, I, lower=False)\n",
    "\n",
    "    Q = A @ Rinv\n",
    "\n",
    "    return Q, R\n",
    "\n",
    "\n",
    "def compute_R(block):\n",
    "    # np.linalg.qr with mode='r' gives just the R matrix\n",
    "    R = np.linalg.qr(block, mode=\"r\")\n",
    "    return R\n",
    "\n",
    "\n",
    "def indirect_parallel(X_da):\n",
    "    \"\"\"\n",
    "    Indirect TSQR with Dask.\n",
    "    Output:\n",
    "        R    : final global triangular factor (n x n, NumPy array on driver)\n",
    "        Q_da : Dask Array (m x n), representing Q = A R^{-1} (lazy)\n",
    "    \"\"\"\n",
    "\n",
    "    n_cols = X_da.shape[1]\n",
    "\n",
    "    # Parallel mapping of the QR blocks\n",
    "    R_blocks = X_da.map_blocks(compute_R, dtype=X_da.dtype, chunks=(n_cols, n_cols))\n",
    "    # Now R_blocks is a stack of n x n matrices (one per partition)\n",
    "    # Its shape is (#chunks * n, n)\n",
    "\n",
    "    # Bring all the blocks together to compute\n",
    "    R_stack = R_blocks.compute()   # NumPy array, shape (p*n, n)\n",
    "\n",
    "    # Small QR on driver to combine them into the final R\n",
    "    R = np.linalg.qr(R_stack, mode=\"r\")\n",
    "\n",
    "    # Instead of materializing Q, compute a small R^{-1} (n x n).\n",
    "    I = np.eye(n_cols, dtype=X_da.dtype)\n",
    "    R_inv = solve_triangular(R, I, lower=False)  # stable\n",
    "\n",
    "    # Broadcast Rinv to every chunk: Q = A @ R^{-1}\n",
    "    Q_da = X_da @ R_inv   # still a Dask Array, lazy\n",
    "\n",
    "    return Q_da, R      #Q_da because it is lazy, it is still a Dask array\n",
    "\n",
    "def indirect_parallel_persisted(X_da):\n",
    "    \"\"\"\n",
    "    Indirect TSQR with Dask.\n",
    "    Output:\n",
    "        R    : final global triangular factor (n x n, persisted)\n",
    "        Q_da : Dask Array (m x n), representing Q = A R^{-1} (lazy)\n",
    "    \"\"\"\n",
    "\n",
    "    n_cols = X_da.shape[1]\n",
    "    R_blocks = X_da.map_blocks(compute_R, dtype=X_da.dtype, chunks=(n_cols, n_cols))\n",
    "\n",
    "    # In this case instead of computing, persist the R blocks\n",
    "    R_stack = R_blocks.persist()  \n",
    "    _, R = np.linalg.qr(R_stack)\n",
    "\n",
    "    I = np.eye(n_cols, dtype=X_da.dtype)\n",
    "    R_inv = solve_triangular(R, I, lower=False)  # stable\n",
    "    Q_da = X_da @ R_inv  \n",
    "\n",
    "    return Q_da, R     \n",
    "\n",
    "\n",
    "\n",
    "def indirect_parallel_delayed(X_da):\n",
    "    \"\"\"\n",
    "    Indirect TSQR with Dask, delayed version.\n",
    "    Output:\n",
    "        R    : delayed object\n",
    "        Q_da : Dask Array (m x n), representing Q = A R^{-1} (lazy)\n",
    "    \"\"\"\n",
    "\n",
    "    n_cols = X_da.shape[1]\n",
    "    R_blocks = X_da.map_blocks(compute_R, dtype=X_da.dtype, chunks=(n_cols, n_cols))\n",
    "\n",
    "\n",
    "    # Convert blocks to delayed NumPy arrays, stack via delayed\n",
    "    R_list = list(R_blocks.to_delayed().ravel())     # each is delayed np.ndarray (n x n)\n",
    "    R_stack = dask.delayed(np.vstack)(R_list)        # delayed (p*n x n)\n",
    "    \n",
    "    R_delayed = dask.delayed(compute_R)(R_stack)      # delayed np.ndarray (n x n)\n",
    "\n",
    "\n",
    "    I = np.eye(n_cols, dtype=X_da.dtype)\n",
    "    # compute R^{-1} lazily\n",
    "    R_inv_delayed = dask.delayed(solve_triangular)(R_delayed, I, lower=False)\n",
    "    R_inv_da = dask.from_delayed(R_inv_delayed, shape=(n_cols, n_cols), dtype=X_da.dtype)\n",
    "\n",
    "    # Broadcast multiply (keep Q lazy)\n",
    "    Q_da = X_da @ R_inv_da\n",
    "\n",
    "    return Q_da, R_delayed      #Q_da dask array, R delayed object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c55a8a",
   "metadata": {},
   "source": [
    "The following is an example of how to call the serial function in a local environment, an argument to pass is the number of partitions over which divide the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30a4fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size for each chunk is : 0.0264192 Mb\n",
      "CPU times: user 103 ms, sys: 82.4 ms, total: 186 ms\n",
      "Wall time: 64.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "Q, R = indirect_serial(data.data.values, 50)  # Divide in 50 partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df2cb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not-delayed version\n",
    "start = time.perf_counter()\n",
    "Q_nd, R_nd = indirect_parallel(X_cached)  # returns Q_da, R (NumPy)\n",
    "_ = R_nd  # already materialized\n",
    "t_nd_R = time.perf_counter() - start\n",
    "\n",
    "\n",
    "\n",
    "#Dask Dashboard: Big green block: compute_R. map stage of mapblock that returns the small Ri\n",
    "# Tiny yellow finalize-hlg block - Dask housekeeping\n",
    "\"\"\"Why you don’t see “reduce” or “broadcast” here\n",
    "The reduce to the final \n",
    "R is done on the driver with NumPy:\"\"\"\n",
    "\n",
    "\n",
    "t_nd_Q, val_nd = time_Q_apply(Q_nd, v)\n",
    "\n",
    "\"\"\"Teal blocks around ~140–150 ms — blockwise-matmul-…\n",
    "In the not-delayed variant, R^{-1} is a NumPy constant, so each matmul task deserializes it; you may notice slightly more per-task overhead compared to the delayed/Dask-array constant.\n",
    "Purple block — reduction for the norm\n",
    "\n",
    "After the matmul, da.linalg.norm(Qv) triggers a reduction:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503ad3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3. The subtlety\\n\\nPersist = compute and cache now, but still return a Dask collection (with futures).\\n\\nCompute = compute now and return the final NumPy array (collected to driver).\\n\\nSo:\\n\\nIf you persist inside your function and return R, you are indeed returning a Dask object backed by futures, not a NumPy matrix.\\n\\nIf you compute inside your function and return R, you’re returning a NumPy array, which is often what you want for the small triangular \\n𝑅\\nR.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not-delayed persisted version\n",
    "start = time.perf_counter()\n",
    "Q_ndp, R_ndp = indirect_parallel_persisted(X_cached)  # returns Q_da, R (NumPy)\n",
    "_ = R_ndp  # already materialized\n",
    "t_nd_Rp = time.perf_counter() - start\n",
    "\n",
    "t_po_Q, val_po = time_Q_apply(Q_ndp, v)\n",
    "\n",
    "\n",
    "# better due to only calculating R once but with a cost\n",
    "\"\"\"3. The subtlety\n",
    "\n",
    "Persist = compute and cache now, but still return a Dask collection (with futures).\n",
    "\n",
    "Compute = compute now and return the final NumPy array (collected to driver).\n",
    "\n",
    "So:\n",
    "\n",
    "If you persist inside your function and return R, you are indeed returning a Dask object backed by futures, not a NumPy matrix.\n",
    "\n",
    "If you compute inside your function and return R, you’re returning a NumPy array, which is often what you want for the small triangular \n",
    "𝑅\n",
    "R.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01f101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# optimized not-delayed version\n",
    "start = time.perf_counter()\n",
    "Q_po, R_po = indirect_parallel_optimized(X_cached)  # returns Q_da (Dask), R_da (Dask)\n",
    "_ = R_po  # already materialized\n",
    "t_po_R = time.perf_counter() - start\n",
    "\n",
    "t_po_Q, val_po = time_Q_apply(Q_po, v)\n",
    "\n",
    "\n",
    "# I don't think this version makes sense\n",
    "\"\"\"Purple (rechunk-merge): small graph-rewiring step. Dask is aligning the chunks of your broadcasted R_da with those of X_da. Because R_da is now itself a Dask Array (1 chunk of size \n",
    "𝑛\n",
    "×\n",
    "𝑛\n",
    "n×n), it needs to merge graph metadata.\n",
    "\n",
    "Big teal block (array) dominating the timeline:\n",
    "This is your broadcasted matmul tasks: X_da @ Rinv_da.\n",
    "Since R_inv was wrapped as a Dask Array (da.from_delayed or da.from_array), Dask sees a proper array-on-array multiply and generates blockwise matmul tasks.\n",
    "→ That’s why you get this large contiguous band of teal tasks: each row-chunk of X_da multiplied with the single small (n,n) chunk of Rinv_da.\"\"\"\n",
    "\"\"\"Earlier, your “not-delayed” version showed slower Q @ v apply because:\n",
    "\n",
    "You measured at small scales.\n",
    "\n",
    "Serialization overhead per task looked big compared to the tiny math.\n",
    "\n",
    "Wrapping in a Dask array made the graph smaller/cleaner → you saw ~0.06 s instead of ~0.5 s.\n",
    "\n",
    "But at larger scales (big \n",
    "𝑚\n",
    "m, many workers), the story flips:\n",
    "\n",
    "Shipping one tiny NumPy array is cheap compared to all the matmuls.\n",
    "\n",
    "Wrapping it as a Dask array adds pointless graph overhead.\n",
    "\n",
    "So you can lose a bit of time overall.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549b9af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fully-delayed version\n",
    "start = time.perf_counter()\n",
    "Q_fd, R_fd = indirect_parallel_delayed(X_cached)  # returns Q_da (Dask), R_da (Dask)\n",
    "_ = R_fd.compute()  # FORCE the same final R compute\n",
    "t_fd_R = time.perf_counter() - start\n",
    "\n",
    "\n",
    "t_fd_Q, val_fd = time_Q_apply(Q_fd, v)\n",
    "\n",
    "\n",
    "\"\"\"Efficiency Implications\n",
    "\n",
    "Computation of \n",
    "𝑅\n",
    "R: essentially unchanged, still dominated by the map stage (compute_R).\n",
    "\n",
    "Broadcast of \n",
    "𝑅\n",
    "−\n",
    "1\n",
    "R\n",
    "−1\n",
    ": somewhat less efficient as a Dask Array, since it adds extra bookkeeping without reducing the numerical cost.\n",
    "\n",
    "Norm benchmark (Q @ v): the visible red/yellow stages are expected; they confirm that your graph is carrying the computation fully through Dask.\n",
    "\n",
    "because da.from_delayed introduces those extraction tasks.\n",
    "\n",
    "Broadcast multiply still shows up as teal + red.\n",
    "\n",
    "The dashboard is “busier” — more small tasks, more scheduler chatter — because everything, even tiny constants, was lifted into the Dask graph.\n",
    "\n",
    "Efficiency interpretation\n",
    "\n",
    "For small \n",
    "𝑛\n",
    "n: making \n",
    "𝑅\n",
    "R a Dask array (fully delayed) adds overhead without real benefit — the norm compute shows more yellow/red fragmentation than the optimized/persisted version.\n",
    "\n",
    "For large distributed runs: it’s still correct, but NumPy constants (or scattered small arrays) are cheaper to handle than wrapping them in Dask.\n",
    "\n",
    "That’s why your timings showed the fully delayed version wasn’t consistently faster\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b0490b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R time — not-delayed: 0.080s | persisted: 0.110s | optimized: 0.110s | partial: 0.081s | full-delayed: 0.073s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(f\"R time — not-delayed: {t_nd_R:.3f}s | persisted: {t_po_Rp:.3f}s | optimized: {t_po_R:.3f}s | | full-delayed: {t_fd_R:.3f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c01a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q apply — not-delayed: 0.463s | partial: 0.054s | partial: 0.056s | full-delayed: 0.066s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Q apply — not-delayed: {t_nd_Q:.3f}s | partial: {t_po_Q:.3f}s | partial: {t_pd_Q:.3f}s | full-delayed: {t_fd_Q:.3f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "013d8401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input matrix A: m = 10000000, n = 4\n",
      "The 3 blocks are: (3333333, 3333333, 3333334)\n",
      "Total size of A: 320.0 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 305.18 MiB </td>\n",
       "                        <td> 101.73 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (10000000, 4) </td>\n",
       "                        <td> (3333334, 4) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 3 chunks in 1 graph layer </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> float64 numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"75\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"39\" x2=\"25\" y2=\"39\" />\n",
       "  <line x1=\"0\" y1=\"79\" x2=\"25\" y2=\"79\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 25.412616514582485,0.0 25.412616514582485,120.0 0.0,120.0\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.706308\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >4</text>\n",
       "  <text x=\"45.412617\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,45.412617,60.000000)\">10000000</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<random_sample, shape=(10000000, 4), dtype=float64, chunksize=(3333334, 4), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_WORKERS = 3\n",
    "# Initialization of a distributed random matrix\n",
    "m, n = int(1e7), 4\n",
    "chunks = [m // N_WORKERS for _ in range(N_WORKERS-1)]\n",
    "chunks.append(m - sum(chunks))\n",
    "A = da.random.random((m, n), chunks=(chunks, n))\n",
    "\n",
    "# Persist in memory to avoid recomputation\n",
    "A = A.persist() \n",
    "\n",
    "print(f\"Input matrix A: m = {A.shape[0]}, n = {A.shape[1]}\")\n",
    "print(f\"The {len(A.chunks[0])} blocks are: {A.chunks[0]}\")\n",
    "print(f\"Total size of A: {A.nbytes / 1e6} MB\")\n",
    "A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8a8ea09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- OUR IMPLEMENTATION (QR) --\n",
      "CPU times: user 1.06 s, sys: 1.37 s, total: 2.43 s\n",
      "Wall time: 7.86 s\n",
      "\n",
      "-- DASK'S IMPLEMENTATION (QR) --\n",
      "CPU times: user 3.63 s, sys: 2.45 s, total: 6.07 s\n",
      "Wall time: 8.82 s\n"
     ]
    }
   ],
   "source": [
    "# QR decomposition\n",
    "\n",
    "Q_delayed, R_delayed = indirect_parallel_delayed(A)  # Our implementation\n",
    "Q_delayed_dask, R_delayed_dask = da.linalg.tsqr(A)    # Dask's implementation\n",
    "\n",
    "print(\"-- OUR IMPLEMENTATION (QR) --\")\n",
    "%time Q, R = dask.compute(Q_delayed, R_delayed)\n",
    "\n",
    "print(\"\\n-- DASK'S IMPLEMENTATION (QR) --\")\n",
    "%time Q_dask, R_dask = dask.compute(Q_delayed_dask, R_delayed_dask)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dask-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
