{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df22db80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import dask\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.linalg import solve_triangular \n",
    "from sklearn.datasets import fetch_california_housing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce40f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLUSTER DEPLOYMENT ON CLOUDVENETO\n",
    "from dask.distributed import Client, SSHCluster\n",
    "\n",
    "cluster = SSHCluster(\n",
    "    [\"10.67.22.154\", \"10.67.22.216\", \"10.67.22.116\", \"10.67.22.113\"],\n",
    "    connect_options={\"known_hosts\": None},\n",
    "    remote_python=\"/home/ubuntu/miniconda3/bin/python\",\n",
    "    scheduler_options={\"port\": 8786, \"dashboard_address\": \":8797\"},\n",
    "    worker_options={\n",
    "        \"nprocs\": 1,       # N. of processess per VM. CloudVeneto's large VM offers 4-core CPU, but for now we only spawn 1 process per VM\n",
    "        \"nthreads\": 1      # N. of threads per process\n",
    "    }\n",
    ")\n",
    "\n",
    "client = Client(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd1b8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://127.0.0.1:38527' processes=4 threads=4, memory=5.79 GiB>\n",
      "LocalCluster(102e741e, 'tcp://127.0.0.1:38527', workers=4, threads=4, memory=5.79 GiB)\n"
     ]
    }
   ],
   "source": [
    "# check if everything went smoothly\n",
    "print(client)\n",
    "print(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b933587a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dask.array<array, shape=(20640, 8), dtype=float64, chunksize=(6880, 8), chunktype=numpy.ndarray>\n"
     ]
    }
   ],
   "source": [
    "X_cached = X_da.rechunk((1_000_000, -1)).persist()\n",
    "\n",
    "print(X_da)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e427157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indirect_serial(A, n_div):\n",
    "\n",
    "    \"\"\"\n",
    "    Indirect TSQR (serial, NumPy).\n",
    "    Splits A by rows into n_div blocks, computes local R_i via QR,\n",
    "    reduces to global R by QR on the stacked R_i, then recovers Q = A R^{-1}.\n",
    "    Returns (Q, R).\n",
    "    \"\"\"\n",
    "\n",
    "    n_samp = A.shape[0]\n",
    "    \n",
    "    div_points = int(np.floor(n_samp/n_div))\n",
    "    A_divided = []\n",
    "    Ri = []\n",
    "    \n",
    "    A_divided = [A[div_points * i : div_points * (i + 1)] for i in range(n_div - 1)]\n",
    "    A_divided.append(A[(n_div - 1) * div_points:, :])   # In the case n_samp wasn't divisible by n_div\n",
    "\n",
    "    Ri = [np.linalg.qr(Ai, mode=\"reduced\")[1] for Ai in A_divided]\n",
    "    \n",
    "    R_stack = np.concatenate(Ri, axis = 0)\n",
    "\n",
    "    # Step 3. Stack R_i and compute global R\n",
    "\n",
    "    _, R = np.linalg.qr(R_stack, mode=\"reduced\")\n",
    "\n",
    "\n",
    "    #   I = np.eye(n_samp, dtype=A.dtype)\n",
    "    #   Rinv = solve_triangular(R, I, lower=False)\n",
    "    Rinv = np.linalg.inv(R)\n",
    "    Q = A @ Rinv\n",
    "\n",
    "    return Q, R\n",
    "\n",
    "\n",
    "def compute_R(block):\n",
    "    # np.linalg.qr with mode='r' gives just the R matrix\n",
    "    R = np.linalg.qr(block, mode=\"r\")\n",
    "    return R\n",
    "\n",
    "\n",
    "def indirect_parallel(X_da):\n",
    "\n",
    "    \"\"\"\n",
    "    Indirect TSQR with Dask.\n",
    "    Input:\n",
    "        X_da : Dask Array (m x n), chunked row-wise\n",
    "    Output:\n",
    "        R    : final global triangular factor (n x n, NumPy array on driver)\n",
    "        Q_da : Dask Array (m x n), representing Q = A R^{-1} (lazy)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    n_cols = X_da.shape[1]\n",
    "\n",
    "    R_blocks = X_da.map_blocks(compute_R, dtype=X_da.dtype, chunks=(n_cols, n_cols))\n",
    "    # Now R_blocks is a stack of n x n matrices (one per partition)\n",
    "    # Its shape is (#chunks * n, n)\n",
    "\n",
    "    R_blocks.visualize(filename=\"fig/R_blocks_graph\", format=\"png\")\n",
    " \n",
    "\n",
    "    #Dask has da.linalg.qr, but it assumes the whole array is large and chunked regularly.\n",
    "    #To get the final global R, you must combine all the Ri\n",
    "    #That means at some point, the data has to come together into a single place (can‚Äôt keep it sharded).\n",
    "    # So we bring the data to the driver because it is very small, because it optimizes the uses of np.linalg.qr, we are gathering the small stuff\n",
    "    R_stack = R_blocks.compute()   # NumPy array, shape (p*n, n)\n",
    "\n",
    "    # Small QR on driver to combine them into the final R\n",
    "    R = np.linalg.qr(R_stack, mode=\"r\")\n",
    "    # delay the computing of qr\n",
    "\n",
    "    # Instead of materializing Q, compute a small R^{-1} (n x n).\n",
    "    I = np.eye(n_cols, dtype=X_da.dtype)\n",
    "    R_inv = solve_triangular(R, I, lower=False)  # stable\n",
    "\n",
    "    # Broadcast Rinv to every chunk: Q = A @ R^{-1}\n",
    "    Q_da = X_da @ R_inv   # still a Dask Array, lazy\n",
    "\n",
    "    return Q_da, R      #Q_da because it is lazy, it is still a Dask array\n",
    "\n",
    "def indirect_parallel_persisted(X_da):\n",
    "\n",
    "    \"\"\"\n",
    "    Indirect TSQR with Dask.\n",
    "    Input:\n",
    "        X_da : Dask Array (m x n), chunked row-wise\n",
    "    Output:\n",
    "        R    : final global triangular factor (n x n, NumPy array on driver)\n",
    "        Q_da : Dask Array (m x n), representing Q = A R^{-1} (lazy)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    n_cols = X_da.shape[1]\n",
    "\n",
    "    R_blocks = X_da.map_blocks(compute_R, dtype=X_da.dtype, chunks=(n_cols, n_cols))\n",
    "    # Now R_blocks is a stack of n x n matrices (one per partition)\n",
    "    # Its shape is (#chunks * n, n)\n",
    "\n",
    "    R_blocks.visualize(filename=\"fig/R_blocks_graph\", format=\"png\")\n",
    " \n",
    "\n",
    "    #Dask has da.linalg.qr, but it assumes the whole array is large and chunked regularly.\n",
    "    #To get the final global R, you must combine all the Ri\n",
    "    #That means at some point, the data has to come together into a single place (can‚Äôt keep it sharded).\n",
    "    # So we bring the data to the driver because it is very small, because it optimizes the uses of np.linalg.qr, we are gathering the small stuff\n",
    "    R_stack = R_blocks.persist()   # NumPy array, shape (p*n, n)\n",
    "\n",
    "    # Small QR on driver to combine them into the final R\n",
    "    _, R = np.linalg.qr(R_stack)\n",
    "    # delay the computing of qr\n",
    "\n",
    "    # Instead of materializing Q, compute a small R^{-1} (n x n).\n",
    "    I = np.eye(n_cols, dtype=X_da.dtype)\n",
    "    R_inv = solve_triangular(R, I, lower=False)  # stable\n",
    "\n",
    "    # Broadcast Rinv to every chunk: Q = A @ R^{-1}\n",
    "    Q_da = X_da @ R_inv   # still a Dask Array, lazy\n",
    "\n",
    "    return Q_da, R      #Q_da because it is lazy, it is still a Dask array\n",
    "\n",
    "\n",
    "def indirect_parallel_optimized(X_da):\n",
    "\n",
    "\n",
    "    n_cols = X_da.shape[1]\n",
    "\n",
    "    R_blocks = X_da.map_blocks(compute_R, dtype=X_da.dtype, chunks=(n_cols, n_cols))\n",
    "    # Now R_blocks is a stack of n x n matrices (one per partition)\n",
    "    # Its shape is (#chunks * n, n)\n",
    "\n",
    "    R_blocks.visualize(filename=\"fig/R_blocks_graph\", format=\"png\")\n",
    " \n",
    "\n",
    "    #Dask has da.linalg.qr, but it assumes the whole array is large and chunked regularly.\n",
    "    #To get the final global R, you must combine all the Ri\n",
    "    #That means at some point, the data has to come together into a single place (can‚Äôt keep it sharded).\n",
    "    # So we bring the data to the driver because it is very small, because it optimizes the uses of np.linalg.qr, we are gathering the small stuff\n",
    "    R_stack = R_blocks.compute()   # NumPy array, shape (p*n, n)\n",
    "\n",
    "    # Small QR on driver to combine them into the final R\n",
    "    R = np.linalg.qr(R_stack, mode=\"r\")\n",
    "    # delay the computing of qr\n",
    "\n",
    "    # Instead of materializing Q, compute a small R^{-1} (n x n).\n",
    "    I = np.eye(n_cols, dtype=X_da.dtype)\n",
    "    R_inv = solve_triangular(R, I, lower=False)  # stable\n",
    "\n",
    "    Rinv_da = da.from_array(R_inv, chunks=(R_inv.shape[0], R_inv.shape[1]))\n",
    "    Q_da = X_da @ Rinv_da\n",
    "\n",
    "    return Q_da, R      #Q_da because it is lazy, it is still a Dask array\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def indirect_parallel_delayed(X_da):\n",
    "\n",
    "    n_cols = X_da.shape[1]\n",
    "\n",
    "    R_blocks = X_da.map_blocks(compute_R, dtype=X_da.dtype, chunks=(n_cols, n_cols))\n",
    "    # Now R_blocks is a stack of n x n matrices (one per partition)\n",
    "    # Its shape is (#chunks * n, n)\n",
    "\n",
    "\n",
    "    # 2) Convert blocks to delayed NumPy arrays, stack via delayed\n",
    "    R_list = list(R_blocks.to_delayed().ravel())     # each is delayed np.ndarray (n x n)\n",
    "    R_stack = dask.delayed(np.vstack)(R_list)        # delayed (p*n x n)\n",
    "    \n",
    "\n",
    "    R_delayed = dask.delayed(compute_R)(R_stack)      # delayed np.ndarray (n x n)\n",
    "\n",
    "    # 3) Turn the small delayed R into a Dask Array (single (n,n) chunk)\n",
    "    R_da = da.from_delayed(R_delayed, shape=(n_cols, n_cols), dtype=X_da.dtype)\n",
    "\n",
    "    # 4) Materialize small R on driver; solve for R^{-1}\n",
    "    I = np.eye(n_cols, dtype=X_da.dtype)\n",
    "\n",
    "    # compute R^{-1} lazily\n",
    "    R_inv_delayed = dask.delayed(solve_triangular)(R_delayed, I, lower=False)\n",
    "    R_inv_da = da.from_delayed(R_inv_delayed, shape=(n_cols, n_cols), dtype=X_da.dtype)\n",
    "\n",
    "    # 5) Broadcast multiply (keep Q lazy)\n",
    "    Q_da = X_da @ R_inv_da\n",
    "\n",
    "    return Q_da, R_da      #Q_da because it is lazy, it is still a Dask array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d30a4fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size for each chunk is : 0.0264192 Mb\n",
      "CPU times: user 103 ms, sys: 82.4 ms, total: 186 ms\n",
      "Wall time: 64.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "Q, R = indirect_serial(data.data.values, 50)\n",
    "\n",
    "size = 20640 / 50 * 8 * 8 #rows_per_chunk √ó n_cols √ó 8 bytes\n",
    "print(\"The size for each chunk is :\", size/1e6, \"Mb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df2cb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not-delayed version\n",
    "start = time.perf_counter()\n",
    "Q_nd, R_nd = indirect_parallel(X_cached)  # returns Q_da, R (NumPy)\n",
    "_ = R_nd  # already materialized\n",
    "t_nd_R = time.perf_counter() - start\n",
    "\n",
    "\n",
    "\n",
    "#Dask Dashboard: Big green block: compute_R. map stage of mapblock that returns the small Ri\n",
    "# Tiny yellow finalize-hlg block - Dask housekeeping\n",
    "\"\"\"Why you don‚Äôt see ‚Äúreduce‚Äù or ‚Äúbroadcast‚Äù here\n",
    "The reduce to the final \n",
    "R is done on the driver with NumPy:\"\"\"\n",
    "\n",
    "\n",
    "t_nd_Q, val_nd = time_Q_apply(Q_nd, v)\n",
    "\n",
    "\"\"\"Teal blocks around ~140‚Äì150 ms ‚Äî blockwise-matmul-‚Ä¶\n",
    "In the not-delayed variant, R^{-1} is a NumPy constant, so each matmul task deserializes it; you may notice slightly more per-task overhead compared to the delayed/Dask-array constant.\n",
    "Purple block ‚Äî reduction for the norm\n",
    "\n",
    "After the matmul, da.linalg.norm(Qv) triggers a reduction:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503ad3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3. The subtlety\\n\\nPersist = compute and cache now, but still return a Dask collection (with futures).\\n\\nCompute = compute now and return the final NumPy array (collected to driver).\\n\\nSo:\\n\\nIf you persist inside your function and return R, you are indeed returning a Dask object backed by futures, not a NumPy matrix.\\n\\nIf you compute inside your function and return R, you‚Äôre returning a NumPy array, which is often what you want for the small triangular \\nùëÖ\\nR.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not-delayed persisted version\n",
    "start = time.perf_counter()\n",
    "Q_ndp, R_ndp = indirect_parallel_persisted(X_cached)  # returns Q_da, R (NumPy)\n",
    "_ = R_ndp  # already materialized\n",
    "t_nd_Rp = time.perf_counter() - start\n",
    "\n",
    "t_po_Q, val_po = time_Q_apply(Q_ndp, v)\n",
    "\n",
    "\n",
    "# better due to only calculating R once but with a cost\n",
    "\"\"\"3. The subtlety\n",
    "\n",
    "Persist = compute and cache now, but still return a Dask collection (with futures).\n",
    "\n",
    "Compute = compute now and return the final NumPy array (collected to driver).\n",
    "\n",
    "So:\n",
    "\n",
    "If you persist inside your function and return R, you are indeed returning a Dask object backed by futures, not a NumPy matrix.\n",
    "\n",
    "If you compute inside your function and return R, you‚Äôre returning a NumPy array, which is often what you want for the small triangular \n",
    "ùëÖ\n",
    "R.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01f101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# optimized not-delayed version\n",
    "start = time.perf_counter()\n",
    "Q_po, R_po = indirect_parallel_optimized(X_cached)  # returns Q_da (Dask), R_da (Dask)\n",
    "_ = R_po  # already materialized\n",
    "t_po_R = time.perf_counter() - start\n",
    "\n",
    "t_po_Q, val_po = time_Q_apply(Q_po, v)\n",
    "\n",
    "\n",
    "# I don't think this version makes sense\n",
    "\"\"\"Purple (rechunk-merge): small graph-rewiring step. Dask is aligning the chunks of your broadcasted R_da with those of X_da. Because R_da is now itself a Dask Array (1 chunk of size \n",
    "ùëõ\n",
    "√ó\n",
    "ùëõ\n",
    "n√ón), it needs to merge graph metadata.\n",
    "\n",
    "Big teal block (array) dominating the timeline:\n",
    "This is your broadcasted matmul tasks: X_da @ Rinv_da.\n",
    "Since R_inv was wrapped as a Dask Array (da.from_delayed or da.from_array), Dask sees a proper array-on-array multiply and generates blockwise matmul tasks.\n",
    "‚Üí That‚Äôs why you get this large contiguous band of teal tasks: each row-chunk of X_da multiplied with the single small (n,n) chunk of Rinv_da.\"\"\"\n",
    "\"\"\"Earlier, your ‚Äúnot-delayed‚Äù version showed slower Q @ v apply because:\n",
    "\n",
    "You measured at small scales.\n",
    "\n",
    "Serialization overhead per task looked big compared to the tiny math.\n",
    "\n",
    "Wrapping in a Dask array made the graph smaller/cleaner ‚Üí you saw ~0.06 s instead of ~0.5 s.\n",
    "\n",
    "But at larger scales (big \n",
    "ùëö\n",
    "m, many workers), the story flips:\n",
    "\n",
    "Shipping one tiny NumPy array is cheap compared to all the matmuls.\n",
    "\n",
    "Wrapping it as a Dask array adds pointless graph overhead.\n",
    "\n",
    "So you can lose a bit of time overall.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549b9af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fully-delayed version\n",
    "start = time.perf_counter()\n",
    "Q_fd, R_fd = indirect_parallel_delayed(X_cached)  # returns Q_da (Dask), R_da (Dask)\n",
    "_ = R_fd.compute()  # FORCE the same final R compute\n",
    "t_fd_R = time.perf_counter() - start\n",
    "\n",
    "\n",
    "t_fd_Q, val_fd = time_Q_apply(Q_fd, v)\n",
    "\n",
    "\n",
    "\"\"\"Efficiency Implications\n",
    "\n",
    "Computation of \n",
    "ùëÖ\n",
    "R: essentially unchanged, still dominated by the map stage (compute_R).\n",
    "\n",
    "Broadcast of \n",
    "ùëÖ\n",
    "‚àí\n",
    "1\n",
    "R\n",
    "‚àí1\n",
    ": somewhat less efficient as a Dask Array, since it adds extra bookkeeping without reducing the numerical cost.\n",
    "\n",
    "Norm benchmark (Q @ v): the visible red/yellow stages are expected; they confirm that your graph is carrying the computation fully through Dask.\n",
    "\n",
    "because da.from_delayed introduces those extraction tasks.\n",
    "\n",
    "Broadcast multiply still shows up as teal + red.\n",
    "\n",
    "The dashboard is ‚Äúbusier‚Äù ‚Äî more small tasks, more scheduler chatter ‚Äî because everything, even tiny constants, was lifted into the Dask graph.\n",
    "\n",
    "Efficiency interpretation\n",
    "\n",
    "For small \n",
    "ùëõ\n",
    "n: making \n",
    "ùëÖ\n",
    "R a Dask array (fully delayed) adds overhead without real benefit ‚Äî the norm compute shows more yellow/red fragmentation than the optimized/persisted version.\n",
    "\n",
    "For large distributed runs: it‚Äôs still correct, but NumPy constants (or scattered small arrays) are cheaper to handle than wrapping them in Dask.\n",
    "\n",
    "That‚Äôs why your timings showed the fully delayed version wasn‚Äôt consistently faster\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b0490b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R time ‚Äî not-delayed: 0.080s | persisted: 0.110s | optimized: 0.110s | partial: 0.081s | full-delayed: 0.073s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(f\"R time ‚Äî not-delayed: {t_nd_R:.3f}s | persisted: {t_po_Rp:.3f}s | optimized: {t_po_R:.3f}s | | full-delayed: {t_fd_R:.3f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c01a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q apply ‚Äî not-delayed: 0.463s | partial: 0.054s | partial: 0.056s | full-delayed: 0.066s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Q apply ‚Äî not-delayed: {t_nd_Q:.3f}s | partial: {t_po_Q:.3f}s | partial: {t_pd_Q:.3f}s | full-delayed: {t_fd_Q:.3f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "013d8401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input matrix A: m = 10000000, n = 4\n",
      "The 3 blocks are: (3333333, 3333333, 3333334)\n",
      "Total size of A: 320.0 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 305.18 MiB </td>\n",
       "                        <td> 101.73 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (10000000, 4) </td>\n",
       "                        <td> (3333334, 4) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 3 chunks in 1 graph layer </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> float64 numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"75\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"39\" x2=\"25\" y2=\"39\" />\n",
       "  <line x1=\"0\" y1=\"79\" x2=\"25\" y2=\"79\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 25.412616514582485,0.0 25.412616514582485,120.0 0.0,120.0\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.706308\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >4</text>\n",
       "  <text x=\"45.412617\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,45.412617,60.000000)\">10000000</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<random_sample, shape=(10000000, 4), dtype=float64, chunksize=(3333334, 4), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_WORKERS = 3\n",
    "# Initialization of a distributed random matrix\n",
    "m, n = int(1e7), 4\n",
    "chunks = [m // N_WORKERS for _ in range(N_WORKERS-1)]\n",
    "chunks.append(m - sum(chunks))\n",
    "A = da.random.random((m, n), chunks=(chunks, n))\n",
    "\n",
    "# Persist in memory to avoid recomputation\n",
    "A = A.persist() \n",
    "\n",
    "print(f\"Input matrix A: m = {A.shape[0]}, n = {A.shape[1]}\")\n",
    "print(f\"The {len(A.chunks[0])} blocks are: {A.chunks[0]}\")\n",
    "print(f\"Total size of A: {A.nbytes / 1e6} MB\")\n",
    "A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8a8ea09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- OUR IMPLEMENTATION (QR) --\n",
      "CPU times: user 1.06 s, sys: 1.37 s, total: 2.43 s\n",
      "Wall time: 7.86 s\n",
      "\n",
      "-- DASK'S IMPLEMENTATION (QR) --\n",
      "CPU times: user 3.63 s, sys: 2.45 s, total: 6.07 s\n",
      "Wall time: 8.82 s\n"
     ]
    }
   ],
   "source": [
    "# QR decomposition\n",
    "\n",
    "Q_delayed, R_delayed = indirect_parallel_delayed(A)  # Our implementation\n",
    "Q_delayed_dask, R_delayed_dask = da.linalg.tsqr(A)    # Dask's implementation\n",
    "\n",
    "print(\"-- OUR IMPLEMENTATION (QR) --\")\n",
    "%time Q, R = dask.compute(Q_delayed, R_delayed)\n",
    "\n",
    "print(\"\\n-- DASK'S IMPLEMENTATION (QR) --\")\n",
    "%time Q_dask, R_dask = dask.compute(Q_delayed_dask, R_delayed_dask)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dask-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
