{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef8a014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard: http://127.0.0.1:8787/status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-12 17:35:31,766 - tornado.application - ERROR - Uncaught exception GET /status/ws (127.0.0.1)\n",
      "HTTPServerRequest(protocol='http', host='localhost:8787', method='GET', uri='/status/ws', version='HTTP/1.1', remote_ip='127.0.0.1')\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/riccorte/miniconda3/envs/dask-env/lib/python3.11/site-packages/tornado/websocket.py\", line 965, in _accept_connection\n",
      "    open_result = handler.open(*handler.open_args, **handler.open_kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/riccorte/miniconda3/envs/dask-env/lib/python3.11/site-packages/tornado/web.py\", line 3375, in wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/riccorte/miniconda3/envs/dask-env/lib/python3.11/site-packages/bokeh/server/views/ws.py\", line 149, in open\n",
      "    raise ProtocolError(\"Token is expired. Configure the app with a larger value for --session-token-expiration if necessary\")\n",
      "bokeh.protocol.exceptions.ProtocolError: Token is expired. Configure the app with a larger value for --session-token-expiration if necessary\n",
      "2025-09-12 18:38:42,735 - tornado.application - ERROR - Uncaught exception GET /status/ws (127.0.0.1)\n",
      "HTTPServerRequest(protocol='http', host='127.0.0.1:8787', method='GET', uri='/status/ws', version='HTTP/1.1', remote_ip='127.0.0.1')\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/riccorte/miniconda3/envs/dask-env/lib/python3.11/site-packages/tornado/websocket.py\", line 965, in _accept_connection\n",
      "    open_result = handler.open(*handler.open_args, **handler.open_kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/riccorte/miniconda3/envs/dask-env/lib/python3.11/site-packages/tornado/web.py\", line 3375, in wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/riccorte/miniconda3/envs/dask-env/lib/python3.11/site-packages/bokeh/server/views/ws.py\", line 149, in open\n",
      "    raise ProtocolError(\"Token is expired. Configure the app with a larger value for --session-token-expiration if necessary\")\n",
      "bokeh.protocol.exceptions.ProtocolError: Token is expired. Configure the app with a larger value for --session-token-expiration if necessary\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# Example: 4 workers, 1 thread each\n",
    "cluster = LocalCluster(n_workers=4, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "\n",
    "print(\"Dashboard:\", client.dashboard_link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dd1b8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://10.67.22.154:8786' processes=3 threads=3, memory=5.81 GiB>\n",
      "SSHCluster(SSHCluster, 'tcp://10.67.22.154:8786', workers=3, threads=3, memory=5.81 GiB)\n"
     ]
    }
   ],
   "source": [
    "# check if everything went smoothly\n",
    "print(client)\n",
    "print(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ae28abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Dask partitions: 3\n",
      "Length of each partition: 6880 rows\n",
      "Length of the whole dataset: 20640 rows\n"
     ]
    }
   ],
   "source": [
    "import dask.array as da\n",
    "import dask\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.linalg import solve_triangular \n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Download California Housing dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "\n",
    "# Convert features into Dask Array (it's a matrix).\n",
    "n_partition = 3        # number of partition in memory. We have 4 VMS (1 master + 3 workers), so let's start with just 3 partitions\n",
    "length_partition = data.data.shape[0] // n_partition\n",
    "X_da = da.from_array(data.data.values, chunks=(length_partition, data.data.shape[1]))\n",
    "\n",
    "print(\"Number of Dask partitions:\",  X_da.npartitions) \n",
    "print(\"Length of each partition:\", length_partition, \"rows\")\n",
    "print(\"Length of the whole dataset:\", data.data.shape[0], \"rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b933587a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dask.array<array, shape=(20640, 8), dtype=float64, chunksize=(6880, 8), chunktype=numpy.ndarray>\n"
     ]
    }
   ],
   "source": [
    "X_cached = X_da.rechunk((1_000_000, -1)).persist()\n",
    "\n",
    "print(X_da)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e427157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indirect_serial(A, n_div):\n",
    "\n",
    "    \"\"\"\n",
    "    Indirect TSQR (serial, NumPy).\n",
    "    Splits A by rows into n_div blocks, computes local R_i via QR,\n",
    "    reduces to global R by QR on the stacked R_i, then recovers Q = A R^{-1}.\n",
    "    Returns (Q, R).\n",
    "    \"\"\"\n",
    "\n",
    "    n_samp = A.shape[0]\n",
    "    \n",
    "    div_points = int(np.floor(n_samp/n_div))\n",
    "    A_divided = []\n",
    "    Ri = []\n",
    "    \n",
    "    A_divided = [A[div_points * i : div_points * (i + 1)] for i in range(n_div - 1)]\n",
    "    A_divided.append(A[(n_div - 1) * div_points:, :])   # In the case n_samp wasn't divisible by n_div\n",
    "\n",
    "    Ri = [np.linalg.qr(Ai, mode=\"reduced\")[1] for Ai in A_divided]\n",
    "    \n",
    "    R_stack = np.concatenate(Ri, axis = 0)\n",
    "\n",
    "    # Step 3. Stack R_i and compute global R\n",
    "\n",
    "    _, R = np.linalg.qr(R_stack, mode=\"reduced\")\n",
    "\n",
    "\n",
    "    #   I = np.eye(n_samp, dtype=A.dtype)\n",
    "    #   Rinv = solve_triangular(R, I, lower=False)\n",
    "    Rinv = np.linalg.inv(R)\n",
    "    Q = A @ Rinv\n",
    "\n",
    "    return Q, R\n",
    "\n",
    "\n",
    "def compute_R(block):\n",
    "    # np.linalg.qr with mode='r' gives just the R matrix\n",
    "    R = np.linalg.qr(block, mode=\"r\")\n",
    "    return R\n",
    "\n",
    "\n",
    "def indirect_parallel(X_da):\n",
    "\n",
    "    \"\"\"\n",
    "    Indirect TSQR with Dask.\n",
    "    Input:\n",
    "        X_da : Dask Array (m x n), chunked row-wise\n",
    "    Output:\n",
    "        R    : final global triangular factor (n x n, NumPy array on driver)\n",
    "        Q_da : Dask Array (m x n), representing Q = A R^{-1} (lazy)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    n_cols = X_da.shape[1]\n",
    "\n",
    "    R_blocks = X_da.map_blocks(compute_R, dtype=X_da.dtype, chunks=(n_cols, n_cols))\n",
    "    # Now R_blocks is a stack of n x n matrices (one per partition)\n",
    "    # Its shape is (#chunks * n, n)\n",
    "\n",
    "    #R_blocks.visualize(filename=\"fig/R_blocks_graph\", format=\"png\")\n",
    " \n",
    "\n",
    "    #Dask has da.linalg.qr, but it assumes the whole array is large and chunked regularly.\n",
    "    #To get the final global R, you must combine all the Ri\n",
    "    #That means at some point, the data has to come together into a single place (can‚Äôt keep it sharded).\n",
    "    # So we bring the data to the driver because it is very small, because it optimizes the uses of np.linalg.qr, we are gathering the small stuff\n",
    "    R_stack = R_blocks.compute()   # NumPy array, shape (p*n, n)\n",
    "\n",
    "    # Small QR on driver to combine them into the final R\n",
    "    R = np.linalg.qr(R_stack, mode=\"r\")\n",
    "    # delay the computing of qr\n",
    "\n",
    "    # Instead of materializing Q, compute a small R^{-1} (n x n).\n",
    "    I = np.eye(n_cols, dtype=X_da.dtype)\n",
    "    R_inv = solve_triangular(R, I, lower=False)  # stable\n",
    "\n",
    "    # Broadcast Rinv to every chunk: Q = A @ R^{-1}\n",
    "    Q_da = X_da @ R_inv   # still a Dask Array, lazy\n",
    "\n",
    "    return Q_da, R      #Q_da because it is lazy, it is still a Dask array\n",
    "\n",
    "def indirect_parallel_persisted(X_da):\n",
    "\n",
    "    \"\"\"\n",
    "    Indirect TSQR with Dask.\n",
    "    Input:\n",
    "        X_da : Dask Array (m x n), chunked row-wise\n",
    "    Output:\n",
    "        R    : final global triangular factor (n x n, NumPy array on driver)\n",
    "        Q_da : Dask Array (m x n), representing Q = A R^{-1} (lazy)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "    n_cols = X_da.shape[1]\n",
    "\n",
    "    R_blocks = X_da.map_blocks(compute_R, dtype=X_da.dtype, chunks=(n_cols, n_cols))\n",
    "    # Now R_blocks is a stack of n x n matrices (one per partition)\n",
    "    # Its shape is (#chunks * n, n)\n",
    "\n",
    "    #R_blocks.visualize(filename=\"fig/R_blocks_graph\", format=\"png\")\n",
    " \n",
    "\n",
    "    #Dask has da.linalg.qr, but it assumes the whole array is large and chunked regularly.\n",
    "    #To get the final global R, you must combine all the Ri\n",
    "    #That means at some point, the data has to come together into a single place (can‚Äôt keep it sharded).\n",
    "    # So we bring the data to the driver because it is very small, because it optimizes the uses of np.linalg.qr, we are gathering the small stuff\n",
    "    R_stack = R_blocks.persist()   # NumPy array, shape (p*n, n)\n",
    "\n",
    "    # Small QR on driver to combine them into the final R\n",
    "    _, R = np.linalg.qr(R_stack)\n",
    "    # delay the computing of qr\n",
    "\n",
    "    # Instead of materializing Q, compute a small R^{-1} (n x n).\n",
    "    I = np.eye(n_cols, dtype=X_da.dtype)\n",
    "    R_inv = solve_triangular(R, I, lower=False)  # stable\n",
    "\n",
    "    # Broadcast Rinv to every chunk: Q = A @ R^{-1}\n",
    "    Q_da = X_da @ R_inv   # still a Dask Array, lazy\n",
    "\n",
    "    return Q_da, R      #Q_da because it is lazy, it is still a Dask array\n",
    "\n",
    "\n",
    "def indirect_parallel_optimized(X_da):\n",
    "\n",
    "\n",
    "    n_cols = X_da.shape[1]\n",
    "\n",
    "    R_blocks = X_da.map_blocks(compute_R, dtype=X_da.dtype, chunks=(n_cols, n_cols))\n",
    "    # Now R_blocks is a stack of n x n matrices (one per partition)\n",
    "    # Its shape is (#chunks * n, n)\n",
    "\n",
    "    #R_blocks.visualize(filename=\"fig/R_blocks_graph\", format=\"png\")\n",
    " \n",
    "\n",
    "    #Dask has da.linalg.qr, but it assumes the whole array is large and chunked regularly.\n",
    "    #To get the final global R, you must combine all the Ri\n",
    "    #That means at some point, the data has to come together into a single place (can‚Äôt keep it sharded).\n",
    "    # So we bring the data to the driver because it is very small, because it optimizes the uses of np.linalg.qr, we are gathering the small stuff\n",
    "    R_stack = R_blocks.compute()   # NumPy array, shape (p*n, n)\n",
    "\n",
    "    # Small QR on driver to combine them into the final R\n",
    "    R = np.linalg.qr(R_stack, mode=\"r\")\n",
    "    # delay the computing of qr\n",
    "\n",
    "    # Instead of materializing Q, compute a small R^{-1} (n x n).\n",
    "    I = np.eye(n_cols, dtype=X_da.dtype)\n",
    "    R_inv = solve_triangular(R, I, lower=False)  # stable\n",
    "\n",
    "    Rinv_da = da.from_array(R_inv, chunks=(R_inv.shape[0], R_inv.shape[1]))\n",
    "    Q_da = X_da @ Rinv_da\n",
    "\n",
    "    return Q_da, R      #Q_da because it is lazy, it is still a Dask array\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def indirect_parallel_delayed(X_da):\n",
    "\n",
    "    n_cols = X_da.shape[1]\n",
    "\n",
    "    R_blocks = X_da.map_blocks(compute_R, dtype=X_da.dtype, chunks=(n_cols, n_cols))\n",
    "    # Now R_blocks is a stack of n x n matrices (one per partition)\n",
    "    # Its shape is (#chunks * n, n)\n",
    "\n",
    "\n",
    "    # 2) Convert blocks to delayed NumPy arrays, stack via delayed\n",
    "    R_list = list(R_blocks.to_delayed().ravel())     # each is delayed np.ndarray (n x n)\n",
    "    R_stack = dask.delayed(np.vstack)(R_list)        # delayed (p*n x n)\n",
    "    \n",
    "\n",
    "    R_delayed = dask.delayed(compute_R)(R_stack)      # delayed np.ndarray (n x n)\n",
    "\n",
    "    # 3) Turn the small delayed R into a Dask Array (single (n,n) chunk)\n",
    "    R_da = da.from_delayed(R_delayed, shape=(n_cols, n_cols), dtype=X_da.dtype)\n",
    "\n",
    "    # 4) Materialize small R on driver; solve for R^{-1}\n",
    "    I = np.eye(n_cols, dtype=X_da.dtype)\n",
    "\n",
    "    # compute R^{-1} lazily\n",
    "    R_inv_delayed = dask.delayed(solve_triangular)(R_delayed, I, lower=False)\n",
    "    R_inv_da = da.from_delayed(R_inv_delayed, shape=(n_cols, n_cols), dtype=X_da.dtype)\n",
    "\n",
    "    # 5) Broadcast multiply (keep Q lazy)\n",
    "    Q_da = X_da @ R_inv_da\n",
    "\n",
    "    return Q_da, R_da      #Q_da because it is lazy, it is still a Dask array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d30a4fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size for each chunk is : 0.0264192 Mb\n",
      "CPU times: user 103 ms, sys: 82.4 ms, total: 186 ms\n",
      "Wall time: 64.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "Q, R = indirect_serial(data.data.values, 50)\n",
    "\n",
    "size = 20640 / 50 * 8 * 8 #rows_per_chunk √ó n_cols √ó 8 bytes\n",
    "print(\"The size for each chunk is :\", size/1e6, \"Mb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63709ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final R shape: (8, 8)\n",
      "Q is lazy: dask.array<getitem, shape=(20640, 8), dtype=float64, chunksize=(6880, 8), chunktype=numpy.ndarray>\n",
      "Chunks: ((6880, 6880, 6880), (8,))\n",
      "Number of partitions: 3\n",
      "Total size [MB]: 1.32096\n",
      "Chunk size [MB]: 0.44032\n",
      "Shape: (20640, 8)\n",
      "Chunks: ((6880, 6880, 6880), (8,))\n",
      "Partitions: 3\n",
      "Total size [MB]: 1.32096\n",
      "Avg chunk [MB]: 0.44032\n",
      "CPU times: user 65 ms, sys: 26.2 ms, total: 91.2 ms\n",
      "Wall time: 146 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "Q_da, R = indirect_parallel(X_da)   # X_da is already splitted into three partitions\n",
    "\n",
    "\n",
    "print(\"Final R shape:\", R.shape)  # (n, n), small\n",
    "print(\"Q is lazy:\", Q_da)         # Dask Array, not yet computed\n",
    "\n",
    "print(\"Chunks:\", X_da.chunks)\n",
    "print(\"Number of partitions:\", X_da.npartitions)\n",
    "print(\"Total size [MB]:\", X_da.nbytes / 1e6)\n",
    "print(\"Chunk size [MB]:\", X_da.nbytes / X_da.npartitions / 1e6)    # 10‚Äì100 MB\n",
    "\n",
    "print(\"Shape:\", X_da.shape)\n",
    "print(\"Chunks:\", X_da.chunks)\n",
    "print(\"Partitions:\", X_da.npartitions)\n",
    "print(\"Total size [MB]:\", X_da.nbytes/1e6)\n",
    "print(\"Avg chunk [MB]:\", (X_da.nbytes/X_da.npartitions)/1e6)\n",
    "\n",
    "\n",
    "#print(\"Compact sanity check:\", X_da.shape, \"(m,n);\", R_blocks.shape, \"(n *n_partitions, n);\", R.shape, \"(n,n)\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8d5e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g., apply Q to a random vector and compute the norm ‚Äî same workload for all\n",
    "v = np.random.randn(X_cached.shape[1]).astype(X_cached.dtype)\n",
    "\n",
    "def time_Q_apply(Q_da, v):\n",
    "    start = time.perf_counter()\n",
    "    val = da.linalg.norm(Q_da @ v).compute()  # triggers the matmul + reduction\n",
    "    end = time.perf_counter()\n",
    "    return end - start, val\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df2cb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not-delayed version\n",
    "start = time.perf_counter()\n",
    "Q_nd, R_nd = indirect_parallel(X_cached)  # returns Q_da, R (NumPy)\n",
    "_ = R_nd  # already materialized\n",
    "t_nd_R = time.perf_counter() - start\n",
    "\n",
    "\n",
    "\n",
    "#Dask Dashboard: Big green block: compute_R. map stage of mapblock that returns the small Ri\n",
    "# Tiny yellow finalize-hlg block - Dask housekeeping\n",
    "\"\"\"Why you don‚Äôt see ‚Äúreduce‚Äù or ‚Äúbroadcast‚Äù here\n",
    "The reduce to the final \n",
    "R is done on the driver with NumPy:\"\"\"\n",
    "\n",
    "\n",
    "t_nd_Q, val_nd = time_Q_apply(Q_nd, v)\n",
    "\n",
    "\"\"\"Teal blocks around ~140‚Äì150 ms ‚Äî blockwise-matmul-‚Ä¶\n",
    "In the not-delayed variant, R^{-1} is a NumPy constant, so each matmul task deserializes it; you may notice slightly more per-task overhead compared to the delayed/Dask-array constant.\n",
    "Purple block ‚Äî reduction for the norm\n",
    "\n",
    "After the matmul, da.linalg.norm(Qv) triggers a reduction:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503ad3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3. The subtlety\\n\\nPersist = compute and cache now, but still return a Dask collection (with futures).\\n\\nCompute = compute now and return the final NumPy array (collected to driver).\\n\\nSo:\\n\\nIf you persist inside your function and return R, you are indeed returning a Dask object backed by futures, not a NumPy matrix.\\n\\nIf you compute inside your function and return R, you‚Äôre returning a NumPy array, which is often what you want for the small triangular \\nùëÖ\\nR.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not-delayed persisted version\n",
    "start = time.perf_counter()\n",
    "Q_ndp, R_ndp = indirect_parallel_persisted(X_cached)  # returns Q_da, R (NumPy)\n",
    "_ = R_ndp  # already materialized\n",
    "t_nd_Rp = time.perf_counter() - start\n",
    "\n",
    "t_po_Q, val_po = time_Q_apply(Q_ndp, v)\n",
    "\n",
    "\n",
    "# better due to only calculating R once but with a cost\n",
    "\"\"\"3. The subtlety\n",
    "\n",
    "Persist = compute and cache now, but still return a Dask collection (with futures).\n",
    "\n",
    "Compute = compute now and return the final NumPy array (collected to driver).\n",
    "\n",
    "So:\n",
    "\n",
    "If you persist inside your function and return R, you are indeed returning a Dask object backed by futures, not a NumPy matrix.\n",
    "\n",
    "If you compute inside your function and return R, you‚Äôre returning a NumPy array, which is often what you want for the small triangular \n",
    "ùëÖ\n",
    "R.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01f101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# optimized not-delayed version\n",
    "start = time.perf_counter()\n",
    "Q_po, R_po = indirect_parallel_optimized(X_cached)  # returns Q_da (Dask), R_da (Dask)\n",
    "_ = R_po  # already materialized\n",
    "t_po_R = time.perf_counter() - start\n",
    "\n",
    "t_po_Q, val_po = time_Q_apply(Q_po, v)\n",
    "\n",
    "\n",
    "# I don't think this version makes sense\n",
    "\"\"\"Purple (rechunk-merge): small graph-rewiring step. Dask is aligning the chunks of your broadcasted R_da with those of X_da. Because R_da is now itself a Dask Array (1 chunk of size \n",
    "ùëõ\n",
    "√ó\n",
    "ùëõ\n",
    "n√ón), it needs to merge graph metadata.\n",
    "\n",
    "Big teal block (array) dominating the timeline:\n",
    "This is your broadcasted matmul tasks: X_da @ Rinv_da.\n",
    "Since R_inv was wrapped as a Dask Array (da.from_delayed or da.from_array), Dask sees a proper array-on-array multiply and generates blockwise matmul tasks.\n",
    "‚Üí That‚Äôs why you get this large contiguous band of teal tasks: each row-chunk of X_da multiplied with the single small (n,n) chunk of Rinv_da.\"\"\"\n",
    "\"\"\"Earlier, your ‚Äúnot-delayed‚Äù version showed slower Q @ v apply because:\n",
    "\n",
    "You measured at small scales.\n",
    "\n",
    "Serialization overhead per task looked big compared to the tiny math.\n",
    "\n",
    "Wrapping in a Dask array made the graph smaller/cleaner ‚Üí you saw ~0.06 s instead of ~0.5 s.\n",
    "\n",
    "But at larger scales (big \n",
    "ùëö\n",
    "m, many workers), the story flips:\n",
    "\n",
    "Shipping one tiny NumPy array is cheap compared to all the matmuls.\n",
    "\n",
    "Wrapping it as a Dask array adds pointless graph overhead.\n",
    "\n",
    "So you can lose a bit of time overall.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549b9af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fully-delayed version\n",
    "start = time.perf_counter()\n",
    "Q_fd, R_fd = indirect_parallel_delayed(X_cached)  # returns Q_da (Dask), R_da (Dask)\n",
    "_ = R_fd.compute()  # FORCE the same final R compute\n",
    "t_fd_R = time.perf_counter() - start\n",
    "\n",
    "\n",
    "t_fd_Q, val_fd = time_Q_apply(Q_fd, v)\n",
    "\n",
    "\n",
    "\"\"\"Efficiency Implications\n",
    "\n",
    "Computation of \n",
    "ùëÖ\n",
    "R: essentially unchanged, still dominated by the map stage (compute_R).\n",
    "\n",
    "Broadcast of \n",
    "ùëÖ\n",
    "‚àí\n",
    "1\n",
    "R\n",
    "‚àí1\n",
    ": somewhat less efficient as a Dask Array, since it adds extra bookkeeping without reducing the numerical cost.\n",
    "\n",
    "Norm benchmark (Q @ v): the visible red/yellow stages are expected; they confirm that your graph is carrying the computation fully through Dask.\n",
    "\n",
    "because da.from_delayed introduces those extraction tasks.\n",
    "\n",
    "Broadcast multiply still shows up as teal + red.\n",
    "\n",
    "The dashboard is ‚Äúbusier‚Äù ‚Äî more small tasks, more scheduler chatter ‚Äî because everything, even tiny constants, was lifted into the Dask graph.\n",
    "\n",
    "Efficiency interpretation\n",
    "\n",
    "For small \n",
    "ùëõ\n",
    "n: making \n",
    "ùëÖ\n",
    "R a Dask array (fully delayed) adds overhead without real benefit ‚Äî the norm compute shows more yellow/red fragmentation than the optimized/persisted version.\n",
    "\n",
    "For large distributed runs: it‚Äôs still correct, but NumPy constants (or scattered small arrays) are cheaper to handle than wrapping them in Dask.\n",
    "\n",
    "That‚Äôs why your timings showed the fully delayed version wasn‚Äôt consistently faster\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b0490b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R time ‚Äî not-delayed: 0.080s | persisted: 0.110s | optimized: 0.110s | partial: 0.081s | full-delayed: 0.073s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(f\"R time ‚Äî not-delayed: {t_nd_R:.3f}s | persisted: {t_po_Rp:.3f}s | optimized: {t_po_R:.3f}s | | full-delayed: {t_fd_R:.3f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c01a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q apply ‚Äî not-delayed: 0.463s | partial: 0.054s | partial: 0.056s | full-delayed: 0.066s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(f\"Q apply ‚Äî not-delayed: {t_nd_Q:.3f}s | partial: {t_po_Q:.3f}s | partial: {t_pd_Q:.3f}s | full-delayed: {t_fd_Q:.3f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "013d8401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input matrix A: m = 10000000, n = 4\n",
      "The 3 blocks are: (3333333, 3333333, 3333334)\n",
      "Total size of A: 320.0 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 305.18 MiB </td>\n",
       "                        <td> 101.73 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (10000000, 4) </td>\n",
       "                        <td> (3333334, 4) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 3 chunks in 1 graph layer </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> float64 numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"75\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"39\" x2=\"25\" y2=\"39\" />\n",
       "  <line x1=\"0\" y1=\"79\" x2=\"25\" y2=\"79\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 25.412616514582485,0.0 25.412616514582485,120.0 0.0,120.0\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.706308\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >4</text>\n",
       "  <text x=\"45.412617\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,45.412617,60.000000)\">10000000</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<random_sample, shape=(10000000, 4), dtype=float64, chunksize=(3333334, 4), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_WORKERS = 3\n",
    "# Initialization of a distributed random matrix\n",
    "m, n = int(1e7), 4\n",
    "chunks = [m // N_WORKERS for _ in range(N_WORKERS-1)]\n",
    "chunks.append(m - sum(chunks))\n",
    "A = da.random.random((m, n), chunks=(chunks, n))\n",
    "\n",
    "# Persist in memory to avoid recomputation\n",
    "A = A.persist() \n",
    "\n",
    "print(f\"Input matrix A: m = {A.shape[0]}, n = {A.shape[1]}\")\n",
    "print(f\"The {len(A.chunks[0])} blocks are: {A.chunks[0]}\")\n",
    "print(f\"Total size of A: {A.nbytes / 1e6} MB\")\n",
    "A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b8a8ea09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- OUR IMPLEMENTATION (QR) --\n",
      "CPU times: user 940 ms, sys: 380 ms, total: 1.32 s\n",
      "Wall time: 599 ms\n",
      "\n",
      "-- DASK'S IMPLEMENTATION (QR) --\n",
      "CPU times: user 3.9 s, sys: 1.03 s, total: 4.93 s\n",
      "Wall time: 1.62 s\n"
     ]
    }
   ],
   "source": [
    "# QR decomposition\n",
    "\n",
    "Q_delayed, R_delayed = indirect_parallel_persisted(A)  # Our implementation\n",
    "Q_delayed_dask, R_delayed_dask = da.linalg.tsqr(A)    # Dask's implementation\n",
    "\n",
    "print(\"-- OUR IMPLEMENTATION (QR) --\")\n",
    "%time Q, R = dask.compute(Q_delayed, R_delayed)\n",
    "\n",
    "print(\"\\n-- DASK'S IMPLEMENTATION (QR) --\")\n",
    "%time Q_dask, R_dask = dask.compute(Q_delayed_dask, R_delayed_dask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb9e3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create again a cluster\n",
    "# CLUSTER DEPLOYMENT ON CLOUDVENETO\n",
    "\n",
    "from dask.distributed import Client, SSHCluster\n",
    "\n",
    "cluster = SSHCluster(\n",
    "    [\"10.67.22.154\", \"10.67.22.216\", \"10.67.22.116\", \"10.67.22.113\"],\n",
    "    connect_options={\"known_hosts\": None},\n",
    "    remote_python=\"/home/ubuntu/miniconda3/bin/python\",\n",
    "    scheduler_options={\"port\": 8786, \"dashboard_address\": \":8797\"},\n",
    "    worker_options={\n",
    "        \"nprocs\": 4,        # Now we use all 4 cores -> 12 workers\n",
    "        \"nthreads\": 1       # We use 1 threads. Following Dask documentation, however, Numpy should release well the GIL lock thus we could use more than 1 thread. \n",
    "    }\n",
    ")\n",
    "\n",
    "client = Client(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cc4e2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://10.67.22.154:8786' processes=12 threads=12, memory=23.25 GiB>\n",
      "SSHCluster(SSHCluster, 'tcp://10.67.22.154:8786', workers=12, threads=12, memory=23.25 GiB)\n"
     ]
    }
   ],
   "source": [
    "print(client)\n",
    "print(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94a9fd97-f867-4780-9fa4-e39f8d3cd49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure your dataset is loaded & persisted\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.chdir(\"/home/ubuntu\")\n",
    "path_HIGGS = os.path.join(os.getcwd(), \"datasets\", \"HIGGS.csv\")\n",
    "\n",
    "df = dd.read_csv(path_HIGGS, header=None, blocksize=\"200MB\")\n",
    "X_da = df.iloc[:, 1:].to_dask_array(lengths=True).astype(np.float32)\n",
    "X_da = X_da.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab0e1cf0-789d-4535-89e6-33fce18a3467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import solve_triangular "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ce2e2b7-258f-4967-8595-d1d8afcd4c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.07 ms, sys: 3.23 ms, total: 10.3 ms\n",
      "Wall time: 637 ms\n"
     ]
    }
   ],
   "source": [
    "# --- Now time your function ---\n",
    "%time Q, R = indirect_parallel(X_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e6aa751-a6dc-4742-aedc-572c27024add",
   "metadata": {},
   "outputs": [
    {
     "ename": "FutureCancelledError",
     "evalue": "finalize-hlgfinalizecompute-24f1f03f15b44280ac77d18e10a81e91 cancelled for reason: lost dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFutureCancelledError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m start = time.time()\n\u001b[32m      4\u001b[39m X_da = X_da.persist()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m Q, R1 = \u001b[43mindirect_parallel_persisted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_da\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m wait([Q, R1])\n\u001b[32m      7\u001b[39m end = time.time()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 116\u001b[39m, in \u001b[36mindirect_parallel_persisted\u001b[39m\u001b[34m(X_da)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# delay the computing of qr\u001b[39;00m\n\u001b[32m    113\u001b[39m \n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# Instead of materializing Q, compute a small R^{-1} (n x n).\u001b[39;00m\n\u001b[32m    115\u001b[39m I = np.eye(n_cols, dtype=X_da.dtype)\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m R_inv = \u001b[43msolve_triangular\u001b[49m\u001b[43m(\u001b[49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# stable\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# Broadcast Rinv to every chunk: Q = A @ R^{-1}\u001b[39;00m\n\u001b[32m    119\u001b[39m Q_da = X_da @ R_inv   \u001b[38;5;66;03m# still a Dask Array, lazy\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/scipy/_lib/_util.py:1214\u001b[39m, in \u001b[36m_apply_over_batch.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1212\u001b[39m core_shapes = []\n\u001b[32m   1213\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, (array, ndim) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(arrays, ndims)):\n\u001b[32m-> \u001b[39m\u001b[32m1214\u001b[39m     array = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m array \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1215\u001b[39m     shape = () \u001b[38;5;28;01mif\u001b[39;00m array \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m array.shape\n\u001b[32m   1217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ndim == \u001b[33m\"\u001b[39m\u001b[33m1|2\u001b[39m\u001b[33m\"\u001b[39m:  \u001b[38;5;66;03m# special case for `solve`, etc.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/dask/array/core.py:1729\u001b[39m, in \u001b[36mArray.__array__\u001b[39m\u001b[34m(self, dtype, copy, **kwargs)\u001b[39m\n\u001b[32m   1722\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m   1723\u001b[39m     warnings.warn(\n\u001b[32m   1724\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt acquire a memory view of a Dask array. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1725\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis will raise in the future.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1726\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m   1727\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1729\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1731\u001b[39m \u001b[38;5;66;03m# Apply requested dtype and convert non-numpy backends to numpy.\u001b[39;00m\n\u001b[32m   1732\u001b[39m \u001b[38;5;66;03m# If copy is True, numpy is going to perform its own deep copy\u001b[39;00m\n\u001b[32m   1733\u001b[39m \u001b[38;5;66;03m# after this method returns.\u001b[39;00m\n\u001b[32m   1734\u001b[39m \u001b[38;5;66;03m# If copy is None, finalize() ensures that the returned object\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;66;03m# does not share memory with an object stored in the graph or on a\u001b[39;00m\n\u001b[32m   1736\u001b[39m \u001b[38;5;66;03m# process-local Worker.\u001b[39;00m\n\u001b[32m   1737\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.asarray(x, dtype=dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/dask/base.py:373\u001b[39m, in \u001b[36mDaskMethodsMixin.compute\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs):\n\u001b[32m    350\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[32m    351\u001b[39m \n\u001b[32m    352\u001b[39m \u001b[33;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    371\u001b[39m \u001b[33;03m    dask.compute\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m     (result,) = \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/dask/base.py:681\u001b[39m, in \u001b[36mcompute\u001b[39m\u001b[34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[39m\n\u001b[32m    678\u001b[39m     expr = expr.optimize()\n\u001b[32m    679\u001b[39m     keys = \u001b[38;5;28mlist\u001b[39m(flatten(expr.__dask_keys__()))\n\u001b[32m--> \u001b[39m\u001b[32m681\u001b[39m     results = \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m repack(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/distributed/client.py:2417\u001b[39m, in \u001b[36mClient._gather\u001b[39m\u001b[34m(self, futures, errors, direct, local_worker)\u001b[39m\n\u001b[32m   2415\u001b[39m     exception = st.exception\n\u001b[32m   2416\u001b[39m     traceback = st.traceback\n\u001b[32m-> \u001b[39m\u001b[32m2417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception.with_traceback(traceback)\n\u001b[32m   2418\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mskip\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2419\u001b[39m     bad_keys.add(key)\n",
      "\u001b[31mFutureCancelledError\u001b[39m: finalize-hlgfinalizecompute-24f1f03f15b44280ac77d18e10a81e91 cancelled for reason: lost dependencies."
     ]
    }
   ],
   "source": [
    "import time\n",
    "from dask.distributed import wait\n",
    "start = time.time()\n",
    "X_da = X_da.persist()\n",
    "Q, R1 = indirect_parallel_persisted(X_da)\n",
    "wait([Q, R1])\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8f621a6-2f90-4fcd-9f56-ebe16ba57bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "del R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3278d7d3-2067-4b89-98d2-4c08ae3d7617",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time Q_da, R_da = indirect_parallel_optimized(X_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b54b885d-19f2-4dcc-a690-aecae6f823c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.58 ms, sys: 0 ns, total: 7.58 ms\n",
      "Wall time: 1.79 s\n"
     ]
    }
   ],
   "source": [
    "Q_da, R_da = indirect_parallel_delayed(X_da)\n",
    "%time R = R_da.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8f6b6f6a-07c8-4270-9b89-204812ae9d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27c55f4-9e72-4352-b6a5-f497e2ea8231",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
