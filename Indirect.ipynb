{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ef8a014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard: http://127.0.0.1:8787/status\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# Example: 4 workers, 1 thread each\n",
    "cluster = LocalCluster(n_workers=4, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "\n",
    "print(\"Dashboard:\", client.dashboard_link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce40f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLUSTER DEPLOYMENT ON CLOUDVENETO\n",
    "from dask.distributed import Client, SSHCluster\n",
    "\n",
    "cluster = SSHCluster(\n",
    "    [\"10.67.22.154\", \"10.67.22.216\", \"10.67.22.116\", \"10.67.22.113\"],\n",
    "    connect_options={\"known_hosts\": None},\n",
    "    remote_python=\"/home/ubuntu/miniconda3/bin/python\",\n",
    "    scheduler_options={\"port\": 8786, \"dashboard_address\": \":8797\"},\n",
    "    worker_options={\n",
    "        \"nprocs\": 1,       # N. of processess per VM. CloudVeneto's large VM offers 4-core CPU, but for now we only spawn 1 process per VM\n",
    "        \"nthreads\": 1      # N. of threads per process\n",
    "    }\n",
    ")\n",
    "\n",
    "client = Client(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dd1b8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://127.0.0.1:39081' processes=4 threads=4, memory=5.79 GiB>\n",
      "LocalCluster(8cdcf355, 'tcp://127.0.0.1:39081', workers=4, threads=4, memory=5.79 GiB)\n"
     ]
    }
   ],
   "source": [
    "# check if everything went smoothly\n",
    "print(client)\n",
    "print(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ae28abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Dask partitions: 3\n",
      "Length of each partition: 6880 rows\n",
      "Length of the whole dataset: 20640 rows\n"
     ]
    }
   ],
   "source": [
    "import dask.array as da\n",
    "import dask\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.linalg import solve_triangular \n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Download California Housing dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "\n",
    "# Convert features into Dask Array (it's a matrix).\n",
    "n_partition = 3        # number of partition in memory. We have 4 VMS (1 master + 3 workers), so let's start with just 3 partitions\n",
    "length_partition = data.data.shape[0] // n_partition\n",
    "X_da = da.from_array(data.data.values, chunks=(length_partition, data.data.shape[1]))\n",
    "\n",
    "print(\"Number of Dask partitions:\",  X_da.npartitions) \n",
    "print(\"Length of each partition:\", length_partition, \"rows\")\n",
    "print(\"Length of the whole dataset:\", data.data.shape[0], \"rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b933587a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dask.array<array, shape=(20640, 8), dtype=float64, chunksize=(6880, 8), chunktype=numpy.ndarray>\n"
     ]
    }
   ],
   "source": [
    "X_cached = X_da.rechunk((1_000_000, -1)).persist()\n",
    "\n",
    "print(X_da)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e427157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indirect_serial(A, n_div):\n",
    "\n",
    "    \"\"\"\n",
    "    Indirect TSQR (serial, NumPy).\n",
    "    Splits A by rows into n_div blocks, computes local R_i via QR,\n",
    "    reduces to global R by QR on the stacked R_i, then recovers Q = A R^{-1}.\n",
    "    Returns (Q, R).\n",
    "    \"\"\"\n",
    "\n",
    "    n_samp = A.shape[0]\n",
    "    \n",
    "    div_points = int(np.floor(n_samp/n_div))\n",
    "    A_divided = []\n",
    "    Ri = []\n",
    "    \n",
    "    A_divided = [A[div_points * i : div_points * (i + 1)] for i in range(n_div - 1)]\n",
    "    A_divided.append(A[(n_div - 1) * div_points:, :])   # In the case n_samp wasn't divisible by n_div\n",
    "\n",
    "    Ri = [np.linalg.qr(Ai, mode=\"reduced\")[1] for Ai in A_divided]\n",
    "    \n",
    "    R_stack = np.concatenate(Ri, axis = 0)\n",
    "\n",
    "    # Step 3. Stack R_i and compute global R\n",
    "\n",
    "    _, R = np.linalg.qr(R_stack, mode=\"reduced\")\n",
    "\n",
    "\n",
    "    #   I = np.eye(n_samp, dtype=A.dtype)\n",
    "    #   Rinv = solve_triangular(R, I, lower=False)\n",
    "    Rinv = np.linalg.inv(R)\n",
    "    Q = A @ Rinv\n",
    "\n",
    "    return Q, R\n",
    "\n",
    "\n",
    "def compute_R(block):\n",
    "    # np.linalg.qr with mode='r' gives just the R matrix\n",
    "    R = np.linalg.qr(block, mode=\"r\")\n",
    "    return R\n",
    "\n",
    "\n",
    "def indirect_parallel(X_da):\n",
    "\n",
    "    \"\"\"\n",
    "    Indirect TSQR with Dask.\n",
    "    Input:\n",
    "        X_da : Dask Array (m x n), chunked row-wise\n",
    "    Output:\n",
    "        R    : final global triangular factor (n x n, NumPy array on driver)\n",
    "        Q_da : Dask Array (m x n), representing Q = A R^{-1} (lazy)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    n_cols = X_da.shape[1]\n",
    "\n",
    "    R_blocks = X_da.map_blocks(compute_R, dtype=X_da.dtype, chunks=(n_cols, n_cols))\n",
    "    # Now R_blocks is a stack of n x n matrices (one per partition)\n",
    "    # Its shape is (#chunks * n, n)\n",
    "\n",
    "    R_blocks.visualize(filename=\"fig/R_blocks_graph\", format=\"png\")\n",
    " \n",
    "\n",
    "    #Dask has da.linalg.qr, but it assumes the whole array is large and chunked regularly.\n",
    "    #To get the final global R, you must combine all the Ri\n",
    "    #That means at some point, the data has to come together into a single place (can’t keep it sharded).\n",
    "    # So we bring the data to the driver because it is very small, because it optimizes the uses of np.linalg.qr, we are gathering the small stuff\n",
    "    R_stack = R_blocks.compute()   # NumPy array, shape (p*n, n)\n",
    "\n",
    "    # Small QR on driver to combine them into the final R\n",
    "    R = np.linalg.qr(R_stack, mode=\"r\")\n",
    "    # delay the computing of qr\n",
    "\n",
    "    # Instead of materializing Q, compute a small R^{-1} (n x n).\n",
    "    I = np.eye(n_cols, dtype=X_da.dtype)\n",
    "    R_inv = solve_triangular(R, I, lower=False)  # stable\n",
    "\n",
    "    # Broadcast Rinv to every chunk: Q = A @ R^{-1}\n",
    "    Q_da = X_da @ R_inv   # still a Dask Array, lazy\n",
    "\n",
    "    return Q_da, R      #Q_da because it is lazy, it is still a Dask array\n",
    "\n",
    "\n",
    "def indirect_parallel_optimized(X_da):\n",
    "\n",
    "\n",
    "    n_cols = X_da.shape[1]\n",
    "\n",
    "    R_blocks = X_da.map_blocks(compute_R, dtype=X_da.dtype, chunks=(n_cols, n_cols))\n",
    "    # Now R_blocks is a stack of n x n matrices (one per partition)\n",
    "    # Its shape is (#chunks * n, n)\n",
    "\n",
    "    R_blocks.visualize(filename=\"fig/R_blocks_graph\", format=\"png\")\n",
    " \n",
    "\n",
    "    #Dask has da.linalg.qr, but it assumes the whole array is large and chunked regularly.\n",
    "    #To get the final global R, you must combine all the Ri\n",
    "    #That means at some point, the data has to come together into a single place (can’t keep it sharded).\n",
    "    # So we bring the data to the driver because it is very small, because it optimizes the uses of np.linalg.qr, we are gathering the small stuff\n",
    "    R_stack = R_blocks.compute()   # NumPy array, shape (p*n, n)\n",
    "\n",
    "    # Small QR on driver to combine them into the final R\n",
    "    R = np.linalg.qr(R_stack, mode=\"r\")\n",
    "    # delay the computing of qr\n",
    "\n",
    "    # Instead of materializing Q, compute a small R^{-1} (n x n).\n",
    "    I = np.eye(n_cols, dtype=X_da.dtype)\n",
    "    R_inv = solve_triangular(R, I, lower=False)  # stable\n",
    "\n",
    "    Rinv_da = da.from_array(R_inv, chunks=(R_inv.shape[0], R_inv.shape[1]))\n",
    "    Q_da = X_da @ Rinv_da\n",
    "\n",
    "    return Q_da, R      #Q_da because it is lazy, it is still a Dask array\n",
    "\n",
    "\n",
    "\n",
    "def indirect_parallel_partially_delayed(X_da):\n",
    "\n",
    "    n_cols = X_da.shape[1]\n",
    "\n",
    "    R_blocks = X_da.map_blocks(compute_R, dtype=X_da.dtype, chunks=(n_cols, n_cols))\n",
    "    # Now R_blocks is a stack of n x n matrices (one per partition)\n",
    "    # Its shape is (#chunks * n, n)\n",
    "\n",
    "    R_blocks.visualize(filename=\"fig/R_blocks_graph\", format=\"png\")\n",
    "\n",
    "    # 2) Convert blocks to delayed NumPy arrays, stack via delayed\n",
    "    R_list = list(R_blocks.to_delayed().ravel())     # each is delayed np.ndarray (n x n)\n",
    "    R_stack = dask.delayed(np.vstack)(R_list)        # delayed (p*n x n)\n",
    "\n",
    "\n",
    "    R_da = dask.delayed(compute_R)(R_stack)      # delayed np.ndarray (n x n)\n",
    "\n",
    "    # 4) Materialize small R on driver; solve for R^{-1}\n",
    "    R = R_da.compute()                           # NumPy (n x n)\n",
    "    I = np.eye(n_cols, dtype=X_da.dtype)\n",
    "    R_inv = solve_triangular(R, I, lower=False)       # NumPy (n x n)\n",
    "\n",
    "    # 5) Broadcast multiply (keep Q lazy)\n",
    "    Q_da = X_da @ R_inv\n",
    "\n",
    "    return Q_da, R      #Q_da because it is lazy, it is still a Dask array\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def indirect_parallel_delayed(X_da):\n",
    "\n",
    "    n_cols = X_da.shape[1]\n",
    "\n",
    "    R_blocks = X_da.map_blocks(compute_R, dtype=X_da.dtype, chunks=(n_cols, n_cols))\n",
    "    # Now R_blocks is a stack of n x n matrices (one per partition)\n",
    "    # Its shape is (#chunks * n, n)\n",
    "\n",
    "\n",
    "    # 2) Convert blocks to delayed NumPy arrays, stack via delayed\n",
    "    R_list = list(R_blocks.to_delayed().ravel())     # each is delayed np.ndarray (n x n)\n",
    "    R_stack = dask.delayed(np.vstack)(R_list)        # delayed (p*n x n)\n",
    "    \n",
    "\n",
    "    R_delayed = dask.delayed(compute_R)(R_stack)      # delayed np.ndarray (n x n)\n",
    "\n",
    "    # 3) Turn the small delayed R into a Dask Array (single (n,n) chunk)\n",
    "    R_da = da.from_delayed(R_delayed, shape=(n_cols, n_cols), dtype=X_da.dtype)\n",
    "\n",
    "    # 4) Materialize small R on driver; solve for R^{-1}\n",
    "    I = np.eye(n_cols, dtype=X_da.dtype)\n",
    "\n",
    "    # compute R^{-1} lazily\n",
    "    R_inv_delayed = dask.delayed(solve_triangular)(R_delayed, I, lower=False)\n",
    "    R_inv_da = da.from_delayed(R_inv_delayed, shape=(n_cols, n_cols), dtype=X_da.dtype)\n",
    "\n",
    "    # 5) Broadcast multiply (keep Q lazy)\n",
    "    Q_da = X_da @ R_inv_da\n",
    "\n",
    "    return Q_da, R_da      #Q_da because it is lazy, it is still a Dask array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d30a4fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size for each chunk is : 0.0264192 Mb\n",
      "CPU times: user 95.4 ms, sys: 0 ns, total: 95.4 ms\n",
      "Wall time: 36.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "Q, R = indirect_serial(data.data.values, 50)\n",
    "\n",
    "size = 20640 / 50 * 8 * 8 #rows_per_chunk × n_cols × 8 bytes\n",
    "print(\"The size for each chunk is :\", size/1e6, \"Mb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63709ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final R shape: (8, 8)\n",
      "Q is lazy: dask.array<getitem, shape=(20640, 8), dtype=float64, chunksize=(6880, 8), chunktype=numpy.ndarray>\n",
      "Chunks: ((6880, 6880, 6880), (8,))\n",
      "Number of partitions: 3\n",
      "Total size [MB]: 1.32096\n",
      "Chunk size [MB]: 0.44032\n",
      "Shape: (20640, 8)\n",
      "Chunks: ((6880, 6880, 6880), (8,))\n",
      "Partitions: 3\n",
      "Total size [MB]: 1.32096\n",
      "Avg chunk [MB]: 0.44032\n",
      "CPU times: user 162 ms, sys: 0 ns, total: 162 ms\n",
      "Wall time: 851 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "Q_da, R = indirect_parallel(X_da)   # X_da is already splitted into three partitions\n",
    "\n",
    "\n",
    "print(\"Final R shape:\", R.shape)  # (n, n), small\n",
    "print(\"Q is lazy:\", Q_da)         # Dask Array, not yet computed\n",
    "\n",
    "print(\"Chunks:\", X_da.chunks)\n",
    "print(\"Number of partitions:\", X_da.npartitions)\n",
    "print(\"Total size [MB]:\", X_da.nbytes / 1e6)\n",
    "print(\"Chunk size [MB]:\", X_da.nbytes / X_da.npartitions / 1e6)    # 10–100 MB\n",
    "\n",
    "print(\"Shape:\", X_da.shape)\n",
    "print(\"Chunks:\", X_da.chunks)\n",
    "print(\"Partitions:\", X_da.npartitions)\n",
    "print(\"Total size [MB]:\", X_da.nbytes/1e6)\n",
    "print(\"Avg chunk [MB]:\", (X_da.nbytes/X_da.npartitions)/1e6)\n",
    "\n",
    "\n",
    "#print(\"Compact sanity check:\", X_da.shape, \"(m,n);\", R_blocks.shape, \"(n *n_partitions, n);\", R.shape, \"(n,n)\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6df2cb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R time — not-delayed: 0.107s | optimized: 0.066s | partial: 0.052s | full-delayed: 0.032s\n"
     ]
    }
   ],
   "source": [
    "# not-delayed version\n",
    "start = time.perf_counter()\n",
    "Q_nd, R_nd = indirect_parallel(X_cached)  # returns Q_da, R (NumPy)\n",
    "_ = R_nd  # already materialized\n",
    "t_nd_R = time.perf_counter() - start\n",
    "\n",
    "# optimized not-delayed version\n",
    "start = time.perf_counter()\n",
    "Q_po, R_po = indirect_parallel_optimized(X_cached)  # returns Q_da (Dask), R_da (Dask)\n",
    "_ = R_po  # already materialized\n",
    "t_po_R = time.perf_counter() - start\n",
    "\n",
    "# partially-delayed version\n",
    "start = time.perf_counter()\n",
    "Q_pd, R_pd = indirect_parallel_partially_delayed(X_cached)  # returns Q_da (Dask), R (NumPy)\n",
    "_ = R_pd  # already materialized inside the function\n",
    "t_pd_R = time.perf_counter() - start\n",
    "\n",
    "# fully-delayed version\n",
    "start = time.perf_counter()\n",
    "Q_fd, R_fd = indirect_parallel_delayed(X_cached)  # returns Q_da (Dask), R_da (Dask)\n",
    "_ = R_fd.compute()  # FORCE the same final R compute\n",
    "t_fd_R = time.perf_counter() - start\n",
    "\n",
    "\n",
    "print(f\"R time — not-delayed: {t_nd_R:.3f}s | optimized: {t_po_R:.3f}s | partial: {t_pd_R:.3f}s | full-delayed: {t_fd_R:.3f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "743c01a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q apply — not-delayed: 0.080s | partial: 0.057s | partial: 0.047s | full-delayed: 0.056s\n"
     ]
    }
   ],
   "source": [
    "# e.g., apply Q to a random vector and compute the norm — same workload for all\n",
    "v = np.random.randn(X_cached.shape[1]).astype(X_cached.dtype)\n",
    "\n",
    "def time_Q_apply(Q_da, v):\n",
    "    start = time.perf_counter()\n",
    "    val = da.linalg.norm(Q_da @ v).compute()  # triggers the matmul + reduction\n",
    "    end = time.perf_counter()\n",
    "    return end - start, val\n",
    "\n",
    "t_nd_Q, val_nd = time_Q_apply(Q_nd, v)\n",
    "t_po_Q, val_po = time_Q_apply(Q_po, v)\n",
    "t_pd_Q, val_pd = time_Q_apply(Q_pd, v)\n",
    "t_fd_Q, val_fd = time_Q_apply(Q_fd, v)\n",
    "\n",
    "print(f\"Q apply — not-delayed: {t_nd_Q:.3f}s | partial: {t_po_Q:.3f}s | partial: {t_pd_Q:.3f}s | full-delayed: {t_fd_Q:.3f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013d8401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input matrix A: m = 10000000, n = 4\n",
      "The 3 blocks are: (3333333, 3333333, 3333334)\n",
      "Total size of A: 320.0 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 305.18 MiB </td>\n",
       "                        <td> 101.73 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (10000000, 4) </td>\n",
       "                        <td> (3333334, 4) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 3 chunks in 1 graph layer </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> float64 numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"75\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"39\" x2=\"25\" y2=\"39\" />\n",
       "  <line x1=\"0\" y1=\"79\" x2=\"25\" y2=\"79\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 25.412616514582485,0.0 25.412616514582485,120.0 0.0,120.0\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.706308\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >4</text>\n",
       "  <text x=\"45.412617\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,45.412617,60.000000)\">10000000</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<random_sample, shape=(10000000, 4), dtype=float64, chunksize=(3333334, 4), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_WORKERS = 3\n",
    "# Initialization of a distributed random matrix\n",
    "m, n = int(1e7), 4\n",
    "chunks = [m // N_WORKERS for _ in range(N_WORKERS-1)]\n",
    "chunks.append(m - sum(chunks))\n",
    "A = da.random.random((m, n), chunks=(chunks, n))\n",
    "\n",
    "# Persist in memory to avoid recomputation\n",
    "A = A.persist() \n",
    "\n",
    "print(f\"Input matrix A: m = {A.shape[0]}, n = {A.shape[1]}\")\n",
    "print(f\"The {len(A.chunks[0])} blocks are: {A.chunks[0]}\")\n",
    "print(f\"Total size of A: {A.nbytes / 1e6} MB\")\n",
    "A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a8ea09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- OUR IMPLEMENTATION (QR) --\n",
      "CPU times: user 14.2 s, sys: 5.63 s, total: 19.8 s\n",
      "Wall time: 19.9 s\n",
      "\n",
      "-- DASK'S IMPLEMENTATION (QR) --\n",
      "CPU times: user 1.37 s, sys: 1.86 s, total: 3.23 s\n",
      "Wall time: 16.6 s\n"
     ]
    }
   ],
   "source": [
    "# QR decomposition\n",
    "\n",
    "Q_delayed, R_delayed = indirect_parallel_delayed(A)  # Our implementation\n",
    "Q_delayed_dask, R_delayed_dask = da.linalg.tsqr(A)    # Dask's implementation\n",
    "\n",
    "print(\"-- OUR IMPLEMENTATION (QR) --\")\n",
    "%time Q, R = dask.compute(Q_delayed, R_delayed)\n",
    "\n",
    "print(\"\\n-- DASK'S IMPLEMENTATION (QR) --\")\n",
    "%time Q_dask, R_dask = dask.compute(Q_delayed_dask, R_delayed_dask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb9e3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create again a cluster\n",
    "# CLUSTER DEPLOYMENT ON CLOUDVENETO\n",
    "client.close()   # close the previous one\n",
    "cluster.close()  # close the previous one\n",
    "\n",
    "cluster = SSHCluster(\n",
    "    [\"10.67.22.154\", \"10.67.22.216\", \"10.67.22.116\", \"10.67.22.113\"],\n",
    "    connect_options={\"known_hosts\": None},\n",
    "    remote_python=\"/home/ubuntu/miniconda3/bin/python\",\n",
    "    scheduler_options={\"port\": 8786, \"dashboard_address\": \":8797\"},\n",
    "    worker_options={\n",
    "        \"nprocs\": 4,        # Now we use all 4 cores -> 12 workers\n",
    "        \"nthreads\": 1       # We use 1 threads. Following Dask documentation, however, Numpy should release well the GIL lock thus we could use more than 1 thread. \n",
    "    }\n",
    ")\n",
    "\n",
    "client = Client(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc4e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client)\n",
    "print(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25be363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc91e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import os\n",
    "\n",
    "os.chdir(\"/home/ubuntu\") \n",
    "path_HIGGS = os.getcwd() + \"/datasets/HIGGS.csv\"\n",
    "# A huge dataset\n",
    "df = dd.read_csv(path_HIGGS, header=None, blocksize=\"200MB\")    # The block size can be customized, let's start with 200 MB\n",
    "X_df = df.iloc[:, 1:] \n",
    "X_da = X_df.to_dask_array(lengths=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11c899b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_da' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#Let's print it\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mX_da\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'X_da' is not defined"
     ]
    }
   ],
   "source": [
    "#Let's print it\n",
    "X_da\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021c5b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the number of partition\n",
    "X_da.npartitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3141c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "\n",
    "Q_da, R = indirect_parallel(X_da)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7379f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "R = indirect_parallelR(X_da)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d6ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rechunk to ~1M rows per block (~100 MB) so each worker gets a good-sized task\n",
    "X_da_cached = X_da.rechunk((1_000_000, -1)).persist()\n",
    "\n",
    "# First persist actually loads CSV into memory on workers\n",
    "# This itself will take a few seconds (same cost as run 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1292baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Q_da2, R2 = indirect_parallel(X_da_cached)   # Now works from cached chunks in RAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf66bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "R = indirect_parallelR(X_da_cached)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
