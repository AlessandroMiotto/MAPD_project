{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef8a014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard: http://127.0.0.1:8787/status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-12 17:35:31,766 - tornado.application - ERROR - Uncaught exception GET /status/ws (127.0.0.1)\n",
      "HTTPServerRequest(protocol='http', host='localhost:8787', method='GET', uri='/status/ws', version='HTTP/1.1', remote_ip='127.0.0.1')\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/riccorte/miniconda3/envs/dask-env/lib/python3.11/site-packages/tornado/websocket.py\", line 965, in _accept_connection\n",
      "    open_result = handler.open(*handler.open_args, **handler.open_kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/riccorte/miniconda3/envs/dask-env/lib/python3.11/site-packages/tornado/web.py\", line 3375, in wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/riccorte/miniconda3/envs/dask-env/lib/python3.11/site-packages/bokeh/server/views/ws.py\", line 149, in open\n",
      "    raise ProtocolError(\"Token is expired. Configure the app with a larger value for --session-token-expiration if necessary\")\n",
      "bokeh.protocol.exceptions.ProtocolError: Token is expired. Configure the app with a larger value for --session-token-expiration if necessary\n",
      "2025-09-12 18:38:42,735 - tornado.application - ERROR - Uncaught exception GET /status/ws (127.0.0.1)\n",
      "HTTPServerRequest(protocol='http', host='127.0.0.1:8787', method='GET', uri='/status/ws', version='HTTP/1.1', remote_ip='127.0.0.1')\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/riccorte/miniconda3/envs/dask-env/lib/python3.11/site-packages/tornado/websocket.py\", line 965, in _accept_connection\n",
      "    open_result = handler.open(*handler.open_args, **handler.open_kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/riccorte/miniconda3/envs/dask-env/lib/python3.11/site-packages/tornado/web.py\", line 3375, in wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/riccorte/miniconda3/envs/dask-env/lib/python3.11/site-packages/bokeh/server/views/ws.py\", line 149, in open\n",
      "    raise ProtocolError(\"Token is expired. Configure the app with a larger value for --session-token-expiration if necessary\")\n",
      "bokeh.protocol.exceptions.ProtocolError: Token is expired. Configure the app with a larger value for --session-token-expiration if necessary\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# Example: 4 workers, 1 thread each\n",
    "cluster = LocalCluster(n_workers=4, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "\n",
    "print(\"Dashboard:\", client.dashboard_link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dd1b8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://10.67.22.154:8786' processes=3 threads=3, memory=5.81 GiB>\n",
      "SSHCluster(SSHCluster, 'tcp://10.67.22.154:8786', workers=3, threads=3, memory=5.81 GiB)\n"
     ]
    }
   ],
   "source": [
    "# check if everything went smoothly\n",
    "print(client)\n",
    "print(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ae28abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Dask partitions: 3\n",
      "Length of each partition: 6880 rows\n",
      "Length of the whole dataset: 20640 rows\n"
     ]
    }
   ],
   "source": [
    "import dask.array as da\n",
    "import dask\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.linalg import solve_triangular \n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Download California Housing dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "\n",
    "# Convert features into Dask Array (it's a matrix).\n",
    "n_partition = 3        # number of partition in memory. We have 4 VMS (1 master + 3 workers), so let's start with just 3 partitions\n",
    "length_partition = data.data.shape[0] // n_partition\n",
    "X_da = da.from_array(data.data.values, chunks=(length_partition, data.data.shape[1]))\n",
    "\n",
    "print(\"Number of Dask partitions:\",  X_da.npartitions) \n",
    "print(\"Length of each partition:\", length_partition, \"rows\")\n",
    "print(\"Length of the whole dataset:\", data.data.shape[0], \"rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b933587a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dask.array<array, shape=(20640, 8), dtype=float64, chunksize=(6880, 8), chunktype=numpy.ndarray>\n"
     ]
    }
   ],
   "source": [
    "X_cached = X_da.rechunk((1_000_000, -1)).persist()\n",
    "\n",
    "print(X_da)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e427157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indirect_serial(A, n_div):\n",
    "\n",
    "    \"\"\"\n",
    "    Indirect TSQR (serial, NumPy).\n",
    "    Splits A by rows into n_div blocks, computes local R_i via QR,\n",
    "    reduces to global R by QR on the stacked R_i, then recovers Q = A R^{-1}.\n",
    "    Returns (Q, R).\n",
    "    \"\"\"\n",
    "\n",
    "    n_samp = A.shape[0]\n",
    "    \n",
    "    div_points = int(np.floor(n_samp/n_div))\n",
    "    A_divided = []\n",
    "    Ri = []\n",
    "    \n",
    "    A_divided = [A[div_points * i : div_points * (i + 1)] for i in range(n_div - 1)]\n",
    "    A_divided.append(A[(n_div - 1) * div_points:, :])   # In the case n_samp wasn't divisible by n_div\n",
    "\n",
    "    Ri = [np.linalg.qr(Ai, mode=\"reduced\")[1] for Ai in A_divided]\n",
    "    \n",
    "    R_stack = np.concatenate(Ri, axis = 0)\n",
    "\n",
    "    # Step 3. Stack R_i and compute global R\n",
    "\n",
    "    _, R = np.linalg.qr(R_stack, mode=\"reduced\")\n",
    "\n",
    "\n",
    "    #   I = np.eye(n_samp, dtype=A.dtype)\n",
    "    #   Rinv = solve_triangular(R, I, lower=False)\n",
    "    Rinv = np.linalg.inv(R)\n",
    "    Q = A @ Rinv\n",
    "\n",
    "    return Q, R\n",
    "\n",
    "\n",
    "def compute_R(block):\n",
    "    # np.linalg.qr with mode='r' gives just the R matrix\n",
    "    R = np.linalg.qr(block, mode=\"r\")\n",
    "    return R\n",
    "\n",
    "\n",
    "def indirect_parallel(X_da):\n",
    "\n",
    "    \"\"\"\n",
    "    Indirect TSQR with Dask.\n",
    "    Input:\n",
    "        X_da : Dask Array (m x n), chunked row-wise\n",
    "    Output:\n",
    "        R    : final global triangular factor (n x n, NumPy array on driver)\n",
    "        Q_da : Dask Array (m x n), representing Q = A R^{-1} (lazy)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    n_cols = X_da.shape[1]\n",
    "\n",
    "    R_blocks = X_da.map_blocks(compute_R, dtype=X_da.dtype, chunks=(n_cols, n_cols))\n",
    "    # Now R_blocks is a stack of n x n matrices (one per partition)\n",
    "    # Its shape is (#chunks * n, n)\n",
    "\n",
    "    #R_blocks.visualize(filename=\"fig/R_blocks_graph\", format=\"png\")\n",
    " \n",
    "\n",
    "    #Dask has da.linalg.qr, but it assumes the whole array is large and chunked regularly.\n",
    "    #To get the final global R, you must combine all the Ri\n",
    "    #That means at some point, the data has to come together into a single place (can’t keep it sharded).\n",
    "    # So we bring the data to the driver because it is very small, because it optimizes the uses of np.linalg.qr, we are gathering the small stuff\n",
    "    R_stack = R_blocks.compute()   # NumPy array, shape (p*n, n)\n",
    "\n",
    "    # Small QR on driver to combine them into the final R\n",
    "    R = np.linalg.qr(R_stack, mode=\"r\")\n",
    "    # delay the computing of qr\n",
    "\n",
    "    # Instead of materializing Q, compute a small R^{-1} (n x n).\n",
    "    I = np.eye(n_cols, dtype=X_da.dtype)\n",
    "    R_inv = solve_triangular(R, I, lower=False)  # stable\n",
    "\n",
    "    # Broadcast Rinv to every chunk: Q = A @ R^{-1}\n",
    "    Q_da = X_da @ R_inv   # still a Dask Array, lazy\n",
    "\n",
    "    return Q_da, R      #Q_da because it is lazy, it is still a Dask array\n",
    "\n",
    "def indirect_parallel_persisted(X_da):\n",
    "\n",
    "    \"\"\"\n",
    "    Indirect TSQR with Dask.\n",
    "    Input:\n",
    "        X_da : Dask Array (m x n), chunked row-wise\n",
    "    Output:\n",
    "        R    : final global triangular factor (n x n, NumPy array on driver)\n",
    "        Q_da : Dask Array (m x n), representing Q = A R^{-1} (lazy)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "    n_cols = X_da.shape[1]\n",
    "\n",
    "    R_blocks = X_da.map_blocks(compute_R, dtype=X_da.dtype, chunks=(n_cols, n_cols))\n",
    "    # Now R_blocks is a stack of n x n matrices (one per partition)\n",
    "    # Its shape is (#chunks * n, n)\n",
    "\n",
    "    #R_blocks.visualize(filename=\"fig/R_blocks_graph\", format=\"png\")\n",
    " \n",
    "\n",
    "    #Dask has da.linalg.qr, but it assumes the whole array is large and chunked regularly.\n",
    "    #To get the final global R, you must combine all the Ri\n",
    "    #That means at some point, the data has to come together into a single place (can’t keep it sharded).\n",
    "    # So we bring the data to the driver because it is very small, because it optimizes the uses of np.linalg.qr, we are gathering the small stuff\n",
    "    R_stack = R_blocks.persist()   # NumPy array, shape (p*n, n)\n",
    "\n",
    "    # Small QR on driver to combine them into the final R\n",
    "    _, R = np.linalg.qr(R_stack)\n",
    "    # delay the computing of qr\n",
    "\n",
    "    # Instead of materializing Q, compute a small R^{-1} (n x n).\n",
    "    I = np.eye(n_cols, dtype=X_da.dtype)\n",
    "    R_inv = solve_triangular(R, I, lower=False)  # stable\n",
    "\n",
    "    # Broadcast Rinv to every chunk: Q = A @ R^{-1}\n",
    "    Q_da = X_da @ R_inv   # still a Dask Array, lazy\n",
    "\n",
    "    return Q_da, R      #Q_da because it is lazy, it is still a Dask array\n",
    "\n",
    "\n",
    "def indirect_parallel_optimized(X_da):\n",
    "\n",
    "\n",
    "    n_cols = X_da.shape[1]\n",
    "\n",
    "    R_blocks = X_da.map_blocks(compute_R, dtype=X_da.dtype, chunks=(n_cols, n_cols))\n",
    "    # Now R_blocks is a stack of n x n matrices (one per partition)\n",
    "    # Its shape is (#chunks * n, n)\n",
    "\n",
    "    #R_blocks.visualize(filename=\"fig/R_blocks_graph\", format=\"png\")\n",
    " \n",
    "\n",
    "    #Dask has da.linalg.qr, but it assumes the whole array is large and chunked regularly.\n",
    "    #To get the final global R, you must combine all the Ri\n",
    "    #That means at some point, the data has to come together into a single place (can’t keep it sharded).\n",
    "    # So we bring the data to the driver because it is very small, because it optimizes the uses of np.linalg.qr, we are gathering the small stuff\n",
    "    R_stack = R_blocks.compute()   # NumPy array, shape (p*n, n)\n",
    "\n",
    "    # Small QR on driver to combine them into the final R\n",
    "    R = np.linalg.qr(R_stack, mode=\"r\")\n",
    "    # delay the computing of qr\n",
    "\n",
    "    # Instead of materializing Q, compute a small R^{-1} (n x n).\n",
    "    I = np.eye(n_cols, dtype=X_da.dtype)\n",
    "    R_inv = solve_triangular(R, I, lower=False)  # stable\n",
    "\n",
    "    Rinv_da = da.from_array(R_inv, chunks=(R_inv.shape[0], R_inv.shape[1]))\n",
    "    Q_da = X_da @ Rinv_da\n",
    "\n",
    "    return Q_da, R      #Q_da because it is lazy, it is still a Dask array\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def indirect_parallel_delayed(X_da):\n",
    "\n",
    "    n_cols = X_da.shape[1]\n",
    "\n",
    "    R_blocks = X_da.map_blocks(compute_R, dtype=X_da.dtype, chunks=(n_cols, n_cols))\n",
    "    # Now R_blocks is a stack of n x n matrices (one per partition)\n",
    "    # Its shape is (#chunks * n, n)\n",
    "\n",
    "\n",
    "    # 2) Convert blocks to delayed NumPy arrays, stack via delayed\n",
    "    R_list = list(R_blocks.to_delayed().ravel())     # each is delayed np.ndarray (n x n)\n",
    "    R_stack = dask.delayed(np.vstack)(R_list)        # delayed (p*n x n)\n",
    "    \n",
    "\n",
    "    R_delayed = dask.delayed(compute_R)(R_stack)      # delayed np.ndarray (n x n)\n",
    "\n",
    "    # 3) Turn the small delayed R into a Dask Array (single (n,n) chunk)\n",
    "    R_da = da.from_delayed(R_delayed, shape=(n_cols, n_cols), dtype=X_da.dtype)\n",
    "\n",
    "    # 4) Materialize small R on driver; solve for R^{-1}\n",
    "    I = np.eye(n_cols, dtype=X_da.dtype)\n",
    "\n",
    "    # compute R^{-1} lazily\n",
    "    R_inv_delayed = dask.delayed(solve_triangular)(R_delayed, I, lower=False)\n",
    "    R_inv_da = da.from_delayed(R_inv_delayed, shape=(n_cols, n_cols), dtype=X_da.dtype)\n",
    "\n",
    "    # 5) Broadcast multiply (keep Q lazy)\n",
    "    Q_da = X_da @ R_inv_da\n",
    "\n",
    "    return Q_da, R_da      #Q_da because it is lazy, it is still a Dask array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d30a4fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size for each chunk is : 0.0264192 Mb\n",
      "CPU times: user 103 ms, sys: 82.4 ms, total: 186 ms\n",
      "Wall time: 64.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "Q, R = indirect_serial(data.data.values, 50)\n",
    "\n",
    "size = 20640 / 50 * 8 * 8 #rows_per_chunk × n_cols × 8 bytes\n",
    "print(\"The size for each chunk is :\", size/1e6, \"Mb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63709ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final R shape: (8, 8)\n",
      "Q is lazy: dask.array<getitem, shape=(20640, 8), dtype=float64, chunksize=(6880, 8), chunktype=numpy.ndarray>\n",
      "Chunks: ((6880, 6880, 6880), (8,))\n",
      "Number of partitions: 3\n",
      "Total size [MB]: 1.32096\n",
      "Chunk size [MB]: 0.44032\n",
      "Shape: (20640, 8)\n",
      "Chunks: ((6880, 6880, 6880), (8,))\n",
      "Partitions: 3\n",
      "Total size [MB]: 1.32096\n",
      "Avg chunk [MB]: 0.44032\n",
      "CPU times: user 65 ms, sys: 26.2 ms, total: 91.2 ms\n",
      "Wall time: 146 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "Q_da, R = indirect_parallel(X_da)   # X_da is already splitted into three partitions\n",
    "\n",
    "\n",
    "print(\"Final R shape:\", R.shape)  # (n, n), small\n",
    "print(\"Q is lazy:\", Q_da)         # Dask Array, not yet computed\n",
    "\n",
    "print(\"Chunks:\", X_da.chunks)\n",
    "print(\"Number of partitions:\", X_da.npartitions)\n",
    "print(\"Total size [MB]:\", X_da.nbytes / 1e6)\n",
    "print(\"Chunk size [MB]:\", X_da.nbytes / X_da.npartitions / 1e6)    # 10–100 MB\n",
    "\n",
    "print(\"Shape:\", X_da.shape)\n",
    "print(\"Chunks:\", X_da.chunks)\n",
    "print(\"Partitions:\", X_da.npartitions)\n",
    "print(\"Total size [MB]:\", X_da.nbytes/1e6)\n",
    "print(\"Avg chunk [MB]:\", (X_da.nbytes/X_da.npartitions)/1e6)\n",
    "\n",
    "\n",
    "#print(\"Compact sanity check:\", X_da.shape, \"(m,n);\", R_blocks.shape, \"(n *n_partitions, n);\", R.shape, \"(n,n)\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8d5e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g., apply Q to a random vector and compute the norm — same workload for all\n",
    "v = np.random.randn(X_cached.shape[1]).astype(X_cached.dtype)\n",
    "\n",
    "def time_Q_apply(Q_da, v):\n",
    "    start = time.perf_counter()\n",
    "    val = da.linalg.norm(Q_da @ v).compute()  # triggers the matmul + reduction\n",
    "    end = time.perf_counter()\n",
    "    return end - start, val\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df2cb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not-delayed version\n",
    "start = time.perf_counter()\n",
    "Q_nd, R_nd = indirect_parallel(X_cached)  # returns Q_da, R (NumPy)\n",
    "_ = R_nd  # already materialized\n",
    "t_nd_R = time.perf_counter() - start\n",
    "\n",
    "\n",
    "\n",
    "#Dask Dashboard: Big green block: compute_R. map stage of mapblock that returns the small Ri\n",
    "# Tiny yellow finalize-hlg block - Dask housekeeping\n",
    "\"\"\"Why you don’t see “reduce” or “broadcast” here\n",
    "The reduce to the final \n",
    "R is done on the driver with NumPy:\"\"\"\n",
    "\n",
    "\n",
    "t_nd_Q, val_nd = time_Q_apply(Q_nd, v)\n",
    "\n",
    "\"\"\"Teal blocks around ~140–150 ms — blockwise-matmul-…\n",
    "In the not-delayed variant, R^{-1} is a NumPy constant, so each matmul task deserializes it; you may notice slightly more per-task overhead compared to the delayed/Dask-array constant.\n",
    "Purple block — reduction for the norm\n",
    "\n",
    "After the matmul, da.linalg.norm(Qv) triggers a reduction:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503ad3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3. The subtlety\\n\\nPersist = compute and cache now, but still return a Dask collection (with futures).\\n\\nCompute = compute now and return the final NumPy array (collected to driver).\\n\\nSo:\\n\\nIf you persist inside your function and return R, you are indeed returning a Dask object backed by futures, not a NumPy matrix.\\n\\nIf you compute inside your function and return R, you’re returning a NumPy array, which is often what you want for the small triangular \\n𝑅\\nR.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not-delayed persisted version\n",
    "start = time.perf_counter()\n",
    "Q_ndp, R_ndp = indirect_parallel_persisted(X_cached)  # returns Q_da, R (NumPy)\n",
    "_ = R_ndp  # already materialized\n",
    "t_nd_Rp = time.perf_counter() - start\n",
    "\n",
    "t_po_Q, val_po = time_Q_apply(Q_ndp, v)\n",
    "\n",
    "\n",
    "# better due to only calculating R once but with a cost\n",
    "\"\"\"3. The subtlety\n",
    "\n",
    "Persist = compute and cache now, but still return a Dask collection (with futures).\n",
    "\n",
    "Compute = compute now and return the final NumPy array (collected to driver).\n",
    "\n",
    "So:\n",
    "\n",
    "If you persist inside your function and return R, you are indeed returning a Dask object backed by futures, not a NumPy matrix.\n",
    "\n",
    "If you compute inside your function and return R, you’re returning a NumPy array, which is often what you want for the small triangular \n",
    "𝑅\n",
    "R.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01f101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# optimized not-delayed version\n",
    "start = time.perf_counter()\n",
    "Q_po, R_po = indirect_parallel_optimized(X_cached)  # returns Q_da (Dask), R_da (Dask)\n",
    "_ = R_po  # already materialized\n",
    "t_po_R = time.perf_counter() - start\n",
    "\n",
    "t_po_Q, val_po = time_Q_apply(Q_po, v)\n",
    "\n",
    "\n",
    "# I don't think this version makes sense\n",
    "\"\"\"Purple (rechunk-merge): small graph-rewiring step. Dask is aligning the chunks of your broadcasted R_da with those of X_da. Because R_da is now itself a Dask Array (1 chunk of size \n",
    "𝑛\n",
    "×\n",
    "𝑛\n",
    "n×n), it needs to merge graph metadata.\n",
    "\n",
    "Big teal block (array) dominating the timeline:\n",
    "This is your broadcasted matmul tasks: X_da @ Rinv_da.\n",
    "Since R_inv was wrapped as a Dask Array (da.from_delayed or da.from_array), Dask sees a proper array-on-array multiply and generates blockwise matmul tasks.\n",
    "→ That’s why you get this large contiguous band of teal tasks: each row-chunk of X_da multiplied with the single small (n,n) chunk of Rinv_da.\"\"\"\n",
    "\"\"\"Earlier, your “not-delayed” version showed slower Q @ v apply because:\n",
    "\n",
    "You measured at small scales.\n",
    "\n",
    "Serialization overhead per task looked big compared to the tiny math.\n",
    "\n",
    "Wrapping in a Dask array made the graph smaller/cleaner → you saw ~0.06 s instead of ~0.5 s.\n",
    "\n",
    "But at larger scales (big \n",
    "𝑚\n",
    "m, many workers), the story flips:\n",
    "\n",
    "Shipping one tiny NumPy array is cheap compared to all the matmuls.\n",
    "\n",
    "Wrapping it as a Dask array adds pointless graph overhead.\n",
    "\n",
    "So you can lose a bit of time overall.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549b9af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fully-delayed version\n",
    "start = time.perf_counter()\n",
    "Q_fd, R_fd = indirect_parallel_delayed(X_cached)  # returns Q_da (Dask), R_da (Dask)\n",
    "_ = R_fd.compute()  # FORCE the same final R compute\n",
    "t_fd_R = time.perf_counter() - start\n",
    "\n",
    "\n",
    "t_fd_Q, val_fd = time_Q_apply(Q_fd, v)\n",
    "\n",
    "\n",
    "\"\"\"Efficiency Implications\n",
    "\n",
    "Computation of \n",
    "𝑅\n",
    "R: essentially unchanged, still dominated by the map stage (compute_R).\n",
    "\n",
    "Broadcast of \n",
    "𝑅\n",
    "−\n",
    "1\n",
    "R\n",
    "−1\n",
    ": somewhat less efficient as a Dask Array, since it adds extra bookkeeping without reducing the numerical cost.\n",
    "\n",
    "Norm benchmark (Q @ v): the visible red/yellow stages are expected; they confirm that your graph is carrying the computation fully through Dask.\n",
    "\n",
    "because da.from_delayed introduces those extraction tasks.\n",
    "\n",
    "Broadcast multiply still shows up as teal + red.\n",
    "\n",
    "The dashboard is “busier” — more small tasks, more scheduler chatter — because everything, even tiny constants, was lifted into the Dask graph.\n",
    "\n",
    "Efficiency interpretation\n",
    "\n",
    "For small \n",
    "𝑛\n",
    "n: making \n",
    "𝑅\n",
    "R a Dask array (fully delayed) adds overhead without real benefit — the norm compute shows more yellow/red fragmentation than the optimized/persisted version.\n",
    "\n",
    "For large distributed runs: it’s still correct, but NumPy constants (or scattered small arrays) are cheaper to handle than wrapping them in Dask.\n",
    "\n",
    "That’s why your timings showed the fully delayed version wasn’t consistently faster\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b0490b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R time — not-delayed: 0.080s | persisted: 0.110s | optimized: 0.110s | partial: 0.081s | full-delayed: 0.073s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(f\"R time — not-delayed: {t_nd_R:.3f}s | persisted: {t_po_Rp:.3f}s | optimized: {t_po_R:.3f}s | | full-delayed: {t_fd_R:.3f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c01a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q apply — not-delayed: 0.463s | partial: 0.054s | partial: 0.056s | full-delayed: 0.066s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(f\"Q apply — not-delayed: {t_nd_Q:.3f}s | partial: {t_po_Q:.3f}s | partial: {t_pd_Q:.3f}s | full-delayed: {t_fd_Q:.3f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb9e3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-15 00:22:47,147 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:47,147 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "2025-09-15 00:22:47,168 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:47,168 - distributed.scheduler - INFO - State start\n",
      "2025-09-15 00:22:47,171 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:47,171 - distributed.scheduler - INFO -   Scheduler at:   tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:48,899 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:48,900 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.116:37251'\n",
      "2025-09-15 00:22:48,902 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:48,904 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.116:36337'\n",
      "2025-09-15 00:22:48,905 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:48,906 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.116:33191'\n",
      "2025-09-15 00:22:48,906 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:48,907 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.116:42399'\n",
      "2025-09-15 00:22:48,921 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:48,912 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.113:41407'\n",
      "2025-09-15 00:22:48,923 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:48,915 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.113:34019'\n",
      "2025-09-15 00:22:48,926 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:48,917 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.113:35195'\n",
      "2025-09-15 00:22:48,926 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:48,918 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.113:41611'\n",
      "2025-09-15 00:22:49,256 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,257 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-1j22dqp2', purging\n",
      "2025-09-15 00:22:49,257 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,258 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-fnayha5r', purging\n",
      "2025-09-15 00:22:49,257 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,258 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-6h16an_o', purging\n",
      "2025-09-15 00:22:49,257 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,259 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-cpc_t_mf', purging\n",
      "2025-09-15 00:22:49,260 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,252 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-vontp9tu', purging\n",
      "2025-09-15 00:22:49,261 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,253 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-jr_jrl5t', purging\n",
      "2025-09-15 00:22:49,261 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,253 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-8szyxgxz', purging\n",
      "2025-09-15 00:22:49,262 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,253 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-dgyw0zaz', purging\n",
      "2025-09-15 00:22:49,282 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.116:40543\n",
      "2025-09-15 00:22:49,284 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.116:40543\n",
      "2025-09-15 00:22:49,284 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO -          dashboard at:         10.67.22.116:36283\n",
      "2025-09-15 00:22:49,284 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO - Waiting to connect to:    tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,285 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,285 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.116:36735\n",
      "2025-09-15 00:22:49,286 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.116:36735\n",
      "2025-09-15 00:22:49,286 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO -          dashboard at:         10.67.22.116:37983\n",
      "2025-09-15 00:22:49,287 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO - Waiting to connect to:    tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,287 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,288 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO -               Threads:                          1\n",
      "2025-09-15 00:22:49,288 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO -                Memory:                   1.94 GiB\n",
      "2025-09-15 00:22:49,288 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nwpsusjo\n",
      "2025-09-15 00:22:49,288 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO -               Threads:                          1\n",
      "2025-09-15 00:22:49,289 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,289 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO -                Memory:                   1.94 GiB\n",
      "2025-09-15 00:22:49,290 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wpu8n6vu\n",
      "2025-09-15 00:22:49,290 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,293 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,277 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.113:34663\n",
      "2025-09-15 00:22:49,293 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.113:34663\n",
      "2025-09-15 00:22:49,294 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO -          dashboard at:         10.67.22.113:42347\n",
      "2025-09-15 00:22:49,294 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO - Waiting to connect to:    tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,294 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,295 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,277 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.113:43027\n",
      "2025-09-15 00:22:49,295 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.113:43027\n",
      "2025-09-15 00:22:49,295 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO -          dashboard at:         10.67.22.113:38325\n",
      "2025-09-15 00:22:49,296 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO - Waiting to connect to:    tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,296 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,296 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO -               Threads:                          1\n",
      "2025-09-15 00:22:49,297 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO -                Memory:                   1.94 GiB\n",
      "2025-09-15 00:22:49,297 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0pw4ene4\n",
      "2025-09-15 00:22:49,297 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,298 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO -               Threads:                          1\n",
      "2025-09-15 00:22:49,298 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO -                Memory:                   1.94 GiB\n",
      "2025-09-15 00:22:49,298 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ew7d6k1_\n",
      "2025-09-15 00:22:49,299 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,299 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,280 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.113:42341\n",
      "2025-09-15 00:22:49,299 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,280 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.113:42341\n",
      "2025-09-15 00:22:49,299 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,280 - distributed.worker - INFO -          dashboard at:         10.67.22.113:36529\n",
      "2025-09-15 00:22:49,300 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,280 - distributed.worker - INFO - Waiting to connect to:    tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,300 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,280 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,300 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,280 - distributed.worker - INFO -               Threads:                          1\n",
      "2025-09-15 00:22:49,300 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,280 - distributed.worker - INFO -                Memory:                   1.94 GiB\n",
      "2025-09-15 00:22:49,301 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,280 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l2juld1r\n",
      "2025-09-15 00:22:49,301 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,280 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,303 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,299 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.116:35613\n",
      "2025-09-15 00:22:49,304 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,299 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.116:35613\n",
      "2025-09-15 00:22:49,304 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,299 - distributed.worker - INFO -          dashboard at:         10.67.22.116:34413\n",
      "2025-09-15 00:22:49,305 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,299 - distributed.worker - INFO - Waiting to connect to:    tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,305 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,299 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,305 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,299 - distributed.worker - INFO - Starting Worker plugin shuffle\n",
      "2025-09-15 00:22:49,305 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,299 - distributed.worker - INFO -               Threads:                          1\n",
      "2025-09-15 00:22:49,306 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,299 - distributed.worker - INFO -                Memory:                   1.94 GiB\n",
      "2025-09-15 00:22:49,306 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,299 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f_lt55ez\n",
      "2025-09-15 00:22:49,306 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,299 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,307 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,300 - distributed.worker - INFO -         Registered to:    tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,307 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,300 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,307 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,300 - distributed.worker - INFO - Starting Worker plugin shuffle\n",
      "2025-09-15 00:22:49,308 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,300 - distributed.worker - INFO -         Registered to:    tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,308 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,300 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,308 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,300 - distributed.core - INFO - Starting established connection to tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,308 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,300 - distributed.core - INFO - Starting established connection to tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,309 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,291 - distributed.worker - INFO - Starting Worker plugin shuffle\n",
      "2025-09-15 00:22:49,309 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,291 - distributed.worker - INFO -         Registered to:    tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,309 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,291 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,310 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,291 - distributed.worker - INFO - Starting Worker plugin shuffle\n",
      "2025-09-15 00:22:49,310 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,292 - distributed.worker - INFO -         Registered to:    tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,310 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,292 - distributed.worker - INFO - Starting Worker plugin shuffle\n",
      "2025-09-15 00:22:49,310 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,292 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,311 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,292 - distributed.worker - INFO -         Registered to:    tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,311 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,292 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,311 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,292 - distributed.core - INFO - Starting established connection to tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,312 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,292 - distributed.core - INFO - Starting established connection to tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,312 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,293 - distributed.core - INFO - Starting established connection to tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,316 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,308 - distributed.worker - INFO - Starting Worker plugin shuffle\n",
      "2025-09-15 00:22:49,316 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,308 - distributed.worker - INFO -         Registered to:    tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,316 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,308 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,317 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,308 - distributed.core - INFO - Starting established connection to tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,332 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,324 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.113:34007\n",
      "2025-09-15 00:22:49,335 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,336 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.116:43065\n",
      "2025-09-15 00:22:51,474 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:51,476 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.216:33321'\n",
      "2025-09-15 00:22:51,484 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:51,486 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.216:45893'\n",
      "2025-09-15 00:22:51,488 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:51,490 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.216:38921'\n",
      "2025-09-15 00:22:51,491 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:51,493 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.216:44273'\n",
      "2025-09-15 00:22:51,944 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:51,946 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-c_c9clw2', purging\n",
      "2025-09-15 00:22:51,945 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:51,947 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-hfid84o_', purging\n",
      "2025-09-15 00:22:51,945 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:51,947 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-xc83ur46', purging\n",
      "2025-09-15 00:22:51,946 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:51,947 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-o1iqetto', purging\n",
      "2025-09-15 00:22:52,000 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,002 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.216:33647\n",
      "2025-09-15 00:22:52,002 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.216:33647\n",
      "2025-09-15 00:22:52,002 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO -          dashboard at:         10.67.22.216:44081\n",
      "2025-09-15 00:22:52,003 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO - Waiting to connect to:    tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:52,003 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.216:43543\n",
      "2025-09-15 00:22:52,003 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:52,004 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.216:43543\n",
      "2025-09-15 00:22:52,004 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.216:37623\n",
      "2025-09-15 00:22:52,004 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO -          dashboard at:         10.67.22.216:37563\n",
      "2025-09-15 00:22:52,005 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO - Waiting to connect to:    tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:52,005 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:52,005 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.216:37623\n",
      "2025-09-15 00:22:52,005 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO -          dashboard at:         10.67.22.216:42423\n",
      "2025-09-15 00:22:52,006 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO - Waiting to connect to:    tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:52,006 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:52,006 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO -               Threads:                          1\n",
      "2025-09-15 00:22:52,007 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO -                Memory:                   1.94 GiB\n",
      "2025-09-15 00:22:52,007 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO -               Threads:                          1\n",
      "2025-09-15 00:22:52,009 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO -                Memory:                   1.94 GiB\n",
      "2025-09-15 00:22:52,009 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jg5yia9x\n",
      "2025-09-15 00:22:52,009 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,004 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:52,010 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.216:41077\n"
     ]
    }
   ],
   "source": [
    "# create again a cluster\n",
    "# CLUSTER DEPLOYMENT ON CLOUDVENETO\n",
    "\n",
    "from dask.distributed import Client, SSHCluster\n",
    "\n",
    "cluster = SSHCluster(\n",
    "    [\"10.67.22.154\", \"10.67.22.216\", \"10.67.22.116\", \"10.67.22.113\"],\n",
    "    connect_options={\"known_hosts\": None},\n",
    "    remote_python=\"/home/ubuntu/miniconda3/bin/python\",\n",
    "    scheduler_options={\"port\": 8786, \"dashboard_address\": \":8797\"},\n",
    "    worker_options={\n",
    "        \"n_workers\": 4,        # Now we use all 4 cores -> 12 workers\n",
    "        \"nthreads\": 1       # We use 1 threads. Following Dask documentation, however, Numpy should release well the GIL lock thus we could use more than 1 thread. \n",
    "    }\n",
    ")\n",
    "\n",
    "client = Client(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cc4e2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://10.67.22.154:8786' processes=12 threads=12, memory=23.25 GiB>\n",
      "SSHCluster(SSHCluster, 'tcp://10.67.22.154:8786', workers=12, threads=12, memory=23.25 GiB)\n"
     ]
    }
   ],
   "source": [
    "print(client)\n",
    "print(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0c902d3-5579-4008-b910-6179caf34e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import solve_triangular \n",
    "import dask.array as da\n",
    "import dask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94a9fd97-f867-4780-9fa4-e39f8d3cd49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure your dataset is loaded & persisted\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.chdir(\"/home/ubuntu\")\n",
    "path_HIGGS = os.path.join(os.getcwd(), \"datasets\", \"HIGGS.csv\")\n",
    "\n",
    "df = dd.read_csv(path_HIGGS, header=None, blocksize=\"200MB\")\n",
    "X_da = df.iloc[:, 1:].to_dask_array(lengths=True).astype(np.float32)\n",
    "X_da = X_da.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ce2e2b7-258f-4967-8595-d1d8afcd4c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.82 ms, sys: 124 μs, total: 9.95 ms\n",
      "Wall time: 963 ms\n"
     ]
    }
   ],
   "source": [
    "# --- Now time your function ---\n",
    "%time Q, R = indirect_parallel(X_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e6aa751-a6dc-4742-aedc-572c27024add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7581455707550049\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from dask.distributed import wait\n",
    "start = time.time()\n",
    "X_da = X_da.persist()\n",
    "Q, R1 = indirect_parallel_persisted(X_da)\n",
    "wait([Q, R1])\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8f621a6-2f90-4fcd-9f56-ebe16ba57bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "del R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3278d7d3-2067-4b89-98d2-4c08ae3d7617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.43 ms, sys: 0 ns, total: 9.43 ms\n",
      "Wall time: 863 ms\n"
     ]
    }
   ],
   "source": [
    "%time Q_da, R_da = indirect_parallel_optimized(X_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b54b885d-19f2-4dcc-a690-aecae6f823c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.16 ms, sys: 67 μs, total: 5.23 ms\n",
      "Wall time: 951 ms\n"
     ]
    }
   ],
   "source": [
    "Q_da, R_da = indirect_parallel_delayed(X_da)\n",
    "%time R = R_da.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f6b6f6a-07c8-4270-9b89-204812ae9d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27c55f4-9e72-4352-b6a5-f497e2ea8231",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
