{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef8a014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard: http://127.0.0.1:8787/status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-12 17:35:31,766 - tornado.application - ERROR - Uncaught exception GET /status/ws (127.0.0.1)\n",
      "HTTPServerRequest(protocol='http', host='localhost:8787', method='GET', uri='/status/ws', version='HTTP/1.1', remote_ip='127.0.0.1')\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/riccorte/miniconda3/envs/dask-env/lib/python3.11/site-packages/tornado/websocket.py\", line 965, in _accept_connection\n",
      "    open_result = handler.open(*handler.open_args, **handler.open_kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/riccorte/miniconda3/envs/dask-env/lib/python3.11/site-packages/tornado/web.py\", line 3375, in wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/riccorte/miniconda3/envs/dask-env/lib/python3.11/site-packages/bokeh/server/views/ws.py\", line 149, in open\n",
      "    raise ProtocolError(\"Token is expired. Configure the app with a larger value for --session-token-expiration if necessary\")\n",
      "bokeh.protocol.exceptions.ProtocolError: Token is expired. Configure the app with a larger value for --session-token-expiration if necessary\n",
      "2025-09-12 18:38:42,735 - tornado.application - ERROR - Uncaught exception GET /status/ws (127.0.0.1)\n",
      "HTTPServerRequest(protocol='http', host='127.0.0.1:8787', method='GET', uri='/status/ws', version='HTTP/1.1', remote_ip='127.0.0.1')\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/riccorte/miniconda3/envs/dask-env/lib/python3.11/site-packages/tornado/websocket.py\", line 965, in _accept_connection\n",
      "    open_result = handler.open(*handler.open_args, **handler.open_kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/riccorte/miniconda3/envs/dask-env/lib/python3.11/site-packages/tornado/web.py\", line 3375, in wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/riccorte/miniconda3/envs/dask-env/lib/python3.11/site-packages/bokeh/server/views/ws.py\", line 149, in open\n",
      "    raise ProtocolError(\"Token is expired. Configure the app with a larger value for --session-token-expiration if necessary\")\n",
      "bokeh.protocol.exceptions.ProtocolError: Token is expired. Configure the app with a larger value for --session-token-expiration if necessary\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# Example: 4 workers, 1 thread each\n",
    "cluster = LocalCluster(n_workers=4, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "\n",
    "print(\"Dashboard:\", client.dashboard_link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dd1b8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://10.67.22.154:8786' processes=3 threads=3, memory=5.81 GiB>\n",
      "SSHCluster(SSHCluster, 'tcp://10.67.22.154:8786', workers=3, threads=3, memory=5.81 GiB)\n"
     ]
    }
   ],
   "source": [
    "# check if everything went smoothly\n",
    "print(client)\n",
    "print(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ae28abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Dask partitions: 3\n",
      "Length of each partition: 6880 rows\n",
      "Length of the whole dataset: 20640 rows\n"
     ]
    }
   ],
   "source": [
    "import dask.array as da\n",
    "import dask\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.linalg import solve_triangular \n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Download California Housing dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "\n",
    "# Convert features into Dask Array (it's a matrix).\n",
    "n_partition = 3        # number of partition in memory. We have 4 VMS (1 master + 3 workers), so let's start with just 3 partitions\n",
    "length_partition = data.data.shape[0] // n_partition\n",
    "X_da = da.from_array(data.data.values, chunks=(length_partition, data.data.shape[1]))\n",
    "\n",
    "print(\"Number of Dask partitions:\",  X_da.npartitions) \n",
    "print(\"Length of each partition:\", length_partition, \"rows\")\n",
    "print(\"Length of the whole dataset:\", data.data.shape[0], \"rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b933587a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dask.array<array, shape=(20640, 8), dtype=float64, chunksize=(6880, 8), chunktype=numpy.ndarray>\n"
     ]
    }
   ],
   "source": [
    "X_cached = X_da.rechunk((1_000_000, -1)).persist()\n",
    "\n",
    "print(X_da)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e427157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indirect_serial(A, n_div):\n",
    "\n",
    "    \"\"\"\n",
    "    Indirect TSQR (serial, NumPy).\n",
    "    Splits A by rows into n_div blocks, computes local R_i via QR,\n",
    "    reduces to global R by QR on the stacked R_i, then recovers Q = A R^{-1}.\n",
    "    Returns (Q, R).\n",
    "    \"\"\"\n",
    "\n",
    "    n_samp = A.shape[0]\n",
    "    \n",
    "    div_points = int(np.floor(n_samp/n_div))\n",
    "    A_divided = []\n",
    "    Ri = []\n",
    "    \n",
    "    A_divided = [A[div_points * i : div_points * (i + 1)] for i in range(n_div - 1)]\n",
    "    A_divided.append(A[(n_div - 1) * div_points:, :])   # In the case n_samp wasn't divisible by n_div\n",
    "\n",
    "    Ri = [np.linalg.qr(Ai, mode=\"reduced\")[1] for Ai in A_divided]\n",
    "    \n",
    "    R_stack = np.concatenate(Ri, axis = 0)\n",
    "\n",
    "    # Step 3. Stack R_i and compute global R\n",
    "\n",
    "    _, R = np.linalg.qr(R_stack, mode=\"reduced\")\n",
    "\n",
    "\n",
    "    #   I = np.eye(n_samp, dtype=A.dtype)\n",
    "    #   Rinv = solve_triangular(R, I, lower=False)\n",
    "    Rinv = np.linalg.inv(R)\n",
    "    Q = A @ Rinv\n",
    "\n",
    "    return Q, R\n",
    "\n",
    "\n",
    "def compute_R(block):\n",
    "    # np.linalg.qr with mode='r' gives just the R matrix\n",
    "    R = np.linalg.qr(block, mode=\"r\")\n",
    "    return R\n",
    "\n",
    "\n",
    "def indirect_parallel(X_da):\n",
    "\n",
    "    \"\"\"\n",
    "    Indirect TSQR with Dask.\n",
    "    Input:\n",
    "        X_da : Dask Array (m x n), chunked row-wise\n",
    "    Output:\n",
    "        R    : final global triangular factor (n x n, NumPy array on driver)\n",
    "        Q_da : Dask Array (m x n), representing Q = A R^{-1} (lazy)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    n_cols = X_da.shape[1]\n",
    "\n",
    "    R_blocks = X_da.map_blocks(compute_R, dtype=X_da.dtype, chunks=(n_cols, n_cols))\n",
    "    # Now R_blocks is a stack of n x n matrices (one per partition)\n",
    "    # Its shape is (#chunks * n, n)\n",
    "\n",
    "    #R_blocks.visualize(filename=\"fig/R_blocks_graph\", format=\"png\")\n",
    " \n",
    "\n",
    "    #Dask has da.linalg.qr, but it assumes the whole array is large and chunked regularly.\n",
    "    #To get the final global R, you must combine all the Ri\n",
    "    #That means at some point, the data has to come together into a single place (canâ€™t keep it sharded).\n",
    "    # So we bring the data to the driver because it is very small, because it optimizes the uses of np.linalg.qr, we are gathering the small stuff\n",
    "    R_stack = R_blocks.compute()   # NumPy array, shape (p*n, n)\n",
    "\n",
    "    # Small QR on driver to combine them into the final R\n",
    "    R = np.linalg.qr(R_stack, mode=\"r\")\n",
    "    # delay the computing of qr\n",
    "\n",
    "    # Instead of materializing Q, compute a small R^{-1} (n x n).\n",
    "    I = np.eye(n_cols, dtype=X_da.dtype)\n",
    "    R_inv = solve_triangular(R, I, lower=False)  # stable\n",
    "\n",
    "    # Broadcast Rinv to every chunk: Q = A @ R^{-1}\n",
    "    Q_da = X_da @ R_inv   # still a Dask Array, lazy\n",
    "\n",
    "    return Q_da, R      #Q_da because it is lazy, it is still a Dask array\n",
    "\n",
    "def indirect_parallel_persisted(X_da):\n",
    "\n",
    "    \"\"\"\n",
    "    Indirect TSQR with Dask.\n",
    "    Input:\n",
    "        X_da : Dask Array (m x n), chunked row-wise\n",
    "    Output:\n",
    "        R    : final global triangular factor (n x n, NumPy array on driver)\n",
    "        Q_da : Dask Array (m x n), representing Q = A R^{-1} (lazy)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "    n_cols = X_da.shape[1]\n",
    "\n",
    "    R_blocks = X_da.map_blocks(compute_R, dtype=X_da.dtype, chunks=(n_cols, n_cols))\n",
    "    # Now R_blocks is a stack of n x n matrices (one per partition)\n",
    "    # Its shape is (#chunks * n, n)\n",
    "\n",
    "    #R_blocks.visualize(filename=\"fig/R_blocks_graph\", format=\"png\")\n",
    " \n",
    "\n",
    "    #Dask has da.linalg.qr, but it assumes the whole array is large and chunked regularly.\n",
    "    #To get the final global R, you must combine all the Ri\n",
    "    #That means at some point, the data has to come together into a single place (canâ€™t keep it sharded).\n",
    "    # So we bring the data to the driver because it is very small, because it optimizes the uses of np.linalg.qr, we are gathering the small stuff\n",
    "    R_stack = R_blocks.persist()   # NumPy array, shape (p*n, n)\n",
    "\n",
    "    # Small QR on driver to combine them into the final R\n",
    "    _, R = np.linalg.qr(R_stack)\n",
    "    # delay the computing of qr\n",
    "\n",
    "    # Instead of materializing Q, compute a small R^{-1} (n x n).\n",
    "    I = np.eye(n_cols, dtype=X_da.dtype)\n",
    "    R_inv = solve_triangular(R, I, lower=False)  # stable\n",
    "\n",
    "    # Broadcast Rinv to every chunk: Q = A @ R^{-1}\n",
    "    Q_da = X_da @ R_inv   # still a Dask Array, lazy\n",
    "\n",
    "    return Q_da, R      #Q_da because it is lazy, it is still a Dask array\n",
    "\n",
    "\n",
    "def indirect_parallel_optimized(X_da):\n",
    "\n",
    "\n",
    "    n_cols = X_da.shape[1]\n",
    "\n",
    "    R_blocks = X_da.map_blocks(compute_R, dtype=X_da.dtype, chunks=(n_cols, n_cols))\n",
    "    # Now R_blocks is a stack of n x n matrices (one per partition)\n",
    "    # Its shape is (#chunks * n, n)\n",
    "\n",
    "    #R_blocks.visualize(filename=\"fig/R_blocks_graph\", format=\"png\")\n",
    " \n",
    "\n",
    "    #Dask has da.linalg.qr, but it assumes the whole array is large and chunked regularly.\n",
    "    #To get the final global R, you must combine all the Ri\n",
    "    #That means at some point, the data has to come together into a single place (canâ€™t keep it sharded).\n",
    "    # So we bring the data to the driver because it is very small, because it optimizes the uses of np.linalg.qr, we are gathering the small stuff\n",
    "    R_stack = R_blocks.compute()   # NumPy array, shape (p*n, n)\n",
    "\n",
    "    # Small QR on driver to combine them into the final R\n",
    "    R = np.linalg.qr(R_stack, mode=\"r\")\n",
    "    # delay the computing of qr\n",
    "\n",
    "    # Instead of materializing Q, compute a small R^{-1} (n x n).\n",
    "    I = np.eye(n_cols, dtype=X_da.dtype)\n",
    "    R_inv = solve_triangular(R, I, lower=False)  # stable\n",
    "\n",
    "    Rinv_da = da.from_array(R_inv, chunks=(R_inv.shape[0], R_inv.shape[1]))\n",
    "    Q_da = X_da @ Rinv_da\n",
    "\n",
    "    return Q_da, R      #Q_da because it is lazy, it is still a Dask array\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def indirect_parallel_delayed(X_da):\n",
    "\n",
    "    n_cols = X_da.shape[1]\n",
    "\n",
    "    R_blocks = X_da.map_blocks(compute_R, dtype=X_da.dtype, chunks=(n_cols, n_cols))\n",
    "    # Now R_blocks is a stack of n x n matrices (one per partition)\n",
    "    # Its shape is (#chunks * n, n)\n",
    "\n",
    "\n",
    "    # 2) Convert blocks to delayed NumPy arrays, stack via delayed\n",
    "    R_list = list(R_blocks.to_delayed().ravel())     # each is delayed np.ndarray (n x n)\n",
    "    R_stack = dask.delayed(np.vstack)(R_list)        # delayed (p*n x n)\n",
    "    \n",
    "\n",
    "    R_delayed = dask.delayed(compute_R)(R_stack)      # delayed np.ndarray (n x n)\n",
    "\n",
    "    # 3) Turn the small delayed R into a Dask Array (single (n,n) chunk)\n",
    "    R_da = da.from_delayed(R_delayed, shape=(n_cols, n_cols), dtype=X_da.dtype)\n",
    "\n",
    "    # 4) Materialize small R on driver; solve for R^{-1}\n",
    "    I = np.eye(n_cols, dtype=X_da.dtype)\n",
    "\n",
    "    # compute R^{-1} lazily\n",
    "    R_inv_delayed = dask.delayed(solve_triangular)(R_delayed, I, lower=False)\n",
    "    R_inv_da = da.from_delayed(R_inv_delayed, shape=(n_cols, n_cols), dtype=X_da.dtype)\n",
    "\n",
    "    # 5) Broadcast multiply (keep Q lazy)\n",
    "    Q_da = X_da @ R_inv_da\n",
    "\n",
    "    return Q_da, R_da      #Q_da because it is lazy, it is still a Dask array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d30a4fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size for each chunk is : 0.0264192 Mb\n",
      "CPU times: user 103 ms, sys: 82.4 ms, total: 186 ms\n",
      "Wall time: 64.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "Q, R = indirect_serial(data.data.values, 50)\n",
    "\n",
    "size = 20640 / 50 * 8 * 8 #rows_per_chunk Ã— n_cols Ã— 8 bytes\n",
    "print(\"The size for each chunk is :\", size/1e6, \"Mb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63709ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final R shape: (8, 8)\n",
      "Q is lazy: dask.array<getitem, shape=(20640, 8), dtype=float64, chunksize=(6880, 8), chunktype=numpy.ndarray>\n",
      "Chunks: ((6880, 6880, 6880), (8,))\n",
      "Number of partitions: 3\n",
      "Total size [MB]: 1.32096\n",
      "Chunk size [MB]: 0.44032\n",
      "Shape: (20640, 8)\n",
      "Chunks: ((6880, 6880, 6880), (8,))\n",
      "Partitions: 3\n",
      "Total size [MB]: 1.32096\n",
      "Avg chunk [MB]: 0.44032\n",
      "CPU times: user 65 ms, sys: 26.2 ms, total: 91.2 ms\n",
      "Wall time: 146 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "Q_da, R = indirect_parallel(X_da)   # X_da is already splitted into three partitions\n",
    "\n",
    "\n",
    "print(\"Final R shape:\", R.shape)  # (n, n), small\n",
    "print(\"Q is lazy:\", Q_da)         # Dask Array, not yet computed\n",
    "\n",
    "print(\"Chunks:\", X_da.chunks)\n",
    "print(\"Number of partitions:\", X_da.npartitions)\n",
    "print(\"Total size [MB]:\", X_da.nbytes / 1e6)\n",
    "print(\"Chunk size [MB]:\", X_da.nbytes / X_da.npartitions / 1e6)    # 10â€“100 MB\n",
    "\n",
    "print(\"Shape:\", X_da.shape)\n",
    "print(\"Chunks:\", X_da.chunks)\n",
    "print(\"Partitions:\", X_da.npartitions)\n",
    "print(\"Total size [MB]:\", X_da.nbytes/1e6)\n",
    "print(\"Avg chunk [MB]:\", (X_da.nbytes/X_da.npartitions)/1e6)\n",
    "\n",
    "\n",
    "#print(\"Compact sanity check:\", X_da.shape, \"(m,n);\", R_blocks.shape, \"(n *n_partitions, n);\", R.shape, \"(n,n)\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8d5e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g., apply Q to a random vector and compute the norm â€” same workload for all\n",
    "v = np.random.randn(X_cached.shape[1]).astype(X_cached.dtype)\n",
    "\n",
    "def time_Q_apply(Q_da, v):\n",
    "    start = time.perf_counter()\n",
    "    val = da.linalg.norm(Q_da @ v).compute()  # triggers the matmul + reduction\n",
    "    end = time.perf_counter()\n",
    "    return end - start, val\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df2cb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not-delayed version\n",
    "start = time.perf_counter()\n",
    "Q_nd, R_nd = indirect_parallel(X_cached)  # returns Q_da, R (NumPy)\n",
    "_ = R_nd  # already materialized\n",
    "t_nd_R = time.perf_counter() - start\n",
    "\n",
    "\n",
    "\n",
    "#Dask Dashboard: Big green block: compute_R. map stage of mapblock that returns the small Ri\n",
    "# Tiny yellow finalize-hlg block - Dask housekeeping\n",
    "\"\"\"Why you donâ€™t see â€œreduceâ€ or â€œbroadcastâ€ here\n",
    "The reduce to the final \n",
    "R is done on the driver with NumPy:\"\"\"\n",
    "\n",
    "\n",
    "t_nd_Q, val_nd = time_Q_apply(Q_nd, v)\n",
    "\n",
    "\"\"\"Teal blocks around ~140â€“150 ms â€” blockwise-matmul-â€¦\n",
    "In the not-delayed variant, R^{-1} is a NumPy constant, so each matmul task deserializes it; you may notice slightly more per-task overhead compared to the delayed/Dask-array constant.\n",
    "Purple block â€” reduction for the norm\n",
    "\n",
    "After the matmul, da.linalg.norm(Qv) triggers a reduction:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503ad3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3. The subtlety\\n\\nPersist = compute and cache now, but still return a Dask collection (with futures).\\n\\nCompute = compute now and return the final NumPy array (collected to driver).\\n\\nSo:\\n\\nIf you persist inside your function and return R, you are indeed returning a Dask object backed by futures, not a NumPy matrix.\\n\\nIf you compute inside your function and return R, youâ€™re returning a NumPy array, which is often what you want for the small triangular \\nð‘…\\nR.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not-delayed persisted version\n",
    "start = time.perf_counter()\n",
    "Q_ndp, R_ndp = indirect_parallel_persisted(X_cached)  # returns Q_da, R (NumPy)\n",
    "_ = R_ndp  # already materialized\n",
    "t_nd_Rp = time.perf_counter() - start\n",
    "\n",
    "t_po_Q, val_po = time_Q_apply(Q_ndp, v)\n",
    "\n",
    "\n",
    "# better due to only calculating R once but with a cost\n",
    "\"\"\"3. The subtlety\n",
    "\n",
    "Persist = compute and cache now, but still return a Dask collection (with futures).\n",
    "\n",
    "Compute = compute now and return the final NumPy array (collected to driver).\n",
    "\n",
    "So:\n",
    "\n",
    "If you persist inside your function and return R, you are indeed returning a Dask object backed by futures, not a NumPy matrix.\n",
    "\n",
    "If you compute inside your function and return R, youâ€™re returning a NumPy array, which is often what you want for the small triangular \n",
    "ð‘…\n",
    "R.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01f101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# optimized not-delayed version\n",
    "start = time.perf_counter()\n",
    "Q_po, R_po = indirect_parallel_optimized(X_cached)  # returns Q_da (Dask), R_da (Dask)\n",
    "_ = R_po  # already materialized\n",
    "t_po_R = time.perf_counter() - start\n",
    "\n",
    "t_po_Q, val_po = time_Q_apply(Q_po, v)\n",
    "\n",
    "\n",
    "# I don't think this version makes sense\n",
    "\"\"\"Purple (rechunk-merge): small graph-rewiring step. Dask is aligning the chunks of your broadcasted R_da with those of X_da. Because R_da is now itself a Dask Array (1 chunk of size \n",
    "ð‘›\n",
    "Ã—\n",
    "ð‘›\n",
    "nÃ—n), it needs to merge graph metadata.\n",
    "\n",
    "Big teal block (array) dominating the timeline:\n",
    "This is your broadcasted matmul tasks: X_da @ Rinv_da.\n",
    "Since R_inv was wrapped as a Dask Array (da.from_delayed or da.from_array), Dask sees a proper array-on-array multiply and generates blockwise matmul tasks.\n",
    "â†’ Thatâ€™s why you get this large contiguous band of teal tasks: each row-chunk of X_da multiplied with the single small (n,n) chunk of Rinv_da.\"\"\"\n",
    "\"\"\"Earlier, your â€œnot-delayedâ€ version showed slower Q @ v apply because:\n",
    "\n",
    "You measured at small scales.\n",
    "\n",
    "Serialization overhead per task looked big compared to the tiny math.\n",
    "\n",
    "Wrapping in a Dask array made the graph smaller/cleaner â†’ you saw ~0.06 s instead of ~0.5 s.\n",
    "\n",
    "But at larger scales (big \n",
    "ð‘š\n",
    "m, many workers), the story flips:\n",
    "\n",
    "Shipping one tiny NumPy array is cheap compared to all the matmuls.\n",
    "\n",
    "Wrapping it as a Dask array adds pointless graph overhead.\n",
    "\n",
    "So you can lose a bit of time overall.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549b9af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fully-delayed version\n",
    "start = time.perf_counter()\n",
    "Q_fd, R_fd = indirect_parallel_delayed(X_cached)  # returns Q_da (Dask), R_da (Dask)\n",
    "_ = R_fd.compute()  # FORCE the same final R compute\n",
    "t_fd_R = time.perf_counter() - start\n",
    "\n",
    "\n",
    "t_fd_Q, val_fd = time_Q_apply(Q_fd, v)\n",
    "\n",
    "\n",
    "\"\"\"Efficiency Implications\n",
    "\n",
    "Computation of \n",
    "ð‘…\n",
    "R: essentially unchanged, still dominated by the map stage (compute_R).\n",
    "\n",
    "Broadcast of \n",
    "ð‘…\n",
    "âˆ’\n",
    "1\n",
    "R\n",
    "âˆ’1\n",
    ": somewhat less efficient as a Dask Array, since it adds extra bookkeeping without reducing the numerical cost.\n",
    "\n",
    "Norm benchmark (Q @ v): the visible red/yellow stages are expected; they confirm that your graph is carrying the computation fully through Dask.\n",
    "\n",
    "because da.from_delayed introduces those extraction tasks.\n",
    "\n",
    "Broadcast multiply still shows up as teal + red.\n",
    "\n",
    "The dashboard is â€œbusierâ€ â€” more small tasks, more scheduler chatter â€” because everything, even tiny constants, was lifted into the Dask graph.\n",
    "\n",
    "Efficiency interpretation\n",
    "\n",
    "For small \n",
    "ð‘›\n",
    "n: making \n",
    "ð‘…\n",
    "R a Dask array (fully delayed) adds overhead without real benefit â€” the norm compute shows more yellow/red fragmentation than the optimized/persisted version.\n",
    "\n",
    "For large distributed runs: itâ€™s still correct, but NumPy constants (or scattered small arrays) are cheaper to handle than wrapping them in Dask.\n",
    "\n",
    "Thatâ€™s why your timings showed the fully delayed version wasnâ€™t consistently faster\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b0490b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R time â€” not-delayed: 0.080s | persisted: 0.110s | optimized: 0.110s | partial: 0.081s | full-delayed: 0.073s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(f\"R time â€” not-delayed: {t_nd_R:.3f}s | persisted: {t_po_Rp:.3f}s | optimized: {t_po_R:.3f}s | | full-delayed: {t_fd_R:.3f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c01a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q apply â€” not-delayed: 0.463s | partial: 0.054s | partial: 0.056s | full-delayed: 0.066s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(f\"Q apply â€” not-delayed: {t_nd_Q:.3f}s | partial: {t_po_Q:.3f}s | partial: {t_pd_Q:.3f}s | full-delayed: {t_fd_Q:.3f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb9e3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-15 00:22:47,147 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:47,147 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "2025-09-15 00:22:47,168 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:47,168 - distributed.scheduler - INFO - State start\n",
      "2025-09-15 00:22:47,171 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:47,171 - distributed.scheduler - INFO -   Scheduler at:   tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:48,899 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:48,900 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.116:37251'\n",
      "2025-09-15 00:22:48,902 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:48,904 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.116:36337'\n",
      "2025-09-15 00:22:48,905 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:48,906 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.116:33191'\n",
      "2025-09-15 00:22:48,906 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:48,907 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.116:42399'\n",
      "2025-09-15 00:22:48,921 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:48,912 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.113:41407'\n",
      "2025-09-15 00:22:48,923 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:48,915 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.113:34019'\n",
      "2025-09-15 00:22:48,926 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:48,917 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.113:35195'\n",
      "2025-09-15 00:22:48,926 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:48,918 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.113:41611'\n",
      "2025-09-15 00:22:49,256 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,257 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-1j22dqp2', purging\n",
      "2025-09-15 00:22:49,257 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,258 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-fnayha5r', purging\n",
      "2025-09-15 00:22:49,257 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,258 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-6h16an_o', purging\n",
      "2025-09-15 00:22:49,257 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,259 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-cpc_t_mf', purging\n",
      "2025-09-15 00:22:49,260 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,252 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-vontp9tu', purging\n",
      "2025-09-15 00:22:49,261 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,253 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-jr_jrl5t', purging\n",
      "2025-09-15 00:22:49,261 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,253 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-8szyxgxz', purging\n",
      "2025-09-15 00:22:49,262 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,253 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-dgyw0zaz', purging\n",
      "2025-09-15 00:22:49,282 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.116:40543\n",
      "2025-09-15 00:22:49,284 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.116:40543\n",
      "2025-09-15 00:22:49,284 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO -          dashboard at:         10.67.22.116:36283\n",
      "2025-09-15 00:22:49,284 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO - Waiting to connect to:    tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,285 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,285 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.116:36735\n",
      "2025-09-15 00:22:49,286 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.116:36735\n",
      "2025-09-15 00:22:49,286 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO -          dashboard at:         10.67.22.116:37983\n",
      "2025-09-15 00:22:49,287 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO - Waiting to connect to:    tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,287 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,288 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO -               Threads:                          1\n",
      "2025-09-15 00:22:49,288 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO -                Memory:                   1.94 GiB\n",
      "2025-09-15 00:22:49,288 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nwpsusjo\n",
      "2025-09-15 00:22:49,288 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO -               Threads:                          1\n",
      "2025-09-15 00:22:49,289 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,289 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO -                Memory:                   1.94 GiB\n",
      "2025-09-15 00:22:49,290 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wpu8n6vu\n",
      "2025-09-15 00:22:49,290 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,284 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,293 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,277 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.113:34663\n",
      "2025-09-15 00:22:49,293 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.113:34663\n",
      "2025-09-15 00:22:49,294 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO -          dashboard at:         10.67.22.113:42347\n",
      "2025-09-15 00:22:49,294 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO - Waiting to connect to:    tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,294 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,295 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,277 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.113:43027\n",
      "2025-09-15 00:22:49,295 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.113:43027\n",
      "2025-09-15 00:22:49,295 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO -          dashboard at:         10.67.22.113:38325\n",
      "2025-09-15 00:22:49,296 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO - Waiting to connect to:    tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,296 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,296 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO -               Threads:                          1\n",
      "2025-09-15 00:22:49,297 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO -                Memory:                   1.94 GiB\n",
      "2025-09-15 00:22:49,297 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0pw4ene4\n",
      "2025-09-15 00:22:49,297 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,298 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO -               Threads:                          1\n",
      "2025-09-15 00:22:49,298 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO -                Memory:                   1.94 GiB\n",
      "2025-09-15 00:22:49,298 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ew7d6k1_\n",
      "2025-09-15 00:22:49,299 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,278 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,299 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,280 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.113:42341\n",
      "2025-09-15 00:22:49,299 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,280 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.113:42341\n",
      "2025-09-15 00:22:49,299 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,280 - distributed.worker - INFO -          dashboard at:         10.67.22.113:36529\n",
      "2025-09-15 00:22:49,300 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,280 - distributed.worker - INFO - Waiting to connect to:    tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,300 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,280 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,300 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,280 - distributed.worker - INFO -               Threads:                          1\n",
      "2025-09-15 00:22:49,300 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,280 - distributed.worker - INFO -                Memory:                   1.94 GiB\n",
      "2025-09-15 00:22:49,301 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,280 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l2juld1r\n",
      "2025-09-15 00:22:49,301 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,280 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,303 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,299 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.116:35613\n",
      "2025-09-15 00:22:49,304 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,299 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.116:35613\n",
      "2025-09-15 00:22:49,304 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,299 - distributed.worker - INFO -          dashboard at:         10.67.22.116:34413\n",
      "2025-09-15 00:22:49,305 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,299 - distributed.worker - INFO - Waiting to connect to:    tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,305 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,299 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,305 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,299 - distributed.worker - INFO - Starting Worker plugin shuffle\n",
      "2025-09-15 00:22:49,305 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,299 - distributed.worker - INFO -               Threads:                          1\n",
      "2025-09-15 00:22:49,306 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,299 - distributed.worker - INFO -                Memory:                   1.94 GiB\n",
      "2025-09-15 00:22:49,306 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,299 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f_lt55ez\n",
      "2025-09-15 00:22:49,306 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,299 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,307 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,300 - distributed.worker - INFO -         Registered to:    tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,307 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,300 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,307 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,300 - distributed.worker - INFO - Starting Worker plugin shuffle\n",
      "2025-09-15 00:22:49,308 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,300 - distributed.worker - INFO -         Registered to:    tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,308 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,300 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,308 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,300 - distributed.core - INFO - Starting established connection to tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,308 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,300 - distributed.core - INFO - Starting established connection to tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,309 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,291 - distributed.worker - INFO - Starting Worker plugin shuffle\n",
      "2025-09-15 00:22:49,309 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,291 - distributed.worker - INFO -         Registered to:    tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,309 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,291 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,310 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,291 - distributed.worker - INFO - Starting Worker plugin shuffle\n",
      "2025-09-15 00:22:49,310 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,292 - distributed.worker - INFO -         Registered to:    tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,310 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,292 - distributed.worker - INFO - Starting Worker plugin shuffle\n",
      "2025-09-15 00:22:49,310 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,292 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,311 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,292 - distributed.worker - INFO -         Registered to:    tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,311 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,292 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,311 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,292 - distributed.core - INFO - Starting established connection to tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,312 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,292 - distributed.core - INFO - Starting established connection to tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,312 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,293 - distributed.core - INFO - Starting established connection to tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,316 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,308 - distributed.worker - INFO - Starting Worker plugin shuffle\n",
      "2025-09-15 00:22:49,316 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,308 - distributed.worker - INFO -         Registered to:    tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,316 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,308 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:49,317 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,308 - distributed.core - INFO - Starting established connection to tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:49,332 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,324 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.113:34007\n",
      "2025-09-15 00:22:49,335 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:49,336 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.116:43065\n",
      "2025-09-15 00:22:51,474 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:51,476 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.216:33321'\n",
      "2025-09-15 00:22:51,484 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:51,486 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.216:45893'\n",
      "2025-09-15 00:22:51,488 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:51,490 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.216:38921'\n",
      "2025-09-15 00:22:51,491 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:51,493 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.67.22.216:44273'\n",
      "2025-09-15 00:22:51,944 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:51,946 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-c_c9clw2', purging\n",
      "2025-09-15 00:22:51,945 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:51,947 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-hfid84o_', purging\n",
      "2025-09-15 00:22:51,945 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:51,947 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-xc83ur46', purging\n",
      "2025-09-15 00:22:51,946 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:51,947 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-o1iqetto', purging\n",
      "2025-09-15 00:22:52,000 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,002 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.216:33647\n",
      "2025-09-15 00:22:52,002 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.216:33647\n",
      "2025-09-15 00:22:52,002 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO -          dashboard at:         10.67.22.216:44081\n",
      "2025-09-15 00:22:52,003 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO - Waiting to connect to:    tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:52,003 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.216:43543\n",
      "2025-09-15 00:22:52,003 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:52,004 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.216:43543\n",
      "2025-09-15 00:22:52,004 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.216:37623\n",
      "2025-09-15 00:22:52,004 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO -          dashboard at:         10.67.22.216:37563\n",
      "2025-09-15 00:22:52,005 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO - Waiting to connect to:    tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:52,005 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:52,005 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO -          Listening to:   tcp://10.67.22.216:37623\n",
      "2025-09-15 00:22:52,005 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO -          dashboard at:         10.67.22.216:42423\n",
      "2025-09-15 00:22:52,006 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO - Waiting to connect to:    tcp://10.67.22.154:8786\n",
      "2025-09-15 00:22:52,006 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:52,006 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO -               Threads:                          1\n",
      "2025-09-15 00:22:52,007 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO -                Memory:                   1.94 GiB\n",
      "2025-09-15 00:22:52,007 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO -               Threads:                          1\n",
      "2025-09-15 00:22:52,009 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO -                Memory:                   1.94 GiB\n",
      "2025-09-15 00:22:52,009 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jg5yia9x\n",
      "2025-09-15 00:22:52,009 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,004 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-09-15 00:22:52,010 - distributed.deploy.ssh - INFO - 2025-09-15 00:22:52,003 - distributed.worker - INFO -       Start worker at:   tcp://10.67.22.216:41077\n"
     ]
    }
   ],
   "source": [
    "# create again a cluster\n",
    "# CLUSTER DEPLOYMENT ON CLOUDVENETO\n",
    "\n",
    "from dask.distributed import Client, SSHCluster\n",
    "\n",
    "cluster = SSHCluster(\n",
    "    [\"10.67.22.154\", \"10.67.22.216\", \"10.67.22.116\", \"10.67.22.113\"],\n",
    "    connect_options={\"known_hosts\": None},\n",
    "    remote_python=\"/home/ubuntu/miniconda3/bin/python\",\n",
    "    scheduler_options={\"port\": 8786, \"dashboard_address\": \":8797\"},\n",
    "    worker_options={\n",
    "        \"n_workers\": 4,        # Now we use all 4 cores -> 12 workers\n",
    "        \"nthreads\": 1       # We use 1 threads. Following Dask documentation, however, Numpy should release well the GIL lock thus we could use more than 1 thread. \n",
    "    }\n",
    ")\n",
    "\n",
    "client = Client(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cc4e2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://10.67.22.154:8786' processes=12 threads=12, memory=23.25 GiB>\n",
      "SSHCluster(SSHCluster, 'tcp://10.67.22.154:8786', workers=12, threads=12, memory=23.25 GiB)\n"
     ]
    }
   ],
   "source": [
    "print(client)\n",
    "print(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0c902d3-5579-4008-b910-6179caf34e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import solve_triangular \n",
    "import dask.array as da\n",
    "import dask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94a9fd97-f867-4780-9fa4-e39f8d3cd49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure your dataset is loaded & persisted\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.chdir(\"/home/ubuntu\")\n",
    "path_HIGGS = os.path.join(os.getcwd(), \"datasets\", \"HIGGS.csv\")\n",
    "\n",
    "df = dd.read_csv(path_HIGGS, header=None, blocksize=\"200MB\")\n",
    "X_da = df.iloc[:, 1:].to_dask_array(lengths=True).astype(np.float32)\n",
    "X_da = X_da.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ce2e2b7-258f-4967-8595-d1d8afcd4c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.82 ms, sys: 124 Î¼s, total: 9.95 ms\n",
      "Wall time: 963 ms\n"
     ]
    }
   ],
   "source": [
    "# --- Now time your function ---\n",
    "%time Q, R = indirect_parallel(X_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e6aa751-a6dc-4742-aedc-572c27024add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7581455707550049\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from dask.distributed import wait\n",
    "start = time.time()\n",
    "X_da = X_da.persist()\n",
    "Q, R1 = indirect_parallel_persisted(X_da)\n",
    "wait([Q, R1])\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8f621a6-2f90-4fcd-9f56-ebe16ba57bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "del R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3278d7d3-2067-4b89-98d2-4c08ae3d7617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.43 ms, sys: 0 ns, total: 9.43 ms\n",
      "Wall time: 863 ms\n"
     ]
    }
   ],
   "source": [
    "%time Q_da, R_da = indirect_parallel_optimized(X_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b54b885d-19f2-4dcc-a690-aecae6f823c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.16 ms, sys: 67 Î¼s, total: 5.23 ms\n",
      "Wall time: 951 ms\n"
     ]
    }
   ],
   "source": [
    "Q_da, R_da = indirect_parallel_delayed(X_da)\n",
    "%time R = R_da.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f6b6f6a-07c8-4270-9b89-204812ae9d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27c55f4-9e72-4352-b6a5-f497e2ea8231",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
