{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8786a0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard: http://127.0.0.1:8787/status\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# Example: 4 workers, 1 thread each\n",
    "cluster = LocalCluster(n_workers=4, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "\n",
    "print(\"Dashboard:\", client.dashboard_link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ce40f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.13/site-packages/distributed/deploy/ssh.py:100: FutureWarning: The nprocs argument will be removed in a future release. It has been renamed to n_workers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# CLUSTER DEPLOYMENT ON CLOUDVENETO\n",
    "from dask.distributed import Client, SSHCluster\n",
    "\n",
    "cluster = SSHCluster(\n",
    "    [\"10.67.22.154\", \"10.67.22.216\", \"10.67.22.116\", \"10.67.22.113\"],\n",
    "    connect_options={\"known_hosts\": None},\n",
    "    remote_python=\"/home/ubuntu/miniconda3/bin/python\",\n",
    "    scheduler_options={\"port\": 3333, \"dashboard_address\": \":8797\"},\n",
    "    worker_options={\n",
    "        \"nprocs\": 1,       # N. of processess per VM. CloudVeneto's large VM offers 4-core CPU, but for now we only spawn 1 process per VM\n",
    "        \"nthreads\": 1      # N. of threads per process\n",
    "    }\n",
    ")\n",
    "\n",
    "client = Client(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dd1b8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://10.67.22.154:3333' processes=2 threads=2, memory=3.88 GiB>\n",
      "SSHCluster(SSHCluster, 'tcp://10.67.22.154:3333', workers=2, threads=2, memory=3.88 GiB)\n"
     ]
    }
   ],
   "source": [
    "# check if everything went smoothly\n",
    "print(client)\n",
    "print(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ae28abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Dask partitions: 3\n",
      "Length of each partition: 6880 rows\n",
      "Length of the whole dataset: 20640 rows\n"
     ]
    }
   ],
   "source": [
    "import dask.array as da\n",
    "import dask\n",
    "import numpy as np\n",
    "from scipy.linalg import solve_triangular \n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Download California Housing dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "\n",
    "# Convert features into Dask Array (it's a matrix).\n",
    "n_partition = 3        # number of partition in memory. We have 4 VMS (1 master + 3 workers), so let's start with just 3 partitions\n",
    "length_partition = data.data.shape[0] // n_partition\n",
    "X_da = da.from_array(data.data.values, chunks=(length_partition, data.data.shape[1]))\n",
    "\n",
    "print(\"Number of Dask partitions:\",  X_da.npartitions) \n",
    "print(\"Length of each partition:\", length_partition, \"rows\")\n",
    "print(\"Length of the whole dataset:\", data.data.shape[0], \"rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b933587a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dask.array<array, shape=(20640, 8), dtype=float64, chunksize=(6880, 8), chunktype=numpy.ndarray>\n"
     ]
    }
   ],
   "source": [
    "print(X_da)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e427157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indirect_serial(A, n_div):\n",
    "\n",
    "    \"\"\"\n",
    "    Indirect TSQR (serial, NumPy).\n",
    "    Splits A by rows into n_div blocks, computes local R_i via QR,\n",
    "    reduces to global R by QR on the stacked R_i, then recovers Q = A R^{-1}.\n",
    "    Returns (Q, R).\n",
    "    \"\"\"\n",
    "\n",
    "    n_samp = A.shape[0]\n",
    "    \n",
    "    div_points = int(np.floor(n_samp/n_div))\n",
    "    A_divided = []\n",
    "    Ri = []\n",
    "    \n",
    "    A_divided = [A[div_points * i : div_points * (i + 1)] for i in range(n_div - 1)]\n",
    "    A_divided.append(A[(n_div - 1) * div_points:, :])   # In the case n_samp wasn't divisible by n_div\n",
    "\n",
    "    Ri = [np.linalg.qr(Ai, mode=\"reduced\")[1] for Ai in A_divided]\n",
    "    \n",
    "    R_stack = np.concatenate(Ri, axis = 0)\n",
    "\n",
    "    # Step 3. Stack R_i and compute global R\n",
    "\n",
    "    _, R = np.linalg.qr(R_stack, mode=\"reduced\")\n",
    "\n",
    "\n",
    "    \"\"\"try:\n",
    "        I = np.eye(n, dtype=A.dtype)\n",
    "        Rinv = solve_triangular(R, I, lower=False)\n",
    "    except Exception:\n",
    "        Rinv = np.linalg.inv(R)  # fallback if SciPy not available\n",
    "    Q = A @ Rinv\"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    np.linalg.inv(R) is numerically less stable than solving the triangular system.\n",
    "    R is upper-triangular, so it is possible to try to use scipy.linalg.solve_triangular.\n",
    "    \"\"\"\n",
    "    Q = A @ np.linalg.inv(R)\n",
    "\n",
    "\n",
    "    return Q, R\n",
    "\n",
    "\n",
    "def compute_R(block):\n",
    "    # np.linalg.qr with mode='r' gives just the R matrix\n",
    "    R = np.linalg.qr(block, mode=\"r\")\n",
    "    return R\n",
    "\n",
    "\n",
    "def indirect_parallel(X_da):\n",
    "\n",
    "    \"\"\"\n",
    "    Indirect TSQR with Dask.\n",
    "    Input:\n",
    "        X_da : Dask Array (m x n), chunked row-wise\n",
    "    Output:\n",
    "        R    : final global triangular factor (n x n, NumPy array on driver)\n",
    "        Q_da : Dask Array (m x n), representing Q = A R^{-1} (lazy)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    n_cols = X_da.shape[1]\n",
    "\n",
    "    R_blocks = X_da.map_blocks(compute_R, dtype=X_da.dtype, chunks=(n_cols, n_cols))\n",
    "    # Now R_blocks is a stack of n x n matrices (one per partition)\n",
    "    # Its shape is (#chunks * n, n)\n",
    "\n",
    "    #R_blocks.visualize(filename=\"fig/R_blocks_graph\", format=\"png\")\n",
    " \n",
    "\n",
    "    #Dask has da.linalg.qr, but it assumes the whole array is large and chunked regularly.\n",
    "    #To get the final global R, you must combine all the Ri\n",
    "    #That means at some point, the data has to come together into a single place (can’t keep it sharded).\n",
    "    # So we bring the data to the driver because it is very small, because it optimizes the uses of np.linalg.qr, we are gathering the small stuff\n",
    "    R_stack = R_blocks.compute()   # NumPy array, shape (p*n, n)\n",
    "\n",
    "    # Small QR on driver to combine them into the final R\n",
    "    R = np.linalg.qr(R_stack, mode=\"r\")\n",
    "\n",
    "\n",
    "    # Instead of materializing Q, compute a small R^{-1} (n x n).\n",
    "    I = np.eye(n_cols, dtype=X_da.dtype)\n",
    "    R_inv = solve_triangular(R, I, lower=False)  # stable\n",
    "\n",
    "    # Broadcast Rinv to every chunk: Q = A @ R^{-1}\n",
    "    Q_da = X_da @ R_inv   # still a Dask Array, lazy\n",
    "\n",
    "    return Q_da, R      #Q_da because it is lazy, it is still a Dask array\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d30a4fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size for each chunk is : 0.0264192 Mb\n",
      "CPU times: user 16.3 ms, sys: 0 ns, total: 16.3 ms\n",
      "Wall time: 7.48 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "Q, R = indirect_serial(data.data.values, 50)\n",
    "size = 20640 / 50 * 8 * 8 #rows_per_chunk × n_cols × 8 bytes\n",
    "print(\"The size for each chunk is :\", size/1e6, \"Mb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63709ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final R shape: (28, 28)\n",
      "Q is lazy: dask.array<getitem, shape=(11000000, 28), dtype=float64, chunksize=(275002, 28), chunktype=numpy.ndarray>\n",
      "Chunks: ((274999, 274999, 275001, 275001, 275000, 274997, 275001, 275002, 275001, 274997, 275000, 275000, 275002, 275001, 275000, 275000, 275000, 275000, 275002, 275000, 274999, 275000, 275000, 274998, 274999, 275000, 275002, 275000, 275000, 275000, 275000, 275000, 275000, 275000, 275002, 275000, 275001, 274998, 275000, 274998), (28,))\n",
      "Number of partitions: 40\n",
      "Total size [MB]: 2464.0\n",
      "Chunk size [MB]: 61.6\n",
      "Shape: (11000000, 28)\n",
      "Chunks: ((274999, 274999, 275001, 275001, 275000, 274997, 275001, 275002, 275001, 274997, 275000, 275000, 275002, 275001, 275000, 275000, 275000, 275000, 275002, 275000, 274999, 275000, 275000, 274998, 274999, 275000, 275002, 275000, 275000, 275000, 275000, 275000, 275000, 275000, 275002, 275000, 275001, 274998, 275000, 274998), (28,))\n",
      "Partitions: 40\n",
      "Total size [MB]: 2464.0\n",
      "Avg chunk [MB]: 61.6\n",
      "CPU times: user 198 ms, sys: 41.7 ms, total: 239 ms\n",
      "Wall time: 8.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "Q_da, R = indirect_parallel(X_da)   # X_da is already splitted into three partitions\n",
    "\n",
    "\n",
    "print(\"Final R shape:\", R.shape)  # (n, n), small\n",
    "print(\"Q is lazy:\", Q_da)         # Dask Array, not yet computed\n",
    "\n",
    "print(\"Chunks:\", X_da.chunks)\n",
    "print(\"Number of partitions:\", X_da.npartitions)\n",
    "print(\"Total size [MB]:\", X_da.nbytes / 1e6)\n",
    "print(\"Chunk size [MB]:\", X_da.nbytes / X_da.npartitions / 1e6)    # 10–100 MB\n",
    "\n",
    "print(\"Shape:\", X_da.shape)\n",
    "print(\"Chunks:\", X_da.chunks)\n",
    "print(\"Partitions:\", X_da.npartitions)\n",
    "print(\"Total size [MB]:\", X_da.nbytes/1e6)\n",
    "print(\"Avg chunk [MB]:\", (X_da.nbytes/X_da.npartitions)/1e6)\n",
    "\n",
    "\n",
    "#print(\"Compact sanity check:\", X_da.shape, \"(m,n);\", R_blocks.shape, \"(n *n_partitions, n);\", R.shape, \"(n,n)\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ef5a278",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'fig/q_mul_graph.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mQ_da\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfig/q_mul_graph.png\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/dask/base.py:299\u001b[39m, in \u001b[36mDaskMethodsMixin.visualize\u001b[39m\u001b[34m(self, filename, format, optimize_graph, **kwargs)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvisualize\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename=\u001b[33m\"\u001b[39m\u001b[33mmydask\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mformat\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m, optimize_graph=\u001b[38;5;28;01mFalse\u001b[39;00m, **kwargs):\n\u001b[32m    256\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Render the computation of this object's task graph using graphviz.\u001b[39;00m\n\u001b[32m    257\u001b[39m \n\u001b[32m    258\u001b[39m \u001b[33;03m    Requires ``graphviz`` to be installed.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    297\u001b[39m \u001b[33;03m    https://docs.dask.org/en/latest/optimize.html\u001b[39;00m\n\u001b[32m    298\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvisualize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimize_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimize_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/dask/base.py:778\u001b[39m, in \u001b[36mvisualize\u001b[39m\u001b[34m(filename, traverse, optimize_graph, maxval, engine, *args, **kwargs)\u001b[39m\n\u001b[32m    774\u001b[39m args, _ = unpack_collections(*args, traverse=traverse)\n\u001b[32m    776\u001b[39m dsk = collections_to_expr(args, optimize_graph=optimize_graph).__dask_graph__()\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvisualize_dsk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraverse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimize_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimize_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaxval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    784\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/dask/base.py:915\u001b[39m, in \u001b[36mvisualize_dsk\u001b[39m\u001b[34m(dsk, filename, traverse, optimize_graph, maxval, o, engine, limit, **kwargs)\u001b[39m\n\u001b[32m    912\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mgraphviz\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    913\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdask\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dot_graph\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdot_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mcytoscape\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mipycytoscape\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    917\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdask\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cytoscape_graph\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/dask/dot.py:279\u001b[39m, in \u001b[36mdot_graph\u001b[39m\u001b[34m(dsk, filename, format, **kwargs)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    242\u001b[39m \u001b[33;03mRender a task graph using dot.\u001b[39;00m\n\u001b[32m    243\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    276\u001b[39m \u001b[33;03mdask.dot.to_graphviz\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    278\u001b[39m g = to_graphviz(dsk, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraphviz_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/dask/dot.py:312\u001b[39m, in \u001b[36mgraphviz_to_file\u001b[39m\u001b[34m(g, filename, format)\u001b[39m\n\u001b[32m    309\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m display_cls(data=data)\n\u001b[32m    311\u001b[39m full_filename = \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m.join([filename, \u001b[38;5;28mformat\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfull_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    313\u001b[39m     f.write(data)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m display_cls(filename=full_filename)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'fig/q_mul_graph.png'"
     ]
    }
   ],
   "source": [
    "\n",
    "Q_da.visualize(\"fig/q_mul_graph.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb9e3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create again a cluster\n",
    "# CLUSTER DEPLOYMENT ON CLOUDVENETO\n",
    "client.close()   # close the previous one\n",
    "cluster.close()  # close the previous one\n",
    "\n",
    "cluster = SSHCluster(\n",
    "    [\"10.67.22.154\", \"10.67.22.216\", \"10.67.22.116\", \"10.67.22.113\"],\n",
    "    connect_options={\"known_hosts\": None},\n",
    "    remote_python=\"/home/ubuntu/miniconda3/bin/python\",\n",
    "    scheduler_options={\"port\": 3333, \"dashboard_address\": \":8797\"},\n",
    "    worker_options={\n",
    "        \"nprocs\": 4,        # Now we use all 4 cores -> 12 workers\n",
    "        \"nthreads\": 1       # We use 1 threads. Following Dask documentation, however, Numpy should release well the GIL lock thus we could use more than 1 thread. \n",
    "    }\n",
    ")\n",
    "\n",
    "client = Client(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc4e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client)\n",
    "print(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25be363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc91e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import os\n",
    "\n",
    "os.chdir(\"/home/ubuntu\") \n",
    "path_HIGGS = os.getcwd() + \"/datasets/HIGGS.csv\"\n",
    "# A huge dataset\n",
    "df = dd.read_csv(path_HIGGS, header=None, blocksize=\"200MB\")    # The block size can be customized, let's start with 200 MB\n",
    "X_df = df.iloc[:, 1:] \n",
    "X_da = X_df.to_dask_array(lengths=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11c899b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dask.array<read-_to_string_dtype-values, shape=(11000000, 28), dtype=float64, chunksize=(275002, 28), chunktype=numpy.ndarray>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's print it\n",
    "print(X_da)\n",
    "\n",
    "# Print the number of partition\n",
    "X_da.npartitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "021c5b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.15 ms, sys: 390 μs, total: 5.54 ms\n",
      "Wall time: 4.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Q, R = indirect_serial(data.data.values, 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1bef807b-a4ef-4ef9-bfc6-ca2ef3007597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 195 ms, sys: 40.9 ms, total: 236 ms\n",
      "Wall time: 9.48 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "\n",
    "Q_da, R = indirect_parallel(X_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9f9801-91af-49ad-b907-be6e80c575f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
